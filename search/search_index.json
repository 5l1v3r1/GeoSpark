{
    "docs": [
        {
            "location": "/", 
            "text": "Stable\n\n\nLatest\n\n\nSource code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoSpark@Twitter\n || \nGeoSpark Discussion Board\n || \n || \n(since Jan. 2018)\n\n\nNews\n\n\n\n\nThe full \nresearch paper\n of GeoSpark has been accepted by Geoinformatica Journal. This paper has over 40 pages to dissect GeoSpark in details and compare it with many other existing systems such as Magellan, Simba, and SpatialHadoop.\n\n\nGeoSpark 1.1.3 is released. This release contains a critical bug fix for GeoSpark-core RDD API. \nRelease notes\n || \nMaven Coordinate\n.\n\n\nGeoSpark 1.1.2 is released. This release contains several bug fixes. Thanks for the patch from Lucas C.! \nRelease notes\n || \nMaven Coordinate\n.\n\n\n\n\nIntroduction\n\n\nGeoSpark is a cluster computing system for processing large-scale spatial data. GeoSpark extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.\n\n\nGeoSpark contains three modules:\n\n\n\n\n\n\n\n\nName\n\n\nAPI\n\n\nSpark compatibility\n\n\nDependency\n\n\n\n\n\n\n\n\n\n\nGeoSpark-core\n\n\nRDD\n\n\nSpark 2.X/1.X\n\n\nSpark-core\n\n\n\n\n\n\nGeoSpark-SQL\n\n\nSQL/DataFrame\n\n\nSparkSQL 2.1 and later\n\n\nSpark-core, Spark-SQL, GeoSpark-core\n\n\n\n\n\n\nGeoSpark-Viz\n\n\nRDD\n\n\nSpark 2.X/1.X\n\n\nSpark-core, GeoSpark-core\n\n\n\n\n\n\n\n\n\n\nCore: GeoSpark SpatialRDDs and Query Operators. \n\n\nSQL: SQL interfaces for GeoSpark core.\n\n\nViz: Visualization extension of GeoSpark core.\n\n\n\n\nGeoSpark development team has published four papers about GeoSpark. Please read \nPublications\n. \n\n\nGeoSpark received an evaluation from PVLDB 2018 paper \n\"How Good Are Modern Spatial Analytics Systems?\"\n Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows: \n\n\n\n\nGeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.\n\n\n\n\nFeatures\n\n\n\n\nSpatial RDD\n\n\nSpatial SQL\n\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Contains\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n)\n\n\nAND\n \ncity\n.\nname\n \n=\n \nGotham\n;\n\n\n\n\nComplex geometries / trajectories\n: point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection\n\n\nVarious input formats\n: CSV, TSV, WKT, WKB, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case\n\n\nSpatial query\n: range query, range join query, distance join query, K Nearest Neighbor query\n\n\nSpatial index\n: R-Tree, Quad-Tree\n\n\nSpatial partitioning\n: KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids\n\n\nCoordinate Reference System / Spatial Reference System Transformation\n: for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)\n\n\nHigh resolution map\n: Scatter plot, heat map, choropleth map\n\n\n\n\nCompanies are using GeoSpark\n\n\n(incomplete list)\n\n\n \n\n\nPlease make a Pull Request to add yourself!\n\n\nGeoSpark Visualization Extension (GeoSparkViz)\n\n\nGeoSparkViz is a large-scale in-memory geospatial visualization system.\n\n\nGeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.\n\n\nMore details are available here: \nGeoSpark Visualization Extension\n \n\n\nGeoSparkViz Gallery\n\n\n\n\n\n\nWatch the high resolution version on a real map", 
            "title": "Home"
        }, 
        {
            "location": "/#news", 
            "text": "The full  research paper  of GeoSpark has been accepted by Geoinformatica Journal. This paper has over 40 pages to dissect GeoSpark in details and compare it with many other existing systems such as Magellan, Simba, and SpatialHadoop.  GeoSpark 1.1.3 is released. This release contains a critical bug fix for GeoSpark-core RDD API.  Release notes  ||  Maven Coordinate .  GeoSpark 1.1.2 is released. This release contains several bug fixes. Thanks for the patch from Lucas C.!  Release notes  ||  Maven Coordinate .", 
            "title": "News"
        }, 
        {
            "location": "/#introduction", 
            "text": "GeoSpark is a cluster computing system for processing large-scale spatial data. GeoSpark extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.  GeoSpark contains three modules:     Name  API  Spark compatibility  Dependency      GeoSpark-core  RDD  Spark 2.X/1.X  Spark-core    GeoSpark-SQL  SQL/DataFrame  SparkSQL 2.1 and later  Spark-core, Spark-SQL, GeoSpark-core    GeoSpark-Viz  RDD  Spark 2.X/1.X  Spark-core, GeoSpark-core      Core: GeoSpark SpatialRDDs and Query Operators.   SQL: SQL interfaces for GeoSpark core.  Viz: Visualization extension of GeoSpark core.   GeoSpark development team has published four papers about GeoSpark. Please read  Publications .   GeoSpark received an evaluation from PVLDB 2018 paper  \"How Good Are Modern Spatial Analytics Systems?\"  Varun Pandey, Andreas Kipf, Thomas Neumann, Alfons Kemper (Technical University of Munich), quoted as follows:    GeoSpark comes close to a complete spatial analytics system. It also exhibits the best performance in most cases.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#features", 
            "text": "Spatial RDD  Spatial SQL  SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Contains ( city . geom ,   superhero . geom )  AND   city . name   =   Gotham ;   Complex geometries / trajectories : point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection  Various input formats : CSV, TSV, WKT, WKB, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case  Spatial query : range query, range join query, distance join query, K Nearest Neighbor query  Spatial index : R-Tree, Quad-Tree  Spatial partitioning : KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids  Coordinate Reference System / Spatial Reference System Transformation : for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)  High resolution map : Scatter plot, heat map, choropleth map", 
            "title": "Features"
        }, 
        {
            "location": "/#companies-are-using-geospark", 
            "text": "(incomplete list)     Please make a Pull Request to add yourself!", 
            "title": "Companies are using GeoSpark"
        }, 
        {
            "location": "/#geospark-visualization-extension-geosparkviz", 
            "text": "GeoSparkViz is a large-scale in-memory geospatial visualization system.  GeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.  More details are available here:  GeoSpark Visualization Extension    GeoSparkViz Gallery    Watch the high resolution version on a real map", 
            "title": "GeoSpark Visualization Extension (GeoSparkViz)"
        }, 
        {
            "location": "/download/overview/", 
            "text": "Direct download\n\n\nGeoSpark source code is hosted on \nGeoSpark GitHub repository\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nGeoSpark GitHub Releases\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nMaven Central\n.\n\n\nGeoSpark release notes are here \nRelease notes\n.\n\n\nInstall GeoSpark\n\n\nBefore starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.\n\n\nThere are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.\n\n\n\n\nSpark interactive Scala shell: easy to start, good for new learners to try simple functions\n\n\nSelf-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "Quick start"
        }, 
        {
            "location": "/download/overview/#direct-download", 
            "text": "GeoSpark source code is hosted on  GeoSpark GitHub repository .  GeoSpark pre-compiled JARs are hosted on  GeoSpark GitHub Releases .  GeoSpark pre-compiled JARs are hosted on  Maven Central .  GeoSpark release notes are here  Release notes .", 
            "title": "Direct download"
        }, 
        {
            "location": "/download/overview/#install-geospark", 
            "text": "Before starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.  There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.   Spark interactive Scala shell: easy to start, good for new learners to try simple functions  Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "Install GeoSpark"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/", 
            "text": "v1.1.3\n\n\nThis version contains a critical bug fix for GeoSpark-core RDD API.\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n222\n: geometry toString() method has cumulative non-spatial attributes. See PR #\n223\n\n\n\n\nGeoSpark SQL\n\n\nNone\n\n\nGeoSpark Viz\n\n\nNone\n\n\nv1.1.2\n\n\nThis version contains several bug fixes and several small improvements.\n\n\nGeoSpark Core\n\n\n\n\nAdded WKB input format support (Issue #\n2\n, \n213\n): See PR #\n203\n, \n216\n. Thanks for the patch from Lucas C.!\n\n\nAdded empty constructors for typed SpatialRDDs. This is especially useful when the users want to load a persisted RDD from disk and assemble a typed SpatialRDD by themselves. See PR #\n211\n\n\nFixed Issue #\n214\n: duplicated geometry parts when print each Geometry in a SpatialRDD to a String using toString() method. See PR #\n216\n\n\n\n\nGeoSpark SQL\n\n\n\n\nAdded ST_GeomFromWKB expression (Issue #\n2\n): See PR #\n203\n. Thanks for the patch from Lucas C.!\n\n\nFixed Issue #\n193\n: IllegalArgumentException in RangeJoin: Number of partitions must be \n= 0. See PR #\n207\n\n\nFixed Issue #\n204\n: Wrong ST_Intersection result. See PR #\n205\n\n\n[For Developer] Separate the expression catalog and the udf registrator to simplify the steps of merging patches among different Spark versions. See PR #\n209\n\n\n\n\nGeoSpark Viz\n\n\nNone\n\n\nv1.1.1\n\n\nThis release has been skipped due to wrong Maven Central configuration.\n\n\nv1.1.0\n\n\nThis version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See \nMaven Central coordinate\n to locate the particular version.\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n185\n: CRStransform throws Exception for Bursa wolf parameters. See PR #\n189\n.\n\n\nFixed Issue #\n190\n: Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR #\n192\n.\n\n\nAdd R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR #\n177\n.\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n194\n: doesn't support Spark 2.3.\n\n\nFixed Issue #\n188\n:ST_ConvexHull should accept any type of geometry as an input. See PR #\n189\n.\n\n\nAdd ST_Intersection function. See Issue #\n110\n and PR #\n189\n.\n\n\n\n\nGeoSpark Viz\n\n\n\n\nFixed Issue #\n154\n: GeoSpark kryp serializer and GeoSparkViz conflict. See PR #\n178\n\n\n\n\nv1.0.1\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n170\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n171\n\n\nAdded the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately (\nMaven Coordinates\n).\n\n\n\n\nGeoSpark Viz\n\nNone\n\n\n\n\nv1.0.0\n\n\nGeoSpark Core\n\n\n\n\nAdd GeoSparkConf class to read GeoSparkConf from SparkConf\n\n\n\n\nGeoSpark SQL\n\n\n\n\nInitial release: fully supports SQL/MM-Part3 Spatial SQL standard\n\n\n\n\nGeoSpark Viz\n\n\n\n\nRepublish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"\n\n\n\n\n\n\n\n\n\n\nv0.9.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fixed \"Missing values when reading Shapefile\": \nIssue #141\n\n\nPerformance improvement\n: Solved Issue \n#91\n, \n#103\n, \n#104\n, \n#125\n, \n#150\n.\n\n\nAdd GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See \nPR 139\n.\n\n\nDelete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See \nPR 131\n.\n\n\n\n\n\n\nNew Functionalities added\n:\n\n\nSpatialJoinQueryFlat/DistanceJoinQueryFlat\n returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results. \n\n\nThe left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe index side in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe generic SpatialRdd supports heterogenous geometries\n\n\nAdd KDB-Tree spatial partitioning method which is more balanced than Quad-Tree\n\n\nRange query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.\n\n\n\n\n\n\n\n\nv0.8.2 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue \nhttps://github.com/DataSystemsLab/GeoSpark/issues/115\n\n\nNew function added\n: Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.\n\n\n\n\nv0.8.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors\n\n\n\n\nv0.8.0 (GeoSpark-core)\n\n\n\n\nNew input format added\n: GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 1\n: GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 2\n: Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.\n\n\nSpatialRDD Initialization Performance enhancement\n: GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.\n\n\nAPI changed\n: Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see \nGeoSpark user supplied format mapper examples\n\n\nAlternative SpatialRDD constructor added\n: GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.\n\n\nIf he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.\n\n\nIf he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.\n\n\nIf he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset\n, the user still has to pass StorageLevel parameter.\n\n\nBug fix\n: Fix bug \nIssue #97\n and \nIssue #100\n.\n\n\n\n\nv0.1 - v0.7\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.7.0\n\n\nCoordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:\n GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor (\nExample\n); \nUnnecessary dependencies removed\n: NetCDF/HDF support depends on \nSerNetCDF\n. SetNetCDF becomes optional dependency to reduce fat jar size; \nDefault JDK/JRE change to JDK/JRE 1.8\n: To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default; \nBug fix\n: fix a small format bug when output spatial RDD to disk.\n\n\n\n\n\n\n0.6.2\n\n\nNew input format added:\n Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data (\nScala Example\n,\nJava Example\n); \nBug fix:\n Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file\n\n\n\n\n\n\n0.6.1\n\n\nBug fixes:\n Fix typos LineString DistanceJoin API\n\n\n\n\n\n\n0.6.0\n\n\nMajor updates:\n (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results. \nAPI changes:\n All spatial range and join  queries now take a parameter called \nConsiderBoundaryIntersection\n. This will tell GeoSpark whether returns the objects intersect with windows.\n\n\n\n\n\n\n0.5.3\n\n\nBug fix:\n Fix \nIssue #69\n: Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.\n\n\n\n\n\n\n0.5.2\n\n\nBug fix:\n Fix \nIssue #58\n and \nIssue #60\n; \nPerformance enhancement:\n (1) Deprecate all old Spatial RDD constructors. See the JavaDoc \nhere\n. (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step\n\n\n\n\n\n\n0.5.1\n\n\nBug fix:\n (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call \nIssue #55\n; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support\n\n\n\n\n\n\n0.5.0\n\n\nMajor updates:\n We are pleased to announce the initial version of \nBabylon\n a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here: \nBabylon GeoSpatial Visualization\n\n\n\n\n\n\n0.4.0\n\n\nMajor updates:\n (\nExample\n) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD; \nFunctionality enhancement:\n 1. Release the persist function back to users. 2. Add more exception explanations.\n\n\n\n\n\n\n0.3.2\n\n\nFunctionality enhancement: 1. \nJTSplus Spatial Objects\n now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.\n\n\n\n\n\n\n0.3.1\n\n\nBug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases\n\n\n\n\n\n\n0.3\n\n\nMajor updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining\n\n\n\n\n\n\n0.2\n\n\nImprove code structure and refactor API\n\n\n\n\n\n\n0.1\n\n\nSupport spatial range, join and Knn\n\n\n\n\n\n\n\n\nGeoSpark-Viz (old)\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.2.2\n\n\nAdd the support of new output storage\n: Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.\n\n\n\n\n\n\n0.2.1\n\n\nPerformance enhancement\n: significantly accelerate single image generation pipeline. \nBug fix\n:fix a bug in scatter plot parallel rendering.\n\n\n\n\n\n\n0.2.0\n\n\nAPI updates for \nIssue #80\n:\n 1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs; \nNew feature:\n Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. (\nScala Example\n,\nJava Example\n)\n\n\n\n\n\n\n0.1.1\n\n\nMajor updates:\n Babylon supports vector image and outputs SVG image format\n\n\n\n\n\n\n0.1.0\n\n\nMajor updates:\n Babylon initial version supports raster images", 
            "title": "Release notes"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v113", 
            "text": "This version contains a critical bug fix for GeoSpark-core RDD API.  GeoSpark Core   Fixed Issue # 222 : geometry toString() method has cumulative non-spatial attributes. See PR # 223   GeoSpark SQL  None  GeoSpark Viz  None", 
            "title": "v1.1.3"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v112", 
            "text": "This version contains several bug fixes and several small improvements.  GeoSpark Core   Added WKB input format support (Issue # 2 ,  213 ): See PR # 203 ,  216 . Thanks for the patch from Lucas C.!  Added empty constructors for typed SpatialRDDs. This is especially useful when the users want to load a persisted RDD from disk and assemble a typed SpatialRDD by themselves. See PR # 211  Fixed Issue # 214 : duplicated geometry parts when print each Geometry in a SpatialRDD to a String using toString() method. See PR # 216   GeoSpark SQL   Added ST_GeomFromWKB expression (Issue # 2 ): See PR # 203 . Thanks for the patch from Lucas C.!  Fixed Issue # 193 : IllegalArgumentException in RangeJoin: Number of partitions must be  = 0. See PR # 207  Fixed Issue # 204 : Wrong ST_Intersection result. See PR # 205  [For Developer] Separate the expression catalog and the udf registrator to simplify the steps of merging patches among different Spark versions. See PR # 209   GeoSpark Viz  None", 
            "title": "v1.1.2"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v111", 
            "text": "This release has been skipped due to wrong Maven Central configuration.", 
            "title": "v1.1.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v110", 
            "text": "This version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See  Maven Central coordinate  to locate the particular version.  GeoSpark Core   Fixed Issue # 185 : CRStransform throws Exception for Bursa wolf parameters. See PR # 189 .  Fixed Issue # 190 : Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR # 192 .  Add R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR # 177 .   GeoSpark SQL   Fixed Issue # 194 : doesn't support Spark 2.3.  Fixed Issue # 188 :ST_ConvexHull should accept any type of geometry as an input. See PR # 189 .  Add ST_Intersection function. See Issue # 110  and PR # 189 .   GeoSpark Viz   Fixed Issue # 154 : GeoSpark kryp serializer and GeoSparkViz conflict. See PR # 178", 
            "title": "v1.1.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v101", 
            "text": "GeoSpark Core   Fixed Issue # 170   GeoSpark SQL   Fixed Issue # 171  Added the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately ( Maven Coordinates ).   GeoSpark Viz \nNone", 
            "title": "v1.0.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v100", 
            "text": "GeoSpark Core   Add GeoSparkConf class to read GeoSparkConf from SparkConf   GeoSpark SQL   Initial release: fully supports SQL/MM-Part3 Spatial SQL standard   GeoSpark Viz   Republish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"", 
            "title": "v1.0.0"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v091-geospark-core", 
            "text": "Bug fixes : Fixed \"Missing values when reading Shapefile\":  Issue #141  Performance improvement : Solved Issue  #91 ,  #103 ,  #104 ,  #125 ,  #150 .  Add GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See  PR 139 .  Delete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See  PR 131 .    New Functionalities added :  SpatialJoinQueryFlat/DistanceJoinQueryFlat  returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results.   The left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.  The index side in Range query, Distance query, Range join query, Distance join query can be switched.  The generic SpatialRdd supports heterogenous geometries  Add KDB-Tree spatial partitioning method which is more balanced than Quad-Tree  Range query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.", 
            "title": "v0.9.1 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v082-geospark-core", 
            "text": "Bug fixes : Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue  https://github.com/DataSystemsLab/GeoSpark/issues/115  New function added : Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.", 
            "title": "v0.8.2 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v081-geospark-core", 
            "text": "Bug fixes : (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors", 
            "title": "v0.8.1 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v080-geospark-core", 
            "text": "New input format added : GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. ( Scala Example ,  Java Example )  Join Query Performance enhancement 1 : GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. ( Scala Example ,  Java Example )  Join Query Performance enhancement 2 : Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.  SpatialRDD Initialization Performance enhancement : GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.  API changed : Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see  GeoSpark user supplied format mapper examples  Alternative SpatialRDD constructor added : GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.  If he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.  If he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.  If he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset , the user still has to pass StorageLevel parameter.  Bug fix : Fix bug  Issue #97  and  Issue #100 .", 
            "title": "v0.8.0 (GeoSpark-core)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#v01-v07", 
            "text": "Version  Summary      0.7.0  Coordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:  GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor ( Example );  Unnecessary dependencies removed : NetCDF/HDF support depends on  SerNetCDF . SetNetCDF becomes optional dependency to reduce fat jar size;  Default JDK/JRE change to JDK/JRE 1.8 : To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default;  Bug fix : fix a small format bug when output spatial RDD to disk.    0.6.2  New input format added:  Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data ( Scala Example , Java Example );  Bug fix:  Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file    0.6.1  Bug fixes:  Fix typos LineString DistanceJoin API    0.6.0  Major updates:  (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results.  API changes:  All spatial range and join  queries now take a parameter called  ConsiderBoundaryIntersection . This will tell GeoSpark whether returns the objects intersect with windows.    0.5.3  Bug fix:  Fix  Issue #69 : Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.    0.5.2  Bug fix:  Fix  Issue #58  and  Issue #60 ;  Performance enhancement:  (1) Deprecate all old Spatial RDD constructors. See the JavaDoc  here . (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step    0.5.1  Bug fix:  (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call  Issue #55 ; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support    0.5.0  Major updates:  We are pleased to announce the initial version of  Babylon  a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here:  Babylon GeoSpatial Visualization    0.4.0  Major updates:  ( Example ) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD;  Functionality enhancement:  1. Release the persist function back to users. 2. Add more exception explanations.    0.3.2  Functionality enhancement: 1.  JTSplus Spatial Objects  now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.    0.3.1  Bug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases    0.3  Major updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining    0.2  Improve code structure and refactor API    0.1  Support spatial range, join and Knn", 
            "title": "v0.1 - v0.7"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Release-notes/#geospark-viz-old", 
            "text": "Version  Summary      0.2.2  Add the support of new output storage : Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.    0.2.1  Performance enhancement : significantly accelerate single image generation pipeline.  Bug fix :fix a bug in scatter plot parallel rendering.    0.2.0  API updates for  Issue #80 :  1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs;  New feature:  Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. ( Scala Example , Java Example )    0.1.1  Major updates:  Babylon supports vector image and outputs SVG image format    0.1.0  Major updates:  Babylon initial version supports raster images", 
            "title": "GeoSpark-Viz (old)"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/", 
            "text": "Apache Spark 2.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\nGeoSpark-SQL\n\n\nFor SparkSQL-2.3\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n3\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\nFor SparkSQL-2.2\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n2\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\nFor SparkSQL-2.1\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n1\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\nGeoSpark-Viz\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz\n\n\nversion\n:\n \n1.1\n.\n3\n\n\n\n\n\n\n\nApache Spark 1.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n0.8\n.\n2\n-\nspark\n-\n1\n.\nx\n\n\n\n\n\nGeoSpark-Viz\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \nbabylon\n\n\nversion\n:\n \n0.2\n.\n1\n-\nspark\n-\n1\n.\nx\n\n\n\n\n\n\n\nSNAPSHOT versions\n\n\nSometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase.\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n\n\nversion\n:\n \n1.2\n.\n0\n-\nSNAPSHOT\n\n\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nsql_2\n.\n3\n\n\nversion\n:\n \n1.2\n.\n0\n-\nSNAPSHOT\n\n\n\n\n\ngroupId\n:\n \norg\n.\ndatasyslab\n\n\nartifactId\n:\n \ngeospark\n-\nviz\n\n\nversion\n:\n \n1.2\n.\n0\n-\nSNAPSHOT\n\n\n\n\n\nIn order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt\n\n\nbuild.sbt\n\n\nresolvers +=\n  \"Sonatype OSS Snapshots\" at \"\nhttps://oss.sonatype.org/content/repositories/snapshots\n\"\n\n\nPOM.XML\n\n\nprofiles\n\n    \nprofile\n\n        \nid\nallow-snapshots\n/id\n\n        \nactivation\nactiveByDefault\ntrue\n/activeByDefault\n/activation\n\n        \nrepositories\n\n            \nrepository\n\n                \nid\nsnapshots-repo\n/id\n\n                \nurl\nhttps://oss.sonatype.org/content/repositories/snapshots\n/url\n\n                \nreleases\nenabled\nfalse\n/enabled\n/releases\n\n                \nsnapshots\nenabled\ntrue\n/enabled\n/snapshots\n\n            \n/repository\n\n        \n/repositories\n\n    \n/profile\n\n\n/profiles", 
            "title": "Maven Central coordinate"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-2x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 2.X versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark  version :   1.1 . 3", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-sql", 
            "text": "", 
            "title": "GeoSpark-SQL"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-23", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 3  version :   1.1 . 3", 
            "title": "For SparkSQL-2.3"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-22", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 2  version :   1.1 . 3", 
            "title": "For SparkSQL-2.2"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-21", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 1  version :   1.1 . 3", 
            "title": "For SparkSQL-2.1"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark - viz  version :   1.1 . 3", 
            "title": "GeoSpark-Viz"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-1x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 1.X versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core_1", 
            "text": "groupId :   org . datasyslab  artifactId :   geospark  version :   0.8 . 2 - spark - 1 . x", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz_1", 
            "text": "groupId :   org . datasyslab  artifactId :   babylon  version :   0.2 . 1 - spark - 1 . x", 
            "title": "GeoSpark-Viz"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#snapshot-versions", 
            "text": "Sometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase. groupId :   org . datasyslab  artifactId :   geospark  version :   1.2 . 0 - SNAPSHOT   groupId :   org . datasyslab  artifactId :   geospark - sql_2 . 3  version :   1.2 . 0 - SNAPSHOT   groupId :   org . datasyslab  artifactId :   geospark - viz  version :   1.2 . 0 - SNAPSHOT   In order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt", 
            "title": "SNAPSHOT versions"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#buildsbt", 
            "text": "resolvers +=\n  \"Sonatype OSS Snapshots\" at \" https://oss.sonatype.org/content/repositories/snapshots \"", 
            "title": "build.sbt"
        }, 
        {
            "location": "/download/GeoSpark-All-Modules-Maven-Central-Coordinates/#pomxml", 
            "text": "profiles \n     profile \n         id allow-snapshots /id \n         activation activeByDefault true /activeByDefault /activation \n         repositories \n             repository \n                 id snapshots-repo /id \n                 url https://oss.sonatype.org/content/repositories/snapshots /url \n                 releases enabled false /enabled /releases \n                 snapshots enabled true /enabled /snapshots \n             /repository \n         /repositories \n     /profile  /profiles", 
            "title": "POM.XML"
        }, 
        {
            "location": "/download/cluster/", 
            "text": "Set up your Apache Spark cluster\n\n\nDownload a Spark distribution from \nSpark download page\n.\n\n\nPreliminary\n\n\n\n\nSet up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.\n\n\nMake sure you have installed JRE 1.8 or later.\n\n\nAdd the list of your workers' IP address in ./conf/slaves\n\n\nBesides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:\n\n\n\n\nIn \n./conf/spark-defaults.conf\n\n\nspark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n\n\n\n\n\n\nspark.driver.memory\n tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.\n\n\nspark.network.timeout\n is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.\n\n\nspark.driver.maxResultSize\n is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.\n\n\n\n\nFor more details of Spark parameters, please visit \nSpark Website\n.\n\n\nStart your cluster\n\n\nGo the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal\n\n\n./sbin/start-all.sh", 
            "title": "Set up Spark cluser"
        }, 
        {
            "location": "/download/cluster/#set-up-your-apache-spark-cluster", 
            "text": "Download a Spark distribution from  Spark download page .", 
            "title": "Set up your Apache Spark cluster"
        }, 
        {
            "location": "/download/cluster/#preliminary", 
            "text": "Set up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.  Make sure you have installed JRE 1.8 or later.  Add the list of your workers' IP address in ./conf/slaves  Besides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:   In  ./conf/spark-defaults.conf  spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g   spark.driver.memory  tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.  spark.network.timeout  is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.  spark.driver.maxResultSize  is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.   For more details of Spark parameters, please visit  Spark Website .", 
            "title": "Preliminary"
        }, 
        {
            "location": "/download/cluster/#start-your-cluster", 
            "text": "Go the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal  ./sbin/start-all.sh", 
            "title": "Start your cluster"
        }, 
        {
            "location": "/download/scalashell/", 
            "text": "Spark Scala shell\n\n\nSpark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.\n\n\n\n\nWarning\n\n\nGeoSparkSQL cannot run in this mode. We are still working on this issue.\n\n\n\n\nDownload GeoSpark jar automatically\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nRun Spark shell with \n--packages\n option. This command will automatically download GeoSpark jars from Maven Central.\n\n./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION\n\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n\n./bin/spark-shell --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0\n\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n\n./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0\n\n\n\n\n\n\n\nDownload GeoSpark jar manually\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nDownload GeoSpark jars:\n\n\n\n\nDownload the pre-compiled jars from \nGeoSpark Releases on GitHub\n\n\nDownload / Git clone GeoSpark source code and compile the code by yourself:\n\nmvn clean install -DskipTests\n\n\n\n\n\n\n\n\n\nRun Spark shell with \n--jars\n option.\n\n./bin/spark-shell --jars /Path/To/GeoSparkJars.jar\n\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n\n./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar\n\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n\n\n./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#spark-scala-shell", 
            "text": "Spark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.   Warning  GeoSparkSQL cannot run in this mode. We are still working on this issue.", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-automatically", 
            "text": "Have your Spark cluster ready.    Run Spark shell with  --packages  option. This command will automatically download GeoSpark jars from Maven Central. ./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION    Local mode: test GeoSpark without setting up a cluster ./bin/spark-shell --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0    Cluster mode: you need to specify Spark Master IP ./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0", 
            "title": "Download GeoSpark jar automatically"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-manually", 
            "text": "Have your Spark cluster ready.    Download GeoSpark jars:   Download the pre-compiled jars from  GeoSpark Releases on GitHub  Download / Git clone GeoSpark source code and compile the code by yourself: mvn clean install -DskipTests     Run Spark shell with  --jars  option. ./bin/spark-shell --jars /Path/To/GeoSparkJars.jar    Local mode: test GeoSpark without setting up a cluster ./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar    Cluster mode: you need to specify Spark Master IP  ./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Download GeoSpark jar manually"
        }, 
        {
            "location": "/download/project/", 
            "text": "Self-contained Spark projects\n\n\nA self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.\n\n\nQuick start\n\n\n\n\nTo add GeoSpark as dependencies, please read \nGeoSpark Maven Central coordinates\n\n\nUse GeoSpark Template project to start: \nGeoSpark Template Project\n\n\nCompile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.\n\n\nSubmit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n\n\n\n\n\n\n\nNote\n\n\nThe detailed explanation of spark-submit is available on \nSpark website\n.\n\n\n\n\nHow to use GeoSpark in an IDE\n\n\nSelect an IDE\n\n\nTo develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.\n\n\nEclipse is also fine if you just want to use Java and Maven.\n\n\nOpen GeoSpark template project\n\n\nSelect a proper GeoSpark project you want from \nGeoSpark Template Project\n. In this tutorial, we use GeoSparkSQL Scala project as an example.\n\n\nOpen the folder that contains \nbuild.sbt\n file in your IDE. The IDE may take a while to index dependencies and source code.\n\n\nTry GeoSpark SQL functions\n\n\nIn your IDE, run \nScalaExample.scala\n file.\n\n\nYou don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.\n\n\nPackage the project\n\n\nTo run this project in cluster mode, you have to package this project to a JAR and then run it using \nspark-submit\n command.\n\n\nBefore packaging this project, you always need to check two places:\n\n\n\n\n\n\nRemove the hardcoded Master IP \nmaster(\nlocal[*]\n)\n. This hardcoded IP is only needed when you run this project in an IDE.\n\nvar\n \nsparkSession\n:\nSparkSession\n \n=\n \nSparkSession\n.\nbuilder\n()\n\n    \n.\nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n    \n.\nconfig\n(\nspark.kryo.registrator\n,\nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n    \n.\nmaster\n(\nlocal[*]\n)\n\n    \n.\nappName\n(\nGeoSparkSQL-demo\n).\ngetOrCreate\n()\n\n\n\n\n\n\n\n\nIn build.sbt (or POM.xml), set Spark dependency scope to \nprovided\n instead of \ncompile\n. \ncompile\n is only needed when you run this project in an IDE.\n\norg.apache.spark\n %% \nspark-core\n % SparkVersion % \ncompile,\norg.apache.spark\n %% \nspark-sql\n % SparkVersion % \ncompile\n\n\n\n\n\n\n\n\n\nWarning\n\n\nForgetting to change the package scope will lead to a very big fat JAR and dependency conflicts when call \nspark-submit\n. For more details, please visit \nMaven Dependency Scope\n.\n\n\n\n\n\n\nMake sure your downloaded Spark binary distribution is the same version with the Spark used in your \nbuild.sbt\n or \nPOM.xml\n.\n\n\n\n\nSubmit the compiled jar\n\n\n\n\nGo to \n./target/scala-2.11\n folder and find a jar called \nGeoSparkSQLScalaTemplate-0.1.0.jar\n. Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under \n./target\n folder)\n\n\n\n\nSubmit this JAR using \nspark-submit\n.\n\n\n\n\n\n\nLocal mode:\n\n./bin/spark-submit /Path/To/YourJar.jar\n\n\n\n\n\n\n\nCluster mode:\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Self-contained project"
        }, 
        {
            "location": "/download/project/#self-contained-spark-projects", 
            "text": "A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.", 
            "title": "Self-contained Spark projects"
        }, 
        {
            "location": "/download/project/#quick-start", 
            "text": "To add GeoSpark as dependencies, please read  GeoSpark Maven Central coordinates  Use GeoSpark Template project to start:  GeoSpark Template Project  Compile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.  Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command: ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar    Note  The detailed explanation of spark-submit is available on  Spark website .", 
            "title": "Quick start"
        }, 
        {
            "location": "/download/project/#how-to-use-geospark-in-an-ide", 
            "text": "", 
            "title": "How to use GeoSpark in an IDE"
        }, 
        {
            "location": "/download/project/#select-an-ide", 
            "text": "To develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.  Eclipse is also fine if you just want to use Java and Maven.", 
            "title": "Select an IDE"
        }, 
        {
            "location": "/download/project/#open-geospark-template-project", 
            "text": "Select a proper GeoSpark project you want from  GeoSpark Template Project . In this tutorial, we use GeoSparkSQL Scala project as an example.  Open the folder that contains  build.sbt  file in your IDE. The IDE may take a while to index dependencies and source code.", 
            "title": "Open GeoSpark template project"
        }, 
        {
            "location": "/download/project/#try-geospark-sql-functions", 
            "text": "In your IDE, run  ScalaExample.scala  file.  You don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.", 
            "title": "Try GeoSpark SQL functions"
        }, 
        {
            "location": "/download/project/#package-the-project", 
            "text": "To run this project in cluster mode, you have to package this project to a JAR and then run it using  spark-submit  command.  Before packaging this project, you always need to check two places:    Remove the hardcoded Master IP  master( local[*] ) . This hardcoded IP is only needed when you run this project in an IDE. var   sparkSession : SparkSession   =   SparkSession . builder () \n     . config ( spark.serializer , classOf [ KryoSerializer ]. getName ) \n     . config ( spark.kryo.registrator , classOf [ GeoSparkKryoRegistrator ]. getName ) \n     . master ( local[*] ) \n     . appName ( GeoSparkSQL-demo ). getOrCreate ()     In build.sbt (or POM.xml), set Spark dependency scope to  provided  instead of  compile .  compile  is only needed when you run this project in an IDE. org.apache.spark  %%  spark-core  % SparkVersion %  compile,\norg.apache.spark  %%  spark-sql  % SparkVersion %  compile     Warning  Forgetting to change the package scope will lead to a very big fat JAR and dependency conflicts when call  spark-submit . For more details, please visit  Maven Dependency Scope .    Make sure your downloaded Spark binary distribution is the same version with the Spark used in your  build.sbt  or  POM.xml .", 
            "title": "Package the project"
        }, 
        {
            "location": "/download/project/#submit-the-compiled-jar", 
            "text": "Go to  ./target/scala-2.11  folder and find a jar called  GeoSparkSQLScalaTemplate-0.1.0.jar . Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under  ./target  folder)   Submit this JAR using  spark-submit .    Local mode: ./bin/spark-submit /Path/To/YourJar.jar    Cluster mode: ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Submit the compiled jar"
        }, 
        {
            "location": "/download/compile/", 
            "text": "Compile GeoSpark\n\n\nSome GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:\n\n\n\n\nDownload / Git clone GeoSpark source code from \nGeoSpark Github repo\n.\n\n\n\n\nCompile the source code\n\n\nGeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3. \n\n\n\n\nMake sure your machine has Java 1.8 and Apache Maven 3.\n\n\n\n\nTo compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:\n\n\nmvn clean install -DskipTests\n\n\nThis command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.\n\n\nTo compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.\n\n\nTo run unit tests, just simply remove \n-DskipTests\n option. The command is like this:\n\nmvn clean install\n\n\n\n\n\nWarning\n\n\nThe unit tests of all three modules may take up to 30 minutes. \n\n\n\n\nCompile the documentation\n\n\nThe source code of GeoSpark documentation website is written in Markdown and then compiled by MkDocs. The website is built upon \nMaterial for MkDocs template\n.\n\n\nIn GeoSpark repository, MkDocs configuration file \nmkdocs.yml\n is in the root folder and all documentation source code is in docs folder.\n\n\nTo compile the source code and test the website on your local machine, please read \nMkDocs Tutorial\n and \nMaterials for MkDocs Tutorial\n .", 
            "title": "Compile the source code"
        }, 
        {
            "location": "/download/compile/#compile-geospark", 
            "text": "Some GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:   Download / Git clone GeoSpark source code from  GeoSpark Github repo .", 
            "title": "Compile GeoSpark"
        }, 
        {
            "location": "/download/compile/#compile-the-source-code", 
            "text": "GeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.    Make sure your machine has Java 1.8 and Apache Maven 3.   To compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:  mvn clean install -DskipTests \nThis command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.  To compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.  To run unit tests, just simply remove  -DskipTests  option. The command is like this: mvn clean install   Warning  The unit tests of all three modules may take up to 30 minutes.", 
            "title": "Compile the source code"
        }, 
        {
            "location": "/download/compile/#compile-the-documentation", 
            "text": "The source code of GeoSpark documentation website is written in Markdown and then compiled by MkDocs. The website is built upon  Material for MkDocs template .  In GeoSpark repository, MkDocs configuration file  mkdocs.yml  is in the root folder and all documentation source code is in docs folder.  To compile the source code and test the website on your local machine, please read  MkDocs Tutorial  and  Materials for MkDocs Tutorial  .", 
            "title": "Compile the documentation"
        }, 
        {
            "location": "/tutorial/rdd/", 
            "text": "The page outlines the steps to create Spatial RDDs and run spatial queries using GeoSpark-core. \nThe example code is written in Scala but also works for Java\n.\n\n\nSet up dependencies\n\n\n\n\nRead \nGeoSpark Maven Central coordinates\n\n\nSelect \nthe minimum dependencies\n: Add Apache Spark (only the Spark core) and GeoSpark (core).\n\n\nAdd the dependencies in build.sbt or pom.xml.\n\n\n\n\n\n\nNote\n\n\nTo enjoy the full functions of GeoSpark, we suggest you include \nthe full dependencies\n: \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n, \nGeoSparkViz\n\n\n\n\nInitiate SparkContext\n\n\nval\n \nconf\n \n=\n \nnew\n \nSparkConf\n()\n\n\nconf\n.\nsetAppName\n(\nGeoSparkRunnableExample\n)\n \n// Change this to a proper name\n\n\nconf\n.\nsetMaster\n(\nlocal[*]\n)\n \n// Delete this if run in cluster mode\n\n\n// Enable GeoSpark custom Kryo serializer\n\n\nconf\n.\nset\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\nconf\n.\nset\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n\nval\n \nsc\n \n=\n \nnew\n \nSparkContext\n(\nconf\n)\n\n\n\n\n\n\n\nWarning\n\n\nGeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.\n\n\n\n\nIf you add \nthe GeoSpark full dependencies\n as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead:\n\nconf\n.\nset\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\nconf\n.\nset\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n)\n\n\n\n\nCreate a SpatialRDD\n\n\nCreate a typed SpatialRDD\n\n\nGeoSpark-core provdies three special SpatialRDDs: \nPointRDD, PolygonRDD, and LineStringRDD\n. They can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON and NetCDF/HDF format.\n\n\nPointRDD from CSV/TSV\n\n\nSuppose we have a \ncheckin.csv\n CSV file at Path \n/Download/checkin.csv\n as follows:\n\n-88.331492,32.324142,hotel\n-88.175933,32.360763,gas\n-88.388954,32.357073,bar\n-88.221102,32.35078,restaurant\n\n\nThis file has three columns and corresponding \noffsets\n(Column IDs) are 0, 1, 2.\nUse the following code to create a PointRDD\n\n\nval\n \npointRDDInputLocation\n \n=\n \n/Download/checkin.csv\n\n\nval\n \npointRDDOffset\n \n=\n \n0\n \n// The point long/lat starts from Column 0\n\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nCSV\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 2 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDOffset\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\n\nIf the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter:\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nTSV\n\n\n\n\nPolygonRDD/LineStringRDD from CSV/TSV\n\n\nIn genereal, polygon and line string data is stored in WKT, WKB, GeoJSON and Shapefile formats instead of CSV/TSV because the geometris in a file may have different lengths. However, if all polygons / line strings in your CSV/TSV possess the same length, you can create PolygonRDD and LineStringRDD from these files.\n\n\nSuppose we have a \ncheckinshape.csv\n CSV file at Path \n/Download/checkinshape.csv\n as follows:\n\n-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,hotel\n-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,gas\n-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,bar\n-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,restaurant\n\n\n\nThis file has 11 columns and corresponding offsets (Column IDs) are 0 - 10. Column 0 - 9 are 5 coordinates (longitude/latitude pairs). In this file, all geometries have the same number of coordinates. The geometries can be polyons or line strings.\n\n\n\n\nWarning\n\n\nFor polygon data, the last coordinate must be the same as the first coordinate because a polygon is a closed linear ring.\n\n\n\n\nUse the following code to create a PolygonRDD.\n\nval\n \npolygonRDDInputLocation\n \n=\n \n/Download/checkin.csv\n\n\nval\n \npolygonRDDStartOffset\n \n=\n \n0\n \n// The coordinates start from Column 0\n\n\nval\n \npolygonRDDEndOffset\n \n=\n \n8\n \n// The coordinates end at Column 8\n\n\nval\n \npolygonRDDSplitter\n \n=\n \nFileDataSplitter\n.\nCSV\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 10 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPolygonRDD\n(\nsc\n,\n \npolygonRDDInputLocation\n,\n \npolygonRDDStartOffset\n,\n \npolygonRDDEndOffset\n,\n \npolygonRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\nIf the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter:\n\nval\n \npolygonRDDSplitter\n \n=\n \nFileDataSplitter\n.\nTSV\n\n\n\n\nThe way to create a LineStringRDD is the same as PolygonRDD.\n\n\nFrom WKT/WKB\n\n\nGeometries in a WKT and WKB file always occucpy a single column no matter how many coordinates they have. Therefore, creating a typed SpatialRDD is easy.\n\n\nSuppose we have a \ncheckin.tsv\n WKT TSV file at Path \n/Download/checkin.tsv\n as follows:\n\nPOINT (-88.331492,32.324142)    hotel\nPOINT (-88.175933,32.360763)    gas\nPOINT (-88.388954,32.357073)    bar\nPOINT (-88.221102,32.35078) restaurant\n\n\nThis file has two columns and corresponding \noffsets\n(Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.\n\n\nUse the following code to create a PointRDD\n\n\nval\n \npointRDDInputLocation\n \n=\n \n/Download/checkin.csv\n\n\nval\n \npointRDDOffset\n \n=\n \n0\n \n// The WKT string starts from Column 0\n\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nWKT\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 1 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDOffset\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\nUse the following code to create a PolygonRDD/LineStringRDD\n\nval\n \npolygonRDDInputLocation\n \n=\n \n/Download/checkin.csv\n\n\nval\n \npolygonRDDStartOffset\n \n=\n \n0\n \n// The coordinates start from Column 0\n\n\nval\n \npolygonRDDEndOffset\n \n=\n \n0\n \n// The coordinates end at Column 0\n\n\nval\n \npolygonRDDSplitter\n \n=\n \nFileDataSplitter\n.\nWKT\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 1 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPolygonRDD\n(\nsc\n,\n \npolygonRDDInputLocation\n,\n \npolygonRDDStartOffset\n,\n \npolygonRDDEndOffset\n,\n \npolygonRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\nFrom GeoJSON\n\n\nGeometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.\n\n\nSuppose we have a \npolygon.json\n GeoJSON file at Path \n/Download/polygon.json\n as follows:\n\n\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n077\n, \nTRACTCE\n: \n011501\n, \nBLKGRPCE\n: \n5\n, \nAFFGEOID\n: \n1500000US010770115015\n, \nGEOID\n: \n010770115015\n, \nNAME\n: \n5\n, \nLSAD\n: \nBG\n, \nALAND\n: 6844991, \nAWATER\n: 32636 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n045\n, \nTRACTCE\n: \n021102\n, \nBLKGRPCE\n: \n4\n, \nAFFGEOID\n: \n1500000US010450211024\n, \nGEOID\n: \n010450211024\n, \nNAME\n: \n4\n, \nLSAD\n: \nBG\n, \nALAND\n: 11360854, \nAWATER\n: 0 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n055\n, \nTRACTCE\n: \n001300\n, \nBLKGRPCE\n: \n3\n, \nAFFGEOID\n: \n1500000US010550013003\n, \nGEOID\n: \n010550013003\n, \nNAME\n: \n3\n, \nLSAD\n: \nBG\n, \nALAND\n: 1378742, \nAWATER\n: 247387 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{ \ntype\n: \nFeature\n, \nproperties\n: { \nSTATEFP\n: \n01\n, \nCOUNTYFP\n: \n089\n, \nTRACTCE\n: \n001700\n, \nBLKGRPCE\n: \n2\n, \nAFFGEOID\n: \n1500000US010890017002\n, \nGEOID\n: \n010890017002\n, \nNAME\n: \n2\n, \nLSAD\n: \nBG\n, \nALAND\n: 1040641, \nAWATER\n: 0 }, \ngeometry\n: { \ntype\n: \nPolygon\n, \ncoordinates\n: [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },\n\n\n\n\nUse the following code to create a typed SpatialRDD (PointRDD/PolygonRDD/LineStringRDD):\n\nval\n \npointRDDInputLocation\n \n=\n \n/Download/polygon.json\n\n\nval\n \npointRDDSplitter\n \n=\n \nFileDataSplitter\n.\nGEOJSON\n\n\nval\n \ncarryOtherAttributes\n \n=\n \ntrue\n \n// Carry Column 1 (hotel, gas, bar...)\n\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\n\n\n\n\nWarning\n\n\nThe way that GeoSpark reads GeoJSON is different from that in SparkSQL\n\n\n\n\nCreate a generic SpatialRDD\n\n\nA generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instace, a WKT file contains three types gemetries \nLineString\n, \nPolygon\n and \nMultiPolygon\n.\n\n\nFrom all formats\n\n\nTo create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you have to use GeoSparkSQL. Make sure you include \nthe full dependencies\n of GeoSpark. Read \nGeoSparkSQL API\n.\n\n\nWe use \ncheckin.csv CSV file\n as the example. You can create a generic SpatialRDD using the following steps:\n\n\n\n\nLoad data in GeoSparkSQL.\n\nvar\n \ndf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\ndelimiter\n,\n \n\\t\n).\noption\n(\nheader\n,\n \nfalse\n).\nload\n(\ncsvPointInputLocation\n)\n\n\ndf\n.\ncreateOrReplaceTempView\n(\ninputtable\n)\n\n\n\n\nCreate a Geometry type column in GeoSparkSQL\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n    \n\n\n        |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin\n\n\n        |FROM inputtable\n\n\n    \n.\nstripMargin\n)\n\n\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nAdapter\n.\ntoRdd\n(\nspatialDf\n)\n\n\n\n\n\n\nFor WKT/WKB/GeoJSON data, please use \nST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON\n instead.\n\n\nFrom Shapefile\n\n\nval\n \nshapefileInputLocation\n=\n/Download/myshapefile\n\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nShapefileReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \nshapefileInputLocation\n)\n\n\n\n\n\n\n\nNote\n\n\nThe file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called \nmyShapefile\n, the file structure should be like this:\n\n- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n\n\n\n\n\nTransform the Coordinate Reference System\n\n\nGeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in GeoSpark is same as the unit of all geometries in an SpatialRDD.\n\n\nTo convert Coordinate Reference System of an SpatialRDD, use the following code:\n\n\nval\n \nsourceCrsCode\n \n=\n \nepsg:4326\n \n// WGS84, the most common degree-based CRS\n\n\nval\n \ntargetCrsCode\n \n=\n \nepsg:3857\n \n// The most common meter-based CRS\n\n\nobjectRDD\n.\nCRSTransform\n(\nsourceCrsCode\n,\n \ntargetCrsCode\n)\n\n\n\n\n\n\n\nWarning\n\n\nCRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instace, use something like this:\n\nvar\n \nobjectRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n,\n \npointRDDInputLocation\n,\n \npointRDDOffset\n,\n \npointRDDSplitter\n,\n \ncarryOtherAttributes\n)\n\n\nobjectRDD\n.\nCRSTransform\n(\nepsg:4326\n,\n \nepsg:3857\n)\n\n\n\n\n\n\nThe details CRS information can be found on \nEPSG.io\n\n\nRead other attributes in an SpatialRDD\n\n\nEach SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets \ncarryOtherAttributes\n as \nTRUE\n.\n\n\nThe other attributes are combined together to a string and stored in \nUserData\n field of each geometry.\n\n\nTo retrieve the UserData field, use the following code:\n\nval\n \nrddWithOtherAttributes\n \n=\n \nobjectRDD\n.\nrawSpatialRDD\n.\nrdd\n.\nmap\n[\nString\n](\nf\n=\nf\n.\ngetUserData\n.\nasInstanceOf\n[\nString\n])\n\n\n\n\nWrite a Spatial Range Query\n\n\nA spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that intersect / are fully covered by the query window.\n\n\nAssume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.\n\n\nval\n \nrangeQueryWindow\n \n=\n \nnew\n \nEnvelope\n(-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by the window\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\nvar\n \nqueryResult\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\nspatialRDD\n,\n \nrangeQueryWindow\n,\n \nconsiderBoundaryIntersection\n,\n \nusingIndex\n)\n\n\n\n\n\nconsiderBoundaryIntersection\n can be set to TRUE to return all geometries intersect with query window.\n\n\n\n\nNote\n\n\nSpatial range query is equal to \nST_Within\n and \nST_Intersects\n in Spatial SQL. An example query is as follows:\n\nSELECT\n \n*\n\n\nFROM\n \ncheckin\n\n\nWHERE\n \nST_Intersects\n(\nqueryWindow\n,\n \ncheckin\n.\nlocation\n)\n\n\n\n\n\n\nRange query window\n\n\nBesides the rectangle (Envelope) type range query window, GeoSpark range query window can be Point/Polygon/LineString.\n\n\nThe code to create a point is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\n\n\n\nThe code to create a polygon (with 4 vertexes) is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \ncoordinates\n \n=\n \nnew\n \nArray\n[\nCoordinate\n](\n5\n)\n\n\ncoordinates\n(\n0\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n0\n)\n\n\ncoordinates\n(\n1\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n4\n)\n\n\ncoordinates\n(\n2\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n4\n)\n\n\ncoordinates\n(\n3\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n0\n)\n\n\ncoordinates\n(\n4\n)\n \n=\n \ncoordinates\n(\n0\n)\n \n// The last coordinate is the same as the first coordinate in order to compose a closed ring\n\n\nval\n \npolygonObject\n \n=\n \ngeometryFactory\n.\ncreatePolygon\n(\ncoordinates\n)\n\n\n\n\n\nThe code to create a line string (with 4 vertexes) is as follows:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \ncoordinates\n \n=\n \nnew\n \nArray\n[\nCoordinate\n](\n5\n)\n\n\ncoordinates\n(\n0\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n0\n)\n\n\ncoordinates\n(\n1\n)\n \n=\n \nnew\n \nCoordinate\n(\n0\n,\n4\n)\n\n\ncoordinates\n(\n2\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n4\n)\n\n\ncoordinates\n(\n3\n)\n \n=\n \nnew\n \nCoordinate\n(\n4\n,\n0\n)\n\n\nval\n \nlinestringObject\n \n=\n \ngeometryFactory\n.\ncreateLineString\n(\ncoordinates\n)\n\n\n\n\n\nUse spatial indexes\n\n\nGeoSpark provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, GeoSpark will build a local tree index on each of the SpatialRDD partition.\n\n\nTo utilize a spatial index in a spatial range query, use the following code:\n\n\nval\n \nrangeQueryWindow\n \n=\n \nnew\n \nEnvelope\n(-\n90.01\n,\n \n-\n80.01\n,\n \n30.01\n,\n \n40.01\n)\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by the window\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \nfalse\n \n// Set to TRUE only if run join query\n\n\nspatialRDD\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nvar\n \nqueryResult\n \n=\n \nRangeQuery\n.\nSpatialRangeQuery\n(\nspatialRDD\n,\n \nrangeQueryWindow\n,\n \nconsiderBoundaryIntersection\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nTip\n\n\nUsing an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.\n\n\n\n\nOutput format\n\n\nThe output format of the spatial range query is another SpatialRDD.\n\n\nWrite a Spatial KNN Query\n\n\nA spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.\n\n\nAssume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\nval\n \nK\n \n=\n \n1000\n \n// K Nearest Neighbors\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\nval\n \nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nobjectRDD\n,\n \npointObject\n,\n \nK\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nNote\n\n\nSpatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL\n\nSELECT\n \nck\n.\nname\n,\n \nck\n.\nrating\n,\n \nST_Distance\n(\nck\n.\nlocation\n,\n \nmyLocation\n)\n \nAS\n \ndistance\n\n\nFROM\n \ncheckins\n \nck\n\n\nORDER\n \nBY\n \ndistance\n \nDESC\n\n\nLIMIT\n \n5\n\n\n\n\n\n\nQuery center geometry\n\n\nBesides the Point type, GeoSpark KNN query center can be Polygon and LineString.\n\n\nTo learn how to create Polygon and LineString object, see \nRange query window\n.\n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial KNN query, use the following code:\n\n\nval\n \ngeometryFactory\n \n=\n \nnew\n \nGeometryFactory\n()\n\n\nval\n \npointObject\n \n=\n \ngeometryFactory\n.\ncreatePoint\n(\nnew\n \nCoordinate\n(-\n84.01\n,\n \n34.01\n))\n\n\nval\n \nK\n \n=\n \n1000\n \n// K Nearest Neighbors\n\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \nfalse\n \n// Set to TRUE only if run join query\n\n\nspatialRDD\n.\nbuildIndex\n(\nIndexType\n.\nRTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nval\n \nresult\n \n=\n \nKNNQuery\n.\nSpatialKnnQuery\n(\nobjectRDD\n,\n \npointObject\n,\n \nK\n,\n \nusingIndex\n)\n\n\n\n\n\n\n\nWarning\n\n\nOnly R-Tree index supports Spatial KNN query\n\n\n\n\nOutput format\n\n\nThe output format of the spatial KNN query is a list of geometries. The list has K geometry objects.\n\n\nWrite a Spatial Join Query\n\n\nA spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.\n\n\nAssume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by each query window in queryWindowRDD\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\n\nobjectRDD\n.\nanalyze\n()\n\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQuery\n(\nobjectRDD\n,\n \nqueryWindowRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\n\n\nNote\n\n\nSpatial join query is equal to the following query in Spatial SQL:\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Contains\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n);\n\n\n\nFind the super heros in each city\n\n\n\n\nUse spatial partitioning\n\n\nGeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.\n\n\nIf you first partition SpatialRDD A, then you must use the partitioner of A to partition B.\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\n\n\nOr \n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nobjectRDD\n.\nspatialPartitioning\n(\nqueryWindowRDD\n.\ngetPartitioner\n)\n\n\n\n\n\nUse spatial indexes\n\n\nTo utilize a spatial index in a spatial join query, use the following code:\n\n\nobjectRDD\n.\nspatialPartitioning\n(\njoinQueryPartitioningType\n)\n\n\nqueryWindowRDD\n.\nspatialPartitioning\n(\nobjectRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nbuildOnSpatialPartitionedRDD\n \n=\n \ntrue\n \n// Set to TRUE only if run join query\n\n\nval\n \nusingIndex\n \n=\n \ntrue\n\n\nqueryWindowRDD\n.\nbuildIndex\n(\nIndexType\n.\nQUADTREE\n,\n \nbuildOnSpatialPartitionedRDD\n)\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nSpatialJoinQueryFlat\n(\nobjectRDD\n,\n \nqueryWindowRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\nThe index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.\n\n\nOutput format\n\n\nThe output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.\n\n\nPoint,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...\n\n\n\n\nEach object on the left is covered/intersected by the object on the right.\n\n\nWrite a Distance Join Query\n\n\nA distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained \nhere\n.\n\n\nAssume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.\n\n\nobjectRddA\n.\nanalyze\n()\n\n\n\nval\n \ncircleRDD\n \n=\n \nnew\n \nCircleRDD\n(\nobjectRddA\n,\n \n0.1\n)\n \n// Create a CircleRDD using the given distance\n\n\n\ncircleRDD\n.\nspatialPartitioning\n(\nGridType\n.\nKDBTREE\n)\n\n\nobjectRddB\n.\nspatialPartitioning\n(\ncircleRDD\n.\ngetPartitioner\n)\n\n\n\nval\n \nconsiderBoundaryIntersection\n \n=\n \nfalse\n \n// Only return gemeotries fully covered by each query window in queryWindowRDD\n\n\nval\n \nusingIndex\n \n=\n \nfalse\n\n\n\nval\n \nresult\n \n=\n \nJoinQuery\n.\nDistanceJoinQueryFlat\n(\nobjectRddB\n,\n \ncircleRDD\n,\n \nusingIndex\n,\n \nconsiderBoundaryIntersection\n)\n\n\n\n\n\nThe rest part of the join query is same as the spatial join query.\n\n\nThe details of spatial partitioning in join query is \nhere\n.\n\n\nThe details of using spatial indexes in join query is \nhere\n.\n\n\nThe output format of the distance join query is \nhere\n.\n\n\n\n\nNote\n\n\nDistance join query is equal to the following query in Spatial SQL:\n\nSELECT\n \nsuperhero\n.\nname\n\n\nFROM\n \ncity\n,\n \nsuperhero\n\n\nWHERE\n \nST_Distance\n(\ncity\n.\ngeom\n,\n \nsuperhero\n.\ngeom\n)\n \n=\n \n10\n;\n\n\n\nFind the super heros within 10 miles of each city\n\n\n\n\nSave to permanent storage\n\n\nYou can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.\n\n\n\n\nNote\n\n\nNon-spatial attributes such as price, age and name will also be stored to permanent storage.\n\n\n\n\nSave an SpatialRDD (not indexed)\n\n\nTyped SpatialRDD and generic SpatialRDD can be saved to permanent storage.\n\n\nSave to distributed WKT text file\n\n\nUse the following code to save an SpatialRDD as a distributed WKT text file:\n\n\nobjectRDD\n.\nrawSpatialRDD\n.\nsaveAsTextFile\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed GeoJSON text file\n\n\nUse the following code to save an SpatialRDD as a distributed GeoJSON text file:\n\n\nobjectRDD\n.\nsaveAsGeoJSON\n(\nhdfs://PATH\n)\n\n\n\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobjectRDD\n.\nrawSpatialRDD\n.\nsaveAsObjectFile\n(\nhdfs://PATH\n)\n\n\n\n\n\n\n\nNote\n\n\nEach object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.\n\n\n\n\nSave an SpatialRDD (indexed)\n\n\nIndexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.\n\n\nSave to distributed object file\n\n\nUse the following code to save an SpatialRDD as a distributed object file:\n\n\nobjectRDD.indexedRawRDD.saveAsObjectFile(\nhdfs://PATH\n)\n\n\n\n\nSave an SpatialRDD (spatialPartitioned W/O indexed)\n\n\nA spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!\n\n\nReload a saved SpatialRDD\n\n\nYou can easily reload an SpatialRDD that has been saved to \na distributed object file\n.\n\n\nLoad to a typed SpatialRDD\n\n\nUse the following code to reload the PointRDD/PolygonRDD/LineStringRDD:\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nPoint\n](\nhdfs://PATH\n))\n\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nPolygon\n](\nhdfs://PATH\n))\n\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nPointRDD\n(\nsc\n.\nobjectFile\n[\nLineString\n](\nhdfs://PATH\n))\n\n\n\n\n\nLoad to a generic SpatialRDD\n\n\nUse the following code to reload the SpatialRDD:\n\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nsavedRDD\n.\nrawSpatialRDD\n \n=\n \nsc\n.\nobjectFile\n[\nGeometry\n](\nhdfs://PATH\n)\n\n\n\n\n\nUse the following code to reload the indexed SpatialRDD:\n\nvar\n \nsavedRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nsavedRDD\n.\nindexedRawRDD\n \n=\n \nsc\n.\nobjectFile\n[\nSpatialIndex\n](\nhdfs://PATH\n)", 
            "title": "Write an Spatial RDD application"
        }, 
        {
            "location": "/tutorial/rdd/#set-up-dependencies", 
            "text": "Read  GeoSpark Maven Central coordinates  Select  the minimum dependencies : Add Apache Spark (only the Spark core) and GeoSpark (core).  Add the dependencies in build.sbt or pom.xml.    Note  To enjoy the full functions of GeoSpark, we suggest you include  the full dependencies :  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL ,  GeoSparkViz", 
            "title": "Set up dependencies"
        }, 
        {
            "location": "/tutorial/rdd/#initiate-sparkcontext", 
            "text": "val   conf   =   new   SparkConf ()  conf . setAppName ( GeoSparkRunnableExample )   // Change this to a proper name  conf . setMaster ( local[*] )   // Delete this if run in cluster mode  // Enable GeoSpark custom Kryo serializer  conf . set ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  conf . set ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName )  val   sc   =   new   SparkContext ( conf )    Warning  GeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.   If you add  the GeoSpark full dependencies  as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead: conf . set ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  conf . set ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName )", 
            "title": "Initiate SparkContext"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-spatialrdd", 
            "text": "", 
            "title": "Create a SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-typed-spatialrdd", 
            "text": "GeoSpark-core provdies three special SpatialRDDs:  PointRDD, PolygonRDD, and LineStringRDD . They can be loaded from CSV, TSV, WKT, WKB, Shapefiles, GeoJSON and NetCDF/HDF format.", 
            "title": "Create a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#pointrdd-from-csvtsv", 
            "text": "Suppose we have a  checkin.csv  CSV file at Path  /Download/checkin.csv  as follows: -88.331492,32.324142,hotel\n-88.175933,32.360763,gas\n-88.388954,32.357073,bar\n-88.221102,32.35078,restaurant \nThis file has three columns and corresponding  offsets (Column IDs) are 0, 1, 2.\nUse the following code to create a PointRDD  val   pointRDDInputLocation   =   /Download/checkin.csv  val   pointRDDOffset   =   0   // The point long/lat starts from Column 0  val   pointRDDSplitter   =   FileDataSplitter . CSV  val   carryOtherAttributes   =   true   // Carry Column 2 (hotel, gas, bar...)  var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDOffset ,   pointRDDSplitter ,   carryOtherAttributes )   If the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter: val   pointRDDSplitter   =   FileDataSplitter . TSV", 
            "title": "PointRDD from CSV/TSV"
        }, 
        {
            "location": "/tutorial/rdd/#polygonrddlinestringrdd-from-csvtsv", 
            "text": "In genereal, polygon and line string data is stored in WKT, WKB, GeoJSON and Shapefile formats instead of CSV/TSV because the geometris in a file may have different lengths. However, if all polygons / line strings in your CSV/TSV possess the same length, you can create PolygonRDD and LineStringRDD from these files.  Suppose we have a  checkinshape.csv  CSV file at Path  /Download/checkinshape.csv  as follows: -88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,hotel\n-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,gas\n-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,bar\n-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,restaurant  This file has 11 columns and corresponding offsets (Column IDs) are 0 - 10. Column 0 - 9 are 5 coordinates (longitude/latitude pairs). In this file, all geometries have the same number of coordinates. The geometries can be polyons or line strings.   Warning  For polygon data, the last coordinate must be the same as the first coordinate because a polygon is a closed linear ring.   Use the following code to create a PolygonRDD. val   polygonRDDInputLocation   =   /Download/checkin.csv  val   polygonRDDStartOffset   =   0   // The coordinates start from Column 0  val   polygonRDDEndOffset   =   8   // The coordinates end at Column 8  val   polygonRDDSplitter   =   FileDataSplitter . CSV  val   carryOtherAttributes   =   true   // Carry Column 10 (hotel, gas, bar...)  var   objectRDD   =   new   PolygonRDD ( sc ,   polygonRDDInputLocation ,   polygonRDDStartOffset ,   polygonRDDEndOffset ,   polygonRDDSplitter ,   carryOtherAttributes )   If the data file is in TSV format, just simply use the following line to replace the old FileDataSplitter: val   polygonRDDSplitter   =   FileDataSplitter . TSV   The way to create a LineStringRDD is the same as PolygonRDD.", 
            "title": "PolygonRDD/LineStringRDD from CSV/TSV"
        }, 
        {
            "location": "/tutorial/rdd/#from-wktwkb", 
            "text": "Geometries in a WKT and WKB file always occucpy a single column no matter how many coordinates they have. Therefore, creating a typed SpatialRDD is easy.  Suppose we have a  checkin.tsv  WKT TSV file at Path  /Download/checkin.tsv  as follows: POINT (-88.331492,32.324142)    hotel\nPOINT (-88.175933,32.360763)    gas\nPOINT (-88.388954,32.357073)    bar\nPOINT (-88.221102,32.35078) restaurant \nThis file has two columns and corresponding  offsets (Column IDs) are 0, 1. Column 0 is the WKT string and Column 1 is the checkin business type.  Use the following code to create a PointRDD  val   pointRDDInputLocation   =   /Download/checkin.csv  val   pointRDDOffset   =   0   // The WKT string starts from Column 0  val   pointRDDSplitter   =   FileDataSplitter . WKT  val   carryOtherAttributes   =   true   // Carry Column 1 (hotel, gas, bar...)  var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDOffset ,   pointRDDSplitter ,   carryOtherAttributes )  \nUse the following code to create a PolygonRDD/LineStringRDD val   polygonRDDInputLocation   =   /Download/checkin.csv  val   polygonRDDStartOffset   =   0   // The coordinates start from Column 0  val   polygonRDDEndOffset   =   0   // The coordinates end at Column 0  val   polygonRDDSplitter   =   FileDataSplitter . WKT  val   carryOtherAttributes   =   true   // Carry Column 1 (hotel, gas, bar...)  var   objectRDD   =   new   PolygonRDD ( sc ,   polygonRDDInputLocation ,   polygonRDDStartOffset ,   polygonRDDEndOffset ,   polygonRDDSplitter ,   carryOtherAttributes )", 
            "title": "From WKT/WKB"
        }, 
        {
            "location": "/tutorial/rdd/#from-geojson", 
            "text": "Geometries in GeoJSON is similar to WKT/WKB. However, a GeoJSON file must be beaked into multiple lines.  Suppose we have a  polygon.json  GeoJSON file at Path  /Download/polygon.json  as follows:  {  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  077 ,  TRACTCE :  011501 ,  BLKGRPCE :  5 ,  AFFGEOID :  1500000US010770115015 ,  GEOID :  010770115015 ,  NAME :  5 ,  LSAD :  BG ,  ALAND : 6844991,  AWATER : 32636 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  045 ,  TRACTCE :  021102 ,  BLKGRPCE :  4 ,  AFFGEOID :  1500000US010450211024 ,  GEOID :  010450211024 ,  NAME :  4 ,  LSAD :  BG ,  ALAND : 11360854,  AWATER : 0 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  055 ,  TRACTCE :  001300 ,  BLKGRPCE :  3 ,  AFFGEOID :  1500000US010550013003 ,  GEOID :  010550013003 ,  NAME :  3 ,  LSAD :  BG ,  ALAND : 1378742,  AWATER : 247387 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },\n{  type :  Feature ,  properties : {  STATEFP :  01 ,  COUNTYFP :  089 ,  TRACTCE :  001700 ,  BLKGRPCE :  2 ,  AFFGEOID :  1500000US010890017002 ,  GEOID :  010890017002 ,  NAME :  2 ,  LSAD :  BG ,  ALAND : 1040641,  AWATER : 0 },  geometry : {  type :  Polygon ,  coordinates : [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },  Use the following code to create a typed SpatialRDD (PointRDD/PolygonRDD/LineStringRDD): val   pointRDDInputLocation   =   /Download/polygon.json  val   pointRDDSplitter   =   FileDataSplitter . GEOJSON  val   carryOtherAttributes   =   true   // Carry Column 1 (hotel, gas, bar...)  var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDSplitter ,   carryOtherAttributes )    Warning  The way that GeoSpark reads GeoJSON is different from that in SparkSQL", 
            "title": "From GeoJSON"
        }, 
        {
            "location": "/tutorial/rdd/#create-a-generic-spatialrdd", 
            "text": "A generic SpatialRDD is not typed to a certain geometry type and open to more scenarios. It allows an input data file contains mixed types of geometries. For instace, a WKT file contains three types gemetries  LineString ,  Polygon  and  MultiPolygon .", 
            "title": "Create a generic SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#from-all-formats", 
            "text": "To create a generic SpatialRDD from CSV, TSV, WKT, WKB and GeoJSON input formats, you have to use GeoSparkSQL. Make sure you include  the full dependencies  of GeoSpark. Read  GeoSparkSQL API .  We use  checkin.csv CSV file  as the example. You can create a generic SpatialRDD using the following steps:   Load data in GeoSparkSQL. var   df   =   sparkSession . read . format ( csv ). option ( delimiter ,   \\t ). option ( header ,   false ). load ( csvPointInputLocation )  df . createOrReplaceTempView ( inputtable )   Create a Geometry type column in GeoSparkSQL var   spatialDf   =   sparkSession . sql ( \n              |SELECT ST_Point(CAST(inputtable._c0 AS Decimal(24,20)),CAST(inputtable._c1 AS Decimal(24,20))) AS checkin          |FROM inputtable       . stripMargin )   Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   Adapter . toRdd ( spatialDf )    For WKT/WKB/GeoJSON data, please use  ST_GeomFromWKT / ST_GeomFromWKB / ST_GeomFromGeoJSON  instead.", 
            "title": "From all formats"
        }, 
        {
            "location": "/tutorial/rdd/#from-shapefile", 
            "text": "val   shapefileInputLocation = /Download/myshapefile  var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   ShapefileReader . readToGeometryRDD ( sparkSession . sparkContext ,   shapefileInputLocation )    Note  The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called  myShapefile , the file structure should be like this: - shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...", 
            "title": "From Shapefile"
        }, 
        {
            "location": "/tutorial/rdd/#transform-the-coordinate-reference-system", 
            "text": "GeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in an SpatialRDD. The unit of all related distances in GeoSpark is same as the unit of all geometries in an SpatialRDD.  To convert Coordinate Reference System of an SpatialRDD, use the following code:  val   sourceCrsCode   =   epsg:4326   // WGS84, the most common degree-based CRS  val   targetCrsCode   =   epsg:3857   // The most common meter-based CRS  objectRDD . CRSTransform ( sourceCrsCode ,   targetCrsCode )    Warning  CRS transformation should be done right after creating each SpatialRDD, otherwise it will lead to wrong query results. For instace, use something like this: var   objectRDD   =   new   PointRDD ( sc ,   pointRDDInputLocation ,   pointRDDOffset ,   pointRDDSplitter ,   carryOtherAttributes )  objectRDD . CRSTransform ( epsg:4326 ,   epsg:3857 )    The details CRS information can be found on  EPSG.io", 
            "title": "Transform the Coordinate Reference System"
        }, 
        {
            "location": "/tutorial/rdd/#read-other-attributes-in-an-spatialrdd", 
            "text": "Each SpatialRDD can carry non-spatial attributes such as price, age and name as long as the user sets  carryOtherAttributes  as  TRUE .  The other attributes are combined together to a string and stored in  UserData  field of each geometry.  To retrieve the UserData field, use the following code: val   rddWithOtherAttributes   =   objectRDD . rawSpatialRDD . rdd . map [ String ]( f = f . getUserData . asInstanceOf [ String ])", 
            "title": "Read other attributes in an SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-range-query", 
            "text": "A spatial range query takes as input a range query window and an SpatialRDD and returns all geometries that intersect / are fully covered by the query window.  Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial Range Query on it.  val   rangeQueryWindow   =   new   Envelope (- 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by the window  val   usingIndex   =   false  var   queryResult   =   RangeQuery . SpatialRangeQuery ( spatialRDD ,   rangeQueryWindow ,   considerBoundaryIntersection ,   usingIndex )   considerBoundaryIntersection  can be set to TRUE to return all geometries intersect with query window.   Note  Spatial range query is equal to  ST_Within  and  ST_Intersects  in Spatial SQL. An example query is as follows: SELECT   *  FROM   checkin  WHERE   ST_Intersects ( queryWindow ,   checkin . location )", 
            "title": "Write a Spatial Range Query"
        }, 
        {
            "location": "/tutorial/rdd/#range-query-window", 
            "text": "Besides the rectangle (Envelope) type range query window, GeoSpark range query window can be Point/Polygon/LineString.  The code to create a point is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))   The code to create a polygon (with 4 vertexes) is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   coordinates   =   new   Array [ Coordinate ]( 5 )  coordinates ( 0 )   =   new   Coordinate ( 0 , 0 )  coordinates ( 1 )   =   new   Coordinate ( 0 , 4 )  coordinates ( 2 )   =   new   Coordinate ( 4 , 4 )  coordinates ( 3 )   =   new   Coordinate ( 4 , 0 )  coordinates ( 4 )   =   coordinates ( 0 )   // The last coordinate is the same as the first coordinate in order to compose a closed ring  val   polygonObject   =   geometryFactory . createPolygon ( coordinates )   The code to create a line string (with 4 vertexes) is as follows:  val   geometryFactory   =   new   GeometryFactory ()  val   coordinates   =   new   Array [ Coordinate ]( 5 )  coordinates ( 0 )   =   new   Coordinate ( 0 , 0 )  coordinates ( 1 )   =   new   Coordinate ( 0 , 4 )  coordinates ( 2 )   =   new   Coordinate ( 4 , 4 )  coordinates ( 3 )   =   new   Coordinate ( 4 , 0 )  val   linestringObject   =   geometryFactory . createLineString ( coordinates )", 
            "title": "Range query window"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes", 
            "text": "GeoSpark provides two types of spatial indexes, Quad-Tree and R-Tree. Once you specify an index type, GeoSpark will build a local tree index on each of the SpatialRDD partition.  To utilize a spatial index in a spatial range query, use the following code:  val   rangeQueryWindow   =   new   Envelope (- 90.01 ,   - 80.01 ,   30.01 ,   40.01 )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by the window  val   buildOnSpatialPartitionedRDD   =   false   // Set to TRUE only if run join query  spatialRDD . buildIndex ( IndexType . QUADTREE ,   buildOnSpatialPartitionedRDD )  val   usingIndex   =   true  var   queryResult   =   RangeQuery . SpatialRangeQuery ( spatialRDD ,   rangeQueryWindow ,   considerBoundaryIntersection ,   usingIndex )    Tip  Using an index might not be the best choice all the time because building index also takes time. A spatial index is very useful when your data is complex polygons and line strings.", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format", 
            "text": "The output format of the spatial range query is another SpatialRDD.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-knn-query", 
            "text": "A spatial K Nearnest Neighbor query takes as input a K, a query point and an SpatialRDD and finds the K geometries in the RDD which are the closest to he query point.  Assume you now have an SpatialRDD (typed or generic). You can use the following code to issue an Spatial KNN Query on it.  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))  val   K   =   1000   // K Nearest Neighbors  val   usingIndex   =   false  val   result   =   KNNQuery . SpatialKnnQuery ( objectRDD ,   pointObject ,   K ,   usingIndex )    Note  Spatial KNN query that returns 5 Nearest Neighbors is equal to the following statement in Spatial SQL SELECT   ck . name ,   ck . rating ,   ST_Distance ( ck . location ,   myLocation )   AS   distance  FROM   checkins   ck  ORDER   BY   distance   DESC  LIMIT   5", 
            "title": "Write a Spatial KNN Query"
        }, 
        {
            "location": "/tutorial/rdd/#query-center-geometry", 
            "text": "Besides the Point type, GeoSpark KNN query center can be Polygon and LineString.  To learn how to create Polygon and LineString object, see  Range query window .", 
            "title": "Query center geometry"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes_1", 
            "text": "To utilize a spatial index in a spatial KNN query, use the following code:  val   geometryFactory   =   new   GeometryFactory ()  val   pointObject   =   geometryFactory . createPoint ( new   Coordinate (- 84.01 ,   34.01 ))  val   K   =   1000   // K Nearest Neighbors  val   buildOnSpatialPartitionedRDD   =   false   // Set to TRUE only if run join query  spatialRDD . buildIndex ( IndexType . RTREE ,   buildOnSpatialPartitionedRDD )  val   usingIndex   =   true  val   result   =   KNNQuery . SpatialKnnQuery ( objectRDD ,   pointObject ,   K ,   usingIndex )    Warning  Only R-Tree index supports Spatial KNN query", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format_1", 
            "text": "The output format of the spatial KNN query is a list of geometries. The list has K geometry objects.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-spatial-join-query", 
            "text": "A spatial join query takes as input two Spatial RDD A and B. For each geometry in A, finds the geometries (from B) covered/intersected by it. A and B can be any geometry type and are not necessary to have the same geometry type.  Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Spatial Join Query on them.  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by each query window in queryWindowRDD  val   usingIndex   =   false  objectRDD . analyze ()  objectRDD . spatialPartitioning ( GridType . KDBTREE )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )  val   result   =   JoinQuery . SpatialJoinQuery ( objectRDD ,   queryWindowRDD ,   usingIndex ,   considerBoundaryIntersection )    Note  Spatial join query is equal to the following query in Spatial SQL: SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Contains ( city . geom ,   superhero . geom );  \nFind the super heros in each city", 
            "title": "Write a Spatial Join Query"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-partitioning", 
            "text": "GeoSpark spatial partitioning method can significantly speed up the join query. Three spatial partitioning methods are available: KDB-Tree, Quad-Tree and R-Tree. Two SpatialRDD must be partitioned by the same way.  If you first partition SpatialRDD A, then you must use the partitioner of A to partition B.  objectRDD . spatialPartitioning ( GridType . KDBTREE )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )   Or   queryWindowRDD . spatialPartitioning ( GridType . KDBTREE )  objectRDD . spatialPartitioning ( queryWindowRDD . getPartitioner )", 
            "title": "Use spatial partitioning"
        }, 
        {
            "location": "/tutorial/rdd/#use-spatial-indexes_2", 
            "text": "To utilize a spatial index in a spatial join query, use the following code:  objectRDD . spatialPartitioning ( joinQueryPartitioningType )  queryWindowRDD . spatialPartitioning ( objectRDD . getPartitioner )  val   buildOnSpatialPartitionedRDD   =   true   // Set to TRUE only if run join query  val   usingIndex   =   true  queryWindowRDD . buildIndex ( IndexType . QUADTREE ,   buildOnSpatialPartitionedRDD )  val   result   =   JoinQuery . SpatialJoinQueryFlat ( objectRDD ,   queryWindowRDD ,   usingIndex ,   considerBoundaryIntersection )   The index should be built on either one of two SpatialRDDs. In general, you should build it on the larger SpatialRDD.", 
            "title": "Use spatial indexes"
        }, 
        {
            "location": "/tutorial/rdd/#output-format_2", 
            "text": "The output format of the spatial join query is a PairRDD. In this PairRDD, each object is a pair of two geometries. The left one is the geometry from objectRDD and the right one is the geometry from the queryWindowRDD.  Point,Polygon\nPoint,Polygon\nPoint,Polygon\nPolygon,Polygon\nLineString,LineString\nPolygon,LineString\n...  Each object on the left is covered/intersected by the object on the right.", 
            "title": "Output format"
        }, 
        {
            "location": "/tutorial/rdd/#write-a-distance-join-query", 
            "text": "A distance join query takes as input two Spatial RDD A and B and a distance. For each geometry in A, finds the geometries (from B) are within the given distance to it. A and B can be any geometry type and are not necessary to have the same geometry type. The unit of the distance is explained  here .  Assume you now have two SpatialRDDs (typed or generic). You can use the following code to issue an Distance Join Query on them.  objectRddA . analyze ()  val   circleRDD   =   new   CircleRDD ( objectRddA ,   0.1 )   // Create a CircleRDD using the given distance  circleRDD . spatialPartitioning ( GridType . KDBTREE )  objectRddB . spatialPartitioning ( circleRDD . getPartitioner )  val   considerBoundaryIntersection   =   false   // Only return gemeotries fully covered by each query window in queryWindowRDD  val   usingIndex   =   false  val   result   =   JoinQuery . DistanceJoinQueryFlat ( objectRddB ,   circleRDD ,   usingIndex ,   considerBoundaryIntersection )   The rest part of the join query is same as the spatial join query.  The details of spatial partitioning in join query is  here .  The details of using spatial indexes in join query is  here .  The output format of the distance join query is  here .   Note  Distance join query is equal to the following query in Spatial SQL: SELECT   superhero . name  FROM   city ,   superhero  WHERE   ST_Distance ( city . geom ,   superhero . geom )   =   10 ;  \nFind the super heros within 10 miles of each city", 
            "title": "Write a Distance Join Query"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-permanent-storage", 
            "text": "You can always save an SpatialRDD back to some permanent storage such as HDFS and Amazon S3. You can save distributed SpatialRDD to WKT, GeoJSON and object files.   Note  Non-spatial attributes such as price, age and name will also be stored to permanent storage.", 
            "title": "Save to permanent storage"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-not-indexed", 
            "text": "Typed SpatialRDD and generic SpatialRDD can be saved to permanent storage.", 
            "title": "Save an SpatialRDD (not indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-wkt-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed WKT text file:  objectRDD . rawSpatialRDD . saveAsTextFile ( hdfs://PATH )", 
            "title": "Save to distributed WKT text file"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-geojson-text-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed GeoJSON text file:  objectRDD . saveAsGeoJSON ( hdfs://PATH )", 
            "title": "Save to distributed GeoJSON text file"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-object-file", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  objectRDD . rawSpatialRDD . saveAsObjectFile ( hdfs://PATH )    Note  Each object in a distributed object file is a byte array (not human-readable). This byte array is the serialized format of a Geometry or a SpatialIndex.", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-indexed", 
            "text": "Indexed typed SpatialRDD and generic SpatialRDD can be saved to permanent storage. However, the indexed SpatialRDD has to be stored as a distributed object file.", 
            "title": "Save an SpatialRDD (indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#save-to-distributed-object-file_1", 
            "text": "Use the following code to save an SpatialRDD as a distributed object file:  objectRDD.indexedRawRDD.saveAsObjectFile( hdfs://PATH )", 
            "title": "Save to distributed object file"
        }, 
        {
            "location": "/tutorial/rdd/#save-an-spatialrdd-spatialpartitioned-wo-indexed", 
            "text": "A spatial partitioned RDD can be saved to permanent storage but Spark is not able to maintain the same RDD partition Id of the original RDD. This will lead to wrong join query results. We are working on some solutions. Stay tuned!", 
            "title": "Save an SpatialRDD (spatialPartitioned W/O indexed)"
        }, 
        {
            "location": "/tutorial/rdd/#reload-a-saved-spatialrdd", 
            "text": "You can easily reload an SpatialRDD that has been saved to  a distributed object file .", 
            "title": "Reload a saved SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#load-to-a-typed-spatialrdd", 
            "text": "Use the following code to reload the PointRDD/PolygonRDD/LineStringRDD:  var   savedRDD   =   new   PointRDD ( sc . objectFile [ Point ]( hdfs://PATH ))  var   savedRDD   =   new   PointRDD ( sc . objectFile [ Polygon ]( hdfs://PATH ))  var   savedRDD   =   new   PointRDD ( sc . objectFile [ LineString ]( hdfs://PATH ))", 
            "title": "Load to a typed SpatialRDD"
        }, 
        {
            "location": "/tutorial/rdd/#load-to-a-generic-spatialrdd", 
            "text": "Use the following code to reload the SpatialRDD:  var   savedRDD   =   new   SpatialRDD [ Geometry ]  savedRDD . rawSpatialRDD   =   sc . objectFile [ Geometry ]( hdfs://PATH )   Use the following code to reload the indexed SpatialRDD: var   savedRDD   =   new   SpatialRDD [ Geometry ]  savedRDD . indexedRawRDD   =   sc . objectFile [ SpatialIndex ]( hdfs://PATH )", 
            "title": "Load to a generic SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/", 
            "text": "The page outlines the steps to manage spatial data using GeoSparkSQL. \nThe example code is written in Scala but also works for Java\n.\n\n\nGeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:\n\nvar\n \nmyDataFrame\n \n=\n \nsparkSession\n.\nsql\n(\nYOUR_SQL\n)\n\n\n\n\nDetailed GeoSparkSQL APIs are available here: \nGeoSparkSQL API\n\n\nSet up dependencies\n\n\n\n\nRead \nGeoSpark Maven Central coordinates\n\n\nSelect \nthe minimum dependencies\n: Add \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n\n\nAdd the dependencies in build.sbt or pom.xml.\n\n\n\n\n\n\nNote\n\n\nTo enjoy the full functions of GeoSpark, we suggest you include \nthe full dependencies\n: \nApache Spark core\n, \nApache SparkSQL\n, \nGeoSpark core\n, \nGeoSparkSQL\n, \nGeoSparkViz\n\n\n\n\nInitiate SparkSession\n\n\nUse the following code to initiate your SparkSession at the beginning:\n\nvar\n \nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n()\n\n\n.\nmaster\n(\nlocal[*]\n)\n \n// Delete this if run in cluster mode\n\n\n.\nappName\n(\nreadTestScala\n)\n \n// Change this to a proper name\n\n\n// Enable GeoSpark custom Kryo serializer\n\n\n.\nconfig\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n)\n\n\n.\ngetOrCreate\n()\n\n\n\n\n\n\nWarning\n\n\nGeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.\n\n\n\n\nIf you add \nthe GeoSpark full dependencies\n as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead:\n\n.\nconfig\n(\nspark.serializer\n,\n \nclassOf\n[\nKryoSerializer\n].\ngetName\n)\n\n\n.\nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkVizKryoRegistrator\n].\ngetName\n)\n\n\n\n\nRegister GeoSparkSQL\n\n\nAdd the following line after your SparkSession declaration\n\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)\n\n\n\n\n\nThis function will register GeoSpark User Defined Type, User Defined Function and optimized join query strategy.\n\n\nLoad data from files\n\n\nAssume we have a WKT file, namely \nusa-county.tsv\n, at Path \n/Download/usa-county.tsv\n as follows:\n\n\nPOLYGON (..., ...)  Cuming County   \nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County\n\n\nThe file may have many other columns.\n\n\nUse the following code to load the data and create a raw DataFrame:\n\n\nvar\n \nrawDf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\ndelimiter\n,\n \n\\t\n).\noption\n(\nheader\n,\n \nfalse\n).\nload\n(\n/Download/usa-county.tsv\n)\n\n\nrawDf\n.\ncreateOrReplaceTempView\n(\nrawdf\n)\n\n\nrawDf\n.\nshow\n()\n\n\n\n\n\nThe output will be like this:\n\n\n|                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n\n\n\n\nCreate a Geometry type column\n\n\nAll geometrical operations in GeoSparkSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.\n\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2\n\n\n    |FROM rawdf\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nYou can select many other attributes to compose this \nspatialdDf\n. The output will be something like this:\n\n\n|                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|\n\n\n\n\nAlthough it looks same with the input, but actually the type of column countyshape has been changed to \nGeometry\n type.\n\n\nTo verify this, use the following code to print the schema of the DataFrame:\n\n\nspatialDf\n.\nprintSchema\n()\n\n\n\n\n\nThe output will be like this:\n\n\nroot\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)\n\n\n\n\n\n\nNote\n\n\nGeoSparkSQL provides more than 10 different functions to create a Geometry column, please read \nGeoSparkSQL constructor API\n.\n\n\n\n\nTransform the Coordinate Reference System\n\n\nGeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in GeoSparkSQL is same as the unit of all geometries in a Geometry column.\n\n\nTo convert Coordinate Reference System of the Geometry column created before, use the following code:\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_Transform(countyshape, \nepsg:4326\n, \nepsg:3857\n) AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7\n\n\n    |FROM spatialdf\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nThe first EPSG code EPSG:4326 in \nST_Transform\n is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.\n\n\nThe second EPSG code EPSG:3857 in \nST_Transform\n is the target CRS of the geometries. It is the most common meter-based CRS.\n\n\nThis \nST_Transform\n transform the CRS of these geomtries from EPSG:4326 to EPSG:3857. The details CRS information can be found on \nEPSG.io\n\n\nThe coordinates of polygons have been changed. The output will be like this:\n\n\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|\n\n\n\n\nRun spatial queries\n\n\nAfter creating a Geometry type column, you are able to run spatial queries.\n\n\nRange query\n\n\nUse \nST_Contains\n, \nST_Intersects\n, \nST_Within\n to run a range query over a single column.\n\n\nThe following example finds all counties that are within the given polygon:\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT *\n\n\n    |FROM spatialdf\n\n\n    |WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\n\n\nNote\n\n\nRead \nGeoSparkSQL constructor API\n to learn how to create a Geometry type query window\n\n\n\n\nKNN query\n\n\nUse \nST_Distance\n to calculate the distance and rank the distance.\n\n\nThe following code returns the 5 nearest neighbor of the given polygon.\n\n\nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance\n\n\n    |FROM spatialdf\n\n\n    |ORDER BY distance DESC\n\n\n    |LIMIT 5\n\n\n  \n.\nstripMargin\n)\n\n\nspatialDf\n.\ncreateOrReplaceTempView\n(\nspatialdf\n)\n\n\nspatialDf\n.\nshow\n()\n\n\n\n\n\nJoin query\n\n\nThe details of a join query is available here \nJoin query\n.\n\n\nOther queries\n\n\nThere are lots of other functions can be combined with these queries. Please read \nGeoSparkSQL functions\n and \nGeoSparkSQL aggregate functions\n.\n\n\nSave to permanent storage\n\n\nTo save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.\n\n\nUse the following code to convert the Geometry column in a DataFrame back to a WKT string column:\n\nsparkSession\n.\nudf\n.\nregister\n(\nST_SaveAsWKT\n,\n \n(\ngeometry\n:\n \nGeometry\n)\n \n=\n \n(\ngeometry\n.\ntoText\n))\n\n\nvar\n \nstringDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n  \n\n\n    |SELECT ST_SaveAsWKT(countyshape)\n\n\n    |FROM polygondf\n\n\n  \n.\nstripMargin\n)\n\n\n\n\n\n\nNote\n\n\nWe are working on providing more user-friendly output functions such as \nST_SaveAsWKT\n and \nST_SaveAsWKB\n. Stay tuned!\n\n\n\n\nTo load the DataFrame back, you first use the regular method to load the saved string DataFrame from the permanent storage and use \nST_GeomFromWKT\n to re-build the Geometry type column.\n\n\nConvert between DataFrame and SpatialRDD\n\n\nDataFrame to SpatialRDD\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nAdapter\n.\ntoRdd\n(\nspatialDf\n)\n\n\n\n\n\n\nWarning\n\n\nGeometry must be the first column in the DataFrame and only one Geometry type column is allowed per DataFrame.\n\n\n\n\n\n\nNote\n\n\nOther non-spatial columns can also be brought to SpatialRDD using the UUIDs. Please read \nGeoSparkSQL constructor API\n.\n\n\n\n\nSpatialRDD to DataFrame\n\n\nUse GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD\n\nvar\n \nspatialDf\n \n=\n \nAdapter\n.\ntoDf\n(\nspatialRDD\n,\n \nsparkSession\n)\n\n\n\n\nAll other attributes such as price and age will be also brought to the DataFrame as long as you specify \ncarryOtherAttributes\n (see \nRead other attributes in an SpatialRDD\n).\n\n\nSpatialPairRDD to DataFrame\n\n\nPairRDD is the result of a spatial join query or distance join query. GeoSparkSQL DataFrame-RDD Adapter can convert the result to a DataFrame:\n\n\nvar\n \njoinResultDf\n \n=\n \nAdapter\n.\ntoDf\n(\njoinResultPairRDD\n,\n \nsparkSession\n)\n\n\n\n\n\nAll other attributes such as price and age will be also brought to the DataFrame as long as you specify \ncarryOtherAttributes\n (see \nRead other attributes in an SpatialRDD\n).", 
            "title": "Write an Spatial SQL/DataFrame application"
        }, 
        {
            "location": "/tutorial/sql/#set-up-dependencies", 
            "text": "Read  GeoSpark Maven Central coordinates  Select  the minimum dependencies : Add  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL  Add the dependencies in build.sbt or pom.xml.    Note  To enjoy the full functions of GeoSpark, we suggest you include  the full dependencies :  Apache Spark core ,  Apache SparkSQL ,  GeoSpark core ,  GeoSparkSQL ,  GeoSparkViz", 
            "title": "Set up dependencies"
        }, 
        {
            "location": "/tutorial/sql/#initiate-sparksession", 
            "text": "Use the following code to initiate your SparkSession at the beginning: var   sparkSession   =   SparkSession . builder ()  . master ( local[*] )   // Delete this if run in cluster mode  . appName ( readTestScala )   // Change this to a proper name  // Enable GeoSpark custom Kryo serializer  . config ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  . config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName )  . getOrCreate ()    Warning  GeoSpark has a suite of well-written geometry and index serializers. Forgetting to enable these serializers will lead to high memory consumption.   If you add  the GeoSpark full dependencies  as suggested above, please use the following two lines to enable GeoSpark Kryo serializer instead: . config ( spark.serializer ,   classOf [ KryoSerializer ]. getName )  . config ( spark.kryo.registrator ,   classOf [ GeoSparkVizKryoRegistrator ]. getName )", 
            "title": "Initiate SparkSession"
        }, 
        {
            "location": "/tutorial/sql/#register-geosparksql", 
            "text": "Add the following line after your SparkSession declaration  GeoSparkSQLRegistrator . registerAll ( sparkSession )   This function will register GeoSpark User Defined Type, User Defined Function and optimized join query strategy.", 
            "title": "Register GeoSparkSQL"
        }, 
        {
            "location": "/tutorial/sql/#load-data-from-files", 
            "text": "Assume we have a WKT file, namely  usa-county.tsv , at Path  /Download/usa-county.tsv  as follows:  POLYGON (..., ...)  Cuming County   \nPOLYGON (..., ...)  Wahkiakum County\nPOLYGON (..., ...)  De Baca County\nPOLYGON (..., ...)  Lancaster County \nThe file may have many other columns.  Use the following code to load the data and create a raw DataFrame:  var   rawDf   =   sparkSession . read . format ( csv ). option ( delimiter ,   \\t ). option ( header ,   false ). load ( /Download/usa-county.tsv )  rawDf . createOrReplaceTempView ( rawdf )  rawDf . show ()   The output will be like this:  |                 _c0|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|", 
            "title": "Load data from files"
        }, 
        {
            "location": "/tutorial/sql/#create-a-geometry-type-column", 
            "text": "All geometrical operations in GeoSparkSQL are on Geometry type objects. Therefore, before any kind of queries, you need to create a Geometry type column on a DataFrame.  var   spatialDf   =   sparkSession . sql ( \n        |SELECT ST_GeomFromWKT(_c0) AS countyshape, _c1, _c2      |FROM rawdf     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()   You can select many other attributes to compose this  spatialdDf . The output will be something like this:  |                 countyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|_c8|  _c9|_c10| _c11|_c12|_c13|      _c14|    _c15|       _c16|        _c17|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+---+-----+----+-----+----+----+----------+--------+-----------+------------+\n|POLYGON ((-97.019...| 31|039|00835841|31039|     Cuming|       Cuming County| 06| H1|G4020|null| null|null|   A|1477895811|10447360|+41.9158651|-096.7885168|\n|POLYGON ((-123.43...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06| H1|G4020|null| null|null|   A| 682138871|61658258|+46.2946377|-123.4244583|\n|POLYGON ((-104.56...| 35|011|00933054|35011|    De Baca|      De Baca County| 06| H1|G4020|null| null|null|   A|6015539696|29159492|+34.3592729|-104.3686961|\n|POLYGON ((-96.910...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06| H1|G4020| 339|30700|null|   A|2169240202|22877180|+40.7835474|-096.6886584|  Although it looks same with the input, but actually the type of column countyshape has been changed to  Geometry  type.  To verify this, use the following code to print the schema of the DataFrame:  spatialDf . printSchema ()   The output will be like this:  root\n |-- countyshape: geometry (nullable = false)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n |-- _c6: string (nullable = true)\n |-- _c7: string (nullable = true)   Note  GeoSparkSQL provides more than 10 different functions to create a Geometry column, please read  GeoSparkSQL constructor API .", 
            "title": "Create a Geometry type column"
        }, 
        {
            "location": "/tutorial/sql/#transform-the-coordinate-reference-system", 
            "text": "GeoSpark doesn't control the coordinate unit (degree-based or meter-based) of all geometries in a Geometry column. The unit of all related distances in GeoSparkSQL is same as the unit of all geometries in a Geometry column.  To convert Coordinate Reference System of the Geometry column created before, use the following code:  spatialDf   =   sparkSession . sql ( \n        |SELECT ST_Transform(countyshape,  epsg:4326 ,  epsg:3857 ) AS newcountyshape, _c1, _c2, _c3, _c4, _c5, _c6, _c7      |FROM spatialdf     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()   The first EPSG code EPSG:4326 in  ST_Transform  is the source CRS of the geometries. It is WGS84, the most common degree-based CRS.  The second EPSG code EPSG:3857 in  ST_Transform  is the target CRS of the geometries. It is the most common meter-based CRS.  This  ST_Transform  transform the CRS of these geomtries from EPSG:4326 to EPSG:3857. The details CRS information can be found on  EPSG.io  The coordinates of polygons have been changed. The output will be like this:  +--------------------+---+---+--------+-----+-----------+--------------------+---+\n|      newcountyshape|_c1|_c2|     _c3|  _c4|        _c5|                 _c6|_c7|\n+--------------------+---+---+--------+-----+-----------+--------------------+---+\n|POLYGON ((-108001...| 31|039|00835841|31039|     Cuming|       Cuming County| 06|\n|POLYGON ((-137408...| 53|069|01513275|53069|  Wahkiakum|    Wahkiakum County| 06|\n|POLYGON ((-116403...| 35|011|00933054|35011|    De Baca|      De Baca County| 06|\n|POLYGON ((-107880...| 31|109|00835876|31109|  Lancaster|    Lancaster County| 06|", 
            "title": "Transform the Coordinate Reference System"
        }, 
        {
            "location": "/tutorial/sql/#run-spatial-queries", 
            "text": "After creating a Geometry type column, you are able to run spatial queries.", 
            "title": "Run spatial queries"
        }, 
        {
            "location": "/tutorial/sql/#range-query", 
            "text": "Use  ST_Contains ,  ST_Intersects ,  ST_Within  to run a range query over a single column.  The following example finds all counties that are within the given polygon:  spatialDf   =   sparkSession . sql ( \n        |SELECT *      |FROM spatialdf      |WHERE ST_Contains (ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape)     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()    Note  Read  GeoSparkSQL constructor API  to learn how to create a Geometry type query window", 
            "title": "Range query"
        }, 
        {
            "location": "/tutorial/sql/#knn-query", 
            "text": "Use  ST_Distance  to calculate the distance and rank the distance.  The following code returns the 5 nearest neighbor of the given polygon.  spatialDf   =   sparkSession . sql ( \n        |SELECT countyname, ST_Distance(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), newcountyshape) AS distance      |FROM spatialdf      |ORDER BY distance DESC      |LIMIT 5     . stripMargin )  spatialDf . createOrReplaceTempView ( spatialdf )  spatialDf . show ()", 
            "title": "KNN query"
        }, 
        {
            "location": "/tutorial/sql/#join-query", 
            "text": "The details of a join query is available here  Join query .", 
            "title": "Join query"
        }, 
        {
            "location": "/tutorial/sql/#other-queries", 
            "text": "There are lots of other functions can be combined with these queries. Please read  GeoSparkSQL functions  and  GeoSparkSQL aggregate functions .", 
            "title": "Other queries"
        }, 
        {
            "location": "/tutorial/sql/#save-to-permanent-storage", 
            "text": "To save a Spatial DataFrame to some permanent storage such as Hive tables and HDFS, you can simply convert each geometry in the Geometry type column back to a plain String and save the plain DataFrame to wherever you want.  Use the following code to convert the Geometry column in a DataFrame back to a WKT string column: sparkSession . udf . register ( ST_SaveAsWKT ,   ( geometry :   Geometry )   =   ( geometry . toText ))  var   stringDf   =   sparkSession . sql ( \n        |SELECT ST_SaveAsWKT(countyshape)      |FROM polygondf     . stripMargin )    Note  We are working on providing more user-friendly output functions such as  ST_SaveAsWKT  and  ST_SaveAsWKB . Stay tuned!   To load the DataFrame back, you first use the regular method to load the saved string DataFrame from the permanent storage and use  ST_GeomFromWKT  to re-build the Geometry type column.", 
            "title": "Save to permanent storage"
        }, 
        {
            "location": "/tutorial/sql/#convert-between-dataframe-and-spatialrdd", 
            "text": "", 
            "title": "Convert between DataFrame and SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/#dataframe-to-spatialrdd", 
            "text": "Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   Adapter . toRdd ( spatialDf )    Warning  Geometry must be the first column in the DataFrame and only one Geometry type column is allowed per DataFrame.    Note  Other non-spatial columns can also be brought to SpatialRDD using the UUIDs. Please read  GeoSparkSQL constructor API .", 
            "title": "DataFrame to SpatialRDD"
        }, 
        {
            "location": "/tutorial/sql/#spatialrdd-to-dataframe", 
            "text": "Use GeoSparkSQL DataFrame-RDD Adapter to convert a DataFrame to an SpatialRDD var   spatialDf   =   Adapter . toDf ( spatialRDD ,   sparkSession )   All other attributes such as price and age will be also brought to the DataFrame as long as you specify  carryOtherAttributes  (see  Read other attributes in an SpatialRDD ).", 
            "title": "SpatialRDD to DataFrame"
        }, 
        {
            "location": "/tutorial/sql/#spatialpairrdd-to-dataframe", 
            "text": "PairRDD is the result of a spatial join query or distance join query. GeoSparkSQL DataFrame-RDD Adapter can convert the result to a DataFrame:  var   joinResultDf   =   Adapter . toDf ( joinResultPairRDD ,   sparkSession )   All other attributes such as price and age will be also brought to the DataFrame as long as you specify  carryOtherAttributes  (see  Read other attributes in an SpatialRDD ).", 
            "title": "SpatialPairRDD to DataFrame"
        }, 
        {
            "location": "/tutorial/viz/", 
            "text": "Coming soon...\n\n\nFor now, please read \nGeoSparkViz JavaDoc\n and \nGeoSparkViz Template Project\n.", 
            "title": "Visualize an Spatial RDD"
        }, 
        {
            "location": "/tutorial/GeoSpark-Runnable-DEMO/", 
            "text": "GeoSpark Template Project contains six template projects for GeoSpark, GeoSparkSQL and GeoSparkViz. The template projects have been configured properly. You are able to compile, package, and run the code locally \nwithout any extra coding\n.\n\n\nScala/Java Template Project", 
            "title": "Template project"
        }, 
        {
            "location": "/tutorial/faq/", 
            "text": "Coming soon...\n\n\nFor now, please read \nGeoSpark Github FAQ Issues\n.", 
            "title": "Frequently Asked Questions (FAQ)"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/", 
            "text": "Advanced tutorial: Tune your GeoSpark RDD application\n\n\nBefore getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.\n\n\nPick a proper GeoSpark version\n\n\nThe versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.\n\n\nThe first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.\n\n\nThe second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read \nGeoSpark version release notes\n and make sure you are ready to accept the API changes.\n\n\nThe third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.\n\n\nChoose a proper Spatial RDD constructor\n\n\nGeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.\n\n\n\n\nInitialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:\n\npublic\n \nPointRDD\n(\nJavaSparkContext\n \nsparkContext\n,\n \nString\n \nInputLocation\n,\n \nInteger\n \nOffset\n,\n \nFileDataSplitter\n \nsplitter\n,\n \nboolean\n \ncarryInputData\n,\n \nInteger\n \npartitions\n,\n \nStorageLevel\n \nnewLevel\n)\n\n\n\n\nInitialize a SpatialRDD from an existing RDD. A typical example is as follows:\n\npublic\n \nPointRDD\n(\nJavaRDD\nPoint\n \nrawSpatialRDD\n,\n \nStorageLevel\n \nnewLevel\n)\n\n\n\n\n\n\nYou may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.\n\n\nHowever, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:\n\n\npublic\n \nPointRDD\n(\nJavaSparkContext\n \nsparkContext\n,\n \nString\n \nInputLocation\n,\n \nInteger\n \nOffset\n,\n \nFileDataSplitter\n \nsplitter\n,\n \nboolean\n \ncarryInputData\n,\n \nInteger\n \npartitions\n,\n \nEnvelope\n \ndatasetBoundary\n,\n \nInteger\n \napproximateTotalCount\n)\n \n{\n\n\n\nManually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.\n\n\nCache the Spatial RDD that is repeatedly used\n\n\nEach SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:\n\n\n\n\nrawSpatialRDD: The RDD generated by SpatialRDD constructors.\n\n\nspatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.\n\n\nindexedRawRDD: The RDD generated by indexing a rawSpatialRDD.\n\n\nindexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.\n\n\n\n\nThese four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:\n\n\n\n\nSpatial Range Query / KNN Query, no index: rawSpatialRDD is used.\n\n\nSpatial Range Query / KNN Query, use index: indexedRawRDD is used.\n\n\nSpatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.\n\n\nSpatial Join Query / Distance Join Query, use index: indexed RDD is used.\n\n\n\n\nTherefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:\n\n\n\n\nIn Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.\n\n\nIn Spark RDD sharing applications such as \nLivy\n and \nSpark Job Server\n, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.\n\n\n\n\nBe aware of Spatial RDD partitions\n\n\nSometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.\n\n\nAfter that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Tune GeoSpark RDD application"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#advanced-tutorial-tune-your-geospark-rdd-application", 
            "text": "Before getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.", 
            "title": "Advanced tutorial: Tune your GeoSpark RDD application"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#pick-a-proper-geospark-version", 
            "text": "The versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.  The first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.  The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read  GeoSpark version release notes  and make sure you are ready to accept the API changes.  The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.", 
            "title": "Pick a proper GeoSpark version"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#choose-a-proper-spatial-rdd-constructor", 
            "text": "GeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.   Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows: public   PointRDD ( JavaSparkContext   sparkContext ,   String   InputLocation ,   Integer   Offset ,   FileDataSplitter   splitter ,   boolean   carryInputData ,   Integer   partitions ,   StorageLevel   newLevel )   Initialize a SpatialRDD from an existing RDD. A typical example is as follows: public   PointRDD ( JavaRDD Point   rawSpatialRDD ,   StorageLevel   newLevel )    You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.  However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:  public   PointRDD ( JavaSparkContext   sparkContext ,   String   InputLocation ,   Integer   Offset ,   FileDataSplitter   splitter ,   boolean   carryInputData ,   Integer   partitions ,   Envelope   datasetBoundary ,   Integer   approximateTotalCount )   {  \nManually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.", 
            "title": "Choose a proper Spatial RDD constructor"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#cache-the-spatial-rdd-that-is-repeatedly-used", 
            "text": "Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:   rawSpatialRDD: The RDD generated by SpatialRDD constructors.  spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.  indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.  indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.   These four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:   Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.  Spatial Range Query / KNN Query, use index: indexedRawRDD is used.  Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.  Spatial Join Query / Distance Join Query, use index: indexed RDD is used.   Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:   In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.  In Spark RDD sharing applications such as  Livy  and  Spark Job Server , many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.", 
            "title": "Cache the Spatial RDD that is repeatedly used"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#be-aware-of-spatial-rdd-partitions", 
            "text": "Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.  After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Be aware of Spatial RDD partitions"
        }, 
        {
            "location": "/tutorial/benchmark/", 
            "text": "Benchmark\n\n\nWe welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,\n\n\n\n\nPlease always use the latest version or state the version used in your benchmark so that we can trace back to the issues.\n\n\nPlease consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.\n\n\nPlease open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/tutorial/benchmark/#benchmark", 
            "text": "We welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,   Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.  Please consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.  Please open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/", 
            "text": "Scala and Java API\n\n\nGeoSpark Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geospark/javadoc/\n\n\nThe \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala/Java doc"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/#scala-and-java-api", 
            "text": "GeoSpark Scala and Java API:  http://www.public.asu.edu/~jiayu2/geospark/javadoc/  The \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.  Note: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/", 
            "text": "Introduction\n\n\nFunction list\n\n\nGeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through:\n\nvar\n \nmyDataFrame\n \n=\n \nsparkSession\n.\nsql\n(\nYOUR_SQL\n)\n\n\n\n\n\n\nConstructor: Construct a Geometry given an input string or coordinates\n\n\nExample: ST_GeomFromWKT (string). Create a Geometry from a WKT String.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nFunction: Execute a function on the given column or columns\n\n\nExample: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nAggregate function: Return the aggregated value on the given column\n\n\nExample: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.\n\n\nDocumentation: \nHere\n\n\n\n\n\n\nPredicate: Execute a logic judgement on the given columns and return true or false\n\n\nExample: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".\n\n\nDocumentation: \nHere\n\n\n\n\n\n\n\n\nGeoSparkSQL supports SparkSQL query optimizer, documentation is \nHere\n\n\nQuick start\n\n\nThe detailed explanation is here \nWrite a SQL/DataFrame application\n.\n\n\n\n\nAdd GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt\n\n\nDeclare your Spark Session\n\nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n().\n\n      \nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n).\n\n      \nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n).\n\n      \nmaster\n(\nlocal[*]\n).\nappName\n(\nmyGeoSparkSQLdemo\n).\ngetOrCreate\n()\n\n\n\n\nAdd the following line after your SparkSession declaration:\n\nGeoSparkSQLRegistrator\n.\nregisterAll\n(\nsparkSession\n)", 
            "title": "Quick start"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#function-list", 
            "text": "GeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through: var   myDataFrame   =   sparkSession . sql ( YOUR_SQL )    Constructor: Construct a Geometry given an input string or coordinates  Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.  Documentation:  Here    Function: Execute a function on the given column or columns  Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.  Documentation:  Here    Aggregate function: Return the aggregated value on the given column  Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.  Documentation:  Here    Predicate: Execute a logic judgement on the given columns and return true or false  Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".  Documentation:  Here     GeoSparkSQL supports SparkSQL query optimizer, documentation is  Here", 
            "title": "Function list"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#quick-start", 
            "text": "The detailed explanation is here  Write a SQL/DataFrame application .   Add GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt  Declare your Spark Session sparkSession   =   SparkSession . builder (). \n       config ( spark.serializer , classOf [ KryoSerializer ]. getName ). \n       config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName ). \n       master ( local[*] ). appName ( myGeoSparkSQLdemo ). getOrCreate ()   Add the following line after your SparkSession declaration: GeoSparkSQLRegistrator . registerAll ( sparkSession )", 
            "title": "Quick start"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/", 
            "text": "Note\n\n\nUUIDs ensure the shape uniqueness of a geometry. It can be any strings. This is only needed when you want to convert an Spatial DataFrame to an Spatial RDD and let each geometry carry some non-spatial attributes (e.g., price, age, ...).\n\n\n\n\nST_GeomFromWKT\n\n\nIntroduction: Construct a Geometry from Wkt. Unlimited UUID strings can be appended.\n\n\nFormat:\n\nST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeomFromWKT\n(\npolygontable\n.\n_c0\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nSELECT\n \nST_GeomFromWKT\n(\nPOINT(40.7128,-74.0060)\n)\n \nAS\n \ngeometry\n\n\n\n\n\nST_GeomFromWKB\n\n\nIntroduction: Construct a Geometry from WKB string. Unlimited UUID strings can be appended.\n\n\nFormat:\n\nST_GeomFromWKB (Wkb:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_GeomFromWKB\n(\npolygontable\n.\n_c0\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nST_GeomFromGeoJSON\n\n\nIntroduction: Construct a Geometry from GeoJson. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nvar\n \npolygonJsonDf\n \n=\n \nsparkSession\n.\nread\n.\nformat\n(\ncsv\n).\noption\n(\ndelimiter\n,\n\\t\n).\noption\n(\nheader\n,\nfalse\n).\nload\n(\ngeoJsonGeomInputLocation\n)\n\n\npolygonJsonDf\n.\ncreateOrReplaceTempView\n(\npolygontable\n)\n\n\npolygonJsonDf\n.\nshow\n()\n\n\nvar\n \npolygonDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n        \n\n\n          | SELECT ST_GeomFromGeoJSON(polygontable._c0) AS countyshape\n\n\n          | FROM polygontable\n\n\n        \n.\nstripMargin\n)\n\n\npolygonDf\n.\nshow\n()\n\n\n\n\n\n\nWarning\n\n\nThe way that GeoSparkSQL reads GeoJSON is different from that in SparkSQL\n\n\n\n\nRead ESRI Shapefile\n\n\nIntroduction: Construct a DataFrame from a Shapefile\n\n\nSince: \nv1.0.0\n\n\nSparkSQL example:\n\n\nvar\n \nspatialRDD\n \n=\n \nnew\n \nSpatialRDD\n[\nGeometry\n]\n\n\nspatialRDD\n.\nrawSpatialRDD\n \n=\n \nShapefileReader\n.\nreadToGeometryRDD\n(\nsparkSession\n.\nsparkContext\n,\n \nshapefileInputLocation\n)\n\n\nvar\n \nrawSpatialDf\n \n=\n \nAdapter\n.\ntoDf\n(\nspatialRDD\n,\nsparkSession\n)\n\n\nrawSpatialDf\n.\ncreateOrReplaceTempView\n(\nrawSpatialDf\n)\n\n\nvar\n \nspatialDf\n \n=\n \nsparkSession\n.\nsql\n(\n\n\n          | ST_GeomFromWKT(rddshape), _c1, _c2\n\n\n          | FROM rawSpatialDf\n\n\n        \n.\nstripMargin\n)\n\n\nspatialDf\n.\nshow\n()\n\n\nspatialDf\n.\nprintSchema\n()\n\n\n\n\n\n\n\nNote\n\n\nThe file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called \nmyShapefile\n, the file structure should be like this:\n\n- shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...\n\n\n\n\n\n\n\nWarning\n\n\nPlease make sure you use \nST_GeomFromWKT\n to create Geometry type column otherwise that column cannot be used in GeoSparkSQL.\n\n\n\n\nST_Point\n\n\nIntroduction: Construct a Point from X and Y. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Point\n(\nCAST\n(\npointtable\n.\n_c0\n \nAS\n \nDecimal\n(\n24\n,\n20\n)),\n \nCAST\n(\npointtable\n.\n_c1\n \nAS\n \nDecimal\n(\n24\n,\n20\n)))\n \nAS\n \npointshape\n\n\nFROM\n \npointtable\n\n\n\n\nST_PointFromText\n\n\nIntroduction: Construct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_PointFromText\n(\npointtable\n.\n_c0\n,\n,\n)\n \nAS\n \npointshape\n\n\nFROM\n \npointtable\n\n\n\n\nSELECT\n \nST_PointFromText\n(\n40.7128,-74.0060\n,\n \n,\n)\n \nAS\n \npointshape\n\n\n\n\n\nST_PolygonFromText\n\n\nIntroduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_PolygonFromText\n(\npolygontable\n.\n_c0\n,\n,\n)\n \nAS\n \npolygonshape\n\n\nFROM\n \npolygontable\n\n\n\n\nSELECT\n \nST_PolygonFromText\n(\n-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969\n,\n \n,\n)\n \nAS\n \npolygonshape\n\n\n\n\n\nST_LineStringFromText\n\n\nIntroduction: Construct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_LineStringFromText\n(\nlinestringtable\n.\n_c0\n,\n,\n)\n \nAS\n \nlinestringshape\n\n\nFROM\n \nlinestringtable\n\n\n\n\nSELECT\n \nST_LineStringFromText\n(\n-74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794\n,\n \n,\n)\n \nAS\n \nlinestringshape\n\n\n\n\n\nST_PolygonFromEnvelope\n\n\nIntroduction: Construct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.\n\n\nFormat: \nST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n\n\nFROM\n \npointdf\n\n\nWHERE\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\npointshape\n)\n\n\n\n\nST_Circle\n\n\nIntroduction: Construct a Circle from A with a Radius.\n\n\nFormat: \nST_Circle (A:Geometry, Radius:decimal)\n\n\nSince: \nv1.0.0\n - \nv1.1.3\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_Circle\n(\npointdf\n.\npointshape\n,\n \n1\n.\n0\n)\n\n\nFROM\n \npointdf\n\n\n\n\n\n\n\nNote\n\n\nGeoSpark doesn't control the radius's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See \nST_Transform\n.", 
            "title": "Constructor"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromwkt", 
            "text": "Introduction: Construct a Geometry from Wkt. Unlimited UUID strings can be appended.  Format: ST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_GeomFromWKT ( polygontable . _c0 )   AS   polygonshape  FROM   polygontable   SELECT   ST_GeomFromWKT ( POINT(40.7128,-74.0060) )   AS   geometry", 
            "title": "ST_GeomFromWKT"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromwkb", 
            "text": "Introduction: Construct a Geometry from WKB string. Unlimited UUID strings can be appended.  Format: ST_GeomFromWKB (Wkb:string, UUID1, UUID2, ...)  Since:  v1.2.0  Spark SQL example: SELECT   ST_GeomFromWKB ( polygontable . _c0 )   AS   polygonshape  FROM   polygontable", 
            "title": "ST_GeomFromWKB"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromgeojson", 
            "text": "Introduction: Construct a Geometry from GeoJson. Unlimited UUID strings can be appended.  Format:  ST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: var   polygonJsonDf   =   sparkSession . read . format ( csv ). option ( delimiter , \\t ). option ( header , false ). load ( geoJsonGeomInputLocation )  polygonJsonDf . createOrReplaceTempView ( polygontable )  polygonJsonDf . show ()  var   polygonDf   =   sparkSession . sql ( \n                    | SELECT ST_GeomFromGeoJSON(polygontable._c0) AS countyshape            | FROM polygontable           . stripMargin )  polygonDf . show ()    Warning  The way that GeoSparkSQL reads GeoJSON is different from that in SparkSQL", 
            "title": "ST_GeomFromGeoJSON"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#read-esri-shapefile", 
            "text": "Introduction: Construct a DataFrame from a Shapefile  Since:  v1.0.0  SparkSQL example:  var   spatialRDD   =   new   SpatialRDD [ Geometry ]  spatialRDD . rawSpatialRDD   =   ShapefileReader . readToGeometryRDD ( sparkSession . sparkContext ,   shapefileInputLocation )  var   rawSpatialDf   =   Adapter . toDf ( spatialRDD , sparkSession )  rawSpatialDf . createOrReplaceTempView ( rawSpatialDf )  var   spatialDf   =   sparkSession . sql (            | ST_GeomFromWKT(rddshape), _c1, _c2            | FROM rawSpatialDf           . stripMargin )  spatialDf . show ()  spatialDf . printSchema ()    Note  The file extensions of .shp, .shx, .dbf must be in lowercase. Assume you have a shape file called  myShapefile , the file structure should be like this: - shapefile1\n- shapefile2\n- myshapefile\n    - myshapefile.shp\n    - myshapefile.shx\n    - myshapefile.dbf\n    - myshapefile...\n    - ...    Warning  Please make sure you use  ST_GeomFromWKT  to create Geometry type column otherwise that column cannot be used in GeoSparkSQL.", 
            "title": "Read ESRI Shapefile"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_point", 
            "text": "Introduction: Construct a Point from X and Y. Unlimited UUID strings can be appended.  Format:  ST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Point ( CAST ( pointtable . _c0   AS   Decimal ( 24 , 20 )),   CAST ( pointtable . _c1   AS   Decimal ( 24 , 20 )))   AS   pointshape  FROM   pointtable", 
            "title": "ST_Point"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_pointfromtext", 
            "text": "Introduction: Construct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Format:  ST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_PointFromText ( pointtable . _c0 , , )   AS   pointshape  FROM   pointtable   SELECT   ST_PointFromText ( 40.7128,-74.0060 ,   , )   AS   pointshape", 
            "title": "ST_PointFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromtext", 
            "text": "Introduction: Construct a Polygon from Text, delimited by Delimiter. Path must be closed. Unlimited UUID strings can be appended.  Format:  ST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_PolygonFromText ( polygontable . _c0 , , )   AS   polygonshape  FROM   polygontable   SELECT   ST_PolygonFromText ( -74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794,-74.0428197,40.6867969 ,   , )   AS   polygonshape", 
            "title": "ST_PolygonFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_linestringfromtext", 
            "text": "Introduction: Construct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Format:  ST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   ST_LineStringFromText ( linestringtable . _c0 , , )   AS   linestringshape  FROM   linestringtable   SELECT   ST_LineStringFromText ( -74.0428197,40.6867969,-74.0421975,40.6921336,-74.0508020,40.6912794 ,   , )   AS   linestringshape", 
            "title": "ST_LineStringFromText"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromenvelope", 
            "text": "Introduction: Construct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.  Format:  ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)  Since:  v1.0.0  Spark SQL example: SELECT   *  FROM   pointdf  WHERE   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . pointshape )", 
            "title": "ST_PolygonFromEnvelope"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_circle", 
            "text": "Introduction: Construct a Circle from A with a Radius.  Format:  ST_Circle (A:Geometry, Radius:decimal)  Since:  v1.0.0  -  v1.1.3  Spark SQL example:  SELECT   ST_Circle ( pointdf . pointshape ,   1 . 0 )  FROM   pointdf    Note  GeoSpark doesn't control the radius's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See  ST_Transform .", 
            "title": "ST_Circle"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/", 
            "text": "ST_Distance\n\n\nIntroduction: Return the Euclidean distance between A and B\n\n\nFormat: \nST_Distance (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Distance\n(\npolygondf\n.\ncountyshape\n,\n \npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_ConvexHull\n\n\nIntroduction: Return the Convex Hull of polgyon A\n\n\nFormat: \nST_ConvexHull (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_ConvexHull\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Envelope\n\n\nIntroduction: Return the envelop boundary of A\n\n\nFormat: \nST_Envelope (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf\n\n\n\nST_Length\n\n\nIntroduction: Return the perimeter of A\n\n\nFormat: ST_Length (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Length\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Area\n\n\nIntroduction: Return the area of A\n\n\nFormat: \nST_Area (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Area\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Centroid\n\n\nIntroduction: Return the centroid point of A\n\n\nFormat: \nST_Centroid (A:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Centroid\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\nST_Transform\n\n\nIntroduction:\n\n\nTransform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS\n\n\n\n\nNote\n\n\nBy default, \nST_Transform\n assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".\n\n\n\n\n\n\nNote\n\n\nIf \nST_Transform\n throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.\n\n\n\n\nFormat: \nST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example (simple):\n\nSELECT\n \nST_Transform\n(\npolygondf\n.\ncountyshape\n,\n \nepsg:4326\n,\nepsg:3857\n)\n \n\nFROM\n \npolygondf\n\n\n\n\nSpark SQL example (with optional parameters):\n\nSELECT\n \nST_Transform\n(\npolygondf\n.\ncountyshape\n,\n \nepsg:4326\n,\nepsg:3857\n,\ntrue\n,\n \nfalse\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\n\nNote\n\n\nThe detailed EPSG information can be searched on \nEPSG.io\n.\n\n\n\n\nST_Intersection\n\n\nIntroduction: Return the intersection geometry of A and B\n\n\nFormat: \nST_Intersection (A:geometry, B:geometry)\n\n\nSince: \nv1.1.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_Intersection\n(\npolygondf\n.\ncountyshape\n,\n \npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_IsValid\n\n\nIntroduction: Test if a geometry is well formed\n\n\nFormat: \nST_IsValid (A:geometry)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_IsValid\n(\npolygondf\n.\ncountyshape\n)\n\n\nFROM\n \npolygondf\n\n\n\n\n\nST_PrecisionReduce\n\n\nIntroduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.\n\n\nFormat: \nST_PrecisionReduce (A:geometry, B:int)\n\n\nSince: \nv1.2.0\n\n\nSpark SQL example:\n\n\nSELECT\n \nST_PrecisionReduce\n(\npolygondf\n.\ncountyshape\n,\n \n9\n)\n\n\nFROM\n \npolygondf\n\n\n\nThe new coordinates will only have 9 decimal places.", 
            "title": "Function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_distance", 
            "text": "Introduction: Return the Euclidean distance between A and B  Format:  ST_Distance (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Distance ( polygondf . countyshape ,   polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Distance"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_convexhull", 
            "text": "Introduction: Return the Convex Hull of polgyon A  Format:  ST_ConvexHull (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_ConvexHull ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_ConvexHull"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_envelope", 
            "text": "Introduction: Return the envelop boundary of A  Format:  ST_Envelope (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT ST_Envelope(polygondf.countyshape)\nFROM polygondf", 
            "title": "ST_Envelope"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_length", 
            "text": "Introduction: Return the perimeter of A  Format: ST_Length (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Length ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Length"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_area", 
            "text": "Introduction: Return the area of A  Format:  ST_Area (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Area ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Area"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_centroid", 
            "text": "Introduction: Return the centroid point of A  Format:  ST_Centroid (A:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Centroid ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Centroid"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_transform", 
            "text": "Introduction:  Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS   Note  By default,  ST_Transform  assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".    Note  If  ST_Transform  throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.   Format:  ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)  Since:  v1.0.0  Spark SQL example (simple): SELECT   ST_Transform ( polygondf . countyshape ,   epsg:4326 , epsg:3857 )   FROM   polygondf   Spark SQL example (with optional parameters): SELECT   ST_Transform ( polygondf . countyshape ,   epsg:4326 , epsg:3857 , true ,   false )  FROM   polygondf    Note  The detailed EPSG information can be searched on  EPSG.io .", 
            "title": "ST_Transform"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_intersection", 
            "text": "Introduction: Return the intersection geometry of A and B  Format:  ST_Intersection (A:geometry, B:geometry)  Since:  v1.1.0  Spark SQL example:  SELECT   ST_Intersection ( polygondf . countyshape ,   polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_Intersection"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_isvalid", 
            "text": "Introduction: Test if a geometry is well formed  Format:  ST_IsValid (A:geometry)  Since:  v1.2.0  Spark SQL example:  SELECT   ST_IsValid ( polygondf . countyshape )  FROM   polygondf", 
            "title": "ST_IsValid"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_precisionreduce", 
            "text": "Introduction: Reduce the decimals places in the coordinates of the geometry to the given number of decimal places. The last decimal place will be rounded.  Format:  ST_PrecisionReduce (A:geometry, B:int)  Since:  v1.2.0  Spark SQL example:  SELECT   ST_PrecisionReduce ( polygondf . countyshape ,   9 )  FROM   polygondf  \nThe new coordinates will only have 9 decimal places.", 
            "title": "ST_PrecisionReduce"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/", 
            "text": "ST_Contains\n\n\nIntroduction: Return true if A fully contains B\n\n\nFormat: \nST_Contains (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\narealandmark\n)\n\n\n\n\nST_Intersects\n\n\nIntroduction: Return true if A intersects B\n\n\nFormat: \nST_Intersects (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Intersects\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n),\n \npointdf\n.\narealandmark\n)\n\n\n\n\nST_Within\n\n\nIntroduction: Return true if A is fully contained by B\n\n\nFormat: \nST_Within (A:geometry, B:geometry)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \n*\n \n\nFROM\n \npointdf\n \n\nWHERE\n \nST_Within\n(\npointdf\n.\narealandmark\n,\n \nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n100\n.\n0\n,\n1000\n.\n0\n,\n1100\n.\n0\n))", 
            "title": "Predicate"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_contains", 
            "text": "Introduction: Return true if A fully contains B  Format:  ST_Contains (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . arealandmark )", 
            "title": "ST_Contains"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_intersects", 
            "text": "Introduction: Return true if A intersects B  Format:  ST_Intersects (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Intersects ( ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ),   pointdf . arealandmark )", 
            "title": "ST_Intersects"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_within", 
            "text": "Introduction: Return true if A is fully contained by B  Format:  ST_Within (A:geometry, B:geometry)  Since:  v1.0.0  Spark SQL example: SELECT   *   FROM   pointdf   WHERE   ST_Within ( pointdf . arealandmark ,   ST_PolygonFromEnvelope ( 1 . 0 , 100 . 0 , 1000 . 0 , 1100 . 0 ))", 
            "title": "ST_Within"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/", 
            "text": "ST_Envelope_Aggr\n\n\nIntroduction: Return the entire envelope boundary of all geometries in A\n\n\nFormat: \nST_Envelope_Aggr (A:geometryColumn)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Envelope_Aggr\n(\npointdf\n.\narealandmark\n)\n\n\nFROM\n \npointdf\n\n\n\n\nST_Union_Aggr\n\n\nIntroduction: Return the polygon union of all polygons in A\n\n\nFormat: \nST_Union_Aggr (A:geometryColumn)\n\n\nSince: \nv1.0.0\n\n\nSpark SQL example:\n\nSELECT\n \nST_Union_Aggr\n(\npolygondf\n.\npolygonshape\n)\n\n\nFROM\n \npolygondf", 
            "title": "Aggregate function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_envelope_aggr", 
            "text": "Introduction: Return the entire envelope boundary of all geometries in A  Format:  ST_Envelope_Aggr (A:geometryColumn)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Envelope_Aggr ( pointdf . arealandmark )  FROM   pointdf", 
            "title": "ST_Envelope_Aggr"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_union_aggr", 
            "text": "Introduction: Return the polygon union of all polygons in A  Format:  ST_Union_Aggr (A:geometryColumn)  Since:  v1.0.0  Spark SQL example: SELECT   ST_Union_Aggr ( polygondf . polygonshape )  FROM   polygondf", 
            "title": "ST_Union_Aggr"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/", 
            "text": "GeoSparkSQL query optimizer\n\n\nGeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:\n\n\n\n\nAutomatically optimizes range join query and distance join query.\n\n\nAutomatically performs predicate pushdown.\n\n\n\n\nRange join\n\n\nIntroduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate\n\n\nSpark SQL Example:\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n\n\nWHERE\n \nST_Contains\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\n\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n\n\nWHERE\n \nST_Intersects\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\n\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf\n,\n \npolygondf\n\n\nWHERE\n \nST_Within\n(\npointdf\n.\npointshape\n,\n \npolygondf\n.\npolygonshape\n)\n\n\n\nSpark SQL Physical plan:\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nRangeJoin\n \npolygonshape#\n20\n:\n \ngeometry\n,\n \npointshape#\n43\n:\n \ngeometry\n,\n \nfalse\n\n\n:-\n \nProject\n \n[\nst_polygonfromenvelope\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc2#\n2\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc3#\n3\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmypolygonid\n)\n \nAS\n \npolygonshape#\n20\n]\n\n\n:\n  \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n31\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n32\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape#\n43\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv\n\n\n\n\n\n\nNote\n\n\nAll join queries in GeoSparkSQL are inner joins\n\n\n\n\nDistance join\n\n\nIntroduction: Find geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance\n\n\nSpark SQL Example:\n\n\nOnly consider \nfully within a certain distance\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf1\n,\n \npointdf2\n\n\nWHERE\n \nST_Distance\n(\npointdf1\n.\npointshape1\n,\npointdf2\n.\npointshape2\n)\n \n \n2\n\n\n\n\nConsider \nintersects within a certain distance\n\n\nSELECT\n \n*\n\n\nFROM\n \npointdf1\n,\n \npointdf2\n\n\nWHERE\n \nST_Distance\n(\npointdf1\n.\npointshape1\n,\npointdf2\n.\npointshape2\n)\n \n=\n \n2\n\n\n\n\nSpark SQL Physical plan:\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nDistanceJoin\n \npointshape1#\n12\n:\n \ngeometry\n,\n \npointshape2#\n33\n:\n \ngeometry\n,\n \n2.0\n,\n \ntrue\n\n\n:-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape1#\n12\n]\n\n\n:\n  \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n21\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n22\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape2#\n33\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv\n\n\n\n\n\n\nWarning\n\n\nGeoSpark doesn't control the distance's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See \nST_Transform\n.\n\n\n\n\nPredicate pushdown\n\n\nIntroduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query*\n\n\nSpark SQL Example:\n\n\nSELECT\n \n*\n\n\nFROM\n \npolygondf\n,\n \npointdf\n \n\nWHERE\n \nST_Contains\n(\npolygondf\n.\npolygonshape\n,\npointdf\n.\npointshape\n)\n\n\nAND\n \nST_Contains\n(\nST_PolygonFromEnvelope\n(\n1\n.\n0\n,\n101\n.\n0\n,\n501\n.\n0\n,\n601\n.\n0\n),\n \npolygondf\n.\npolygonshape\n)\n\n\n\n\n\nSpark SQL Physical plan:\n\n\n==\n \nPhysical\n \nPlan\n \n==\n\n\nRangeJoin\n \npolygonshape#\n20\n:\n \ngeometry\n,\n \npointshape#\n43\n:\n \ngeometry\n,\n \nfalse\n\n\n:-\n \nProject\n \n[\nst_polygonfromenvelope\n(\ncast\n(\n_\nc0#\n0\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n1\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc2#\n2\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc3#\n3\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmypolygonid\n)\n \nAS\n \npolygonshape#\n20\n]\n\n\n:\n  \n+-\n \nFilter\n  \n**org\n.\napache\n.\nspark\n.\nsql\n.\ngeosparksql\n.\nexpressions\n.\nST_Contains\n$\n**\n\n\n:\n     \n+-\n \n*\nFileScan\n \ncsv\n\n\n+-\n \nProject\n \n[\nst_point\n(\ncast\n(\n_\nc0#\n31\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \ncast\n(\n_\nc1#\n32\n \nas\n \ndecimal\n(\n24\n,\n20\n)),\n \nmyPointId\n)\n \nAS\n \npointshape#\n43\n]\n\n   \n+-\n \n*\nFileScan\n \ncsv", 
            "title": "Join query (optimizer)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#geosparksql-query-optimizer", 
            "text": "GeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:   Automatically optimizes range join query and distance join query.  Automatically performs predicate pushdown.", 
            "title": "GeoSparkSQL query optimizer"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#range-join", 
            "text": "Introduction: Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate  Spark SQL Example:  SELECT   *  FROM   polygondf ,   pointdf  WHERE   ST_Contains ( polygondf . polygonshape , pointdf . pointshape )   SELECT   *  FROM   polygondf ,   pointdf  WHERE   ST_Intersects ( polygondf . polygonshape , pointdf . pointshape )   SELECT   *  FROM   pointdf ,   polygondf  WHERE   ST_Within ( pointdf . pointshape ,   polygondf . polygonshape )  \nSpark SQL Physical plan: ==   Physical   Plan   ==  RangeJoin   polygonshape# 20 :   geometry ,   pointshape# 43 :   geometry ,   false  :-   Project   [ st_polygonfromenvelope ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   cast ( _ c2# 2   as   decimal ( 24 , 20 )),   cast ( _ c3# 3   as   decimal ( 24 , 20 )),   mypolygonid )   AS   polygonshape# 20 ]  :    +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 31   as   decimal ( 24 , 20 )),   cast ( _ c1# 32   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape# 43 ] \n    +-   * FileScan   csv    Note  All join queries in GeoSparkSQL are inner joins", 
            "title": "Range join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#distance-join", 
            "text": "Introduction: Find geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance  Spark SQL Example:  Only consider  fully within a certain distance  SELECT   *  FROM   pointdf1 ,   pointdf2  WHERE   ST_Distance ( pointdf1 . pointshape1 , pointdf2 . pointshape2 )     2   Consider  intersects within a certain distance  SELECT   *  FROM   pointdf1 ,   pointdf2  WHERE   ST_Distance ( pointdf1 . pointshape1 , pointdf2 . pointshape2 )   =   2   Spark SQL Physical plan: ==   Physical   Plan   ==  DistanceJoin   pointshape1# 12 :   geometry ,   pointshape2# 33 :   geometry ,   2.0 ,   true  :-   Project   [ st_point ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape1# 12 ]  :    +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 21   as   decimal ( 24 , 20 )),   cast ( _ c1# 22   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape2# 33 ] \n    +-   * FileScan   csv    Warning  GeoSpark doesn't control the distance's unit (degree or meter). It is same with the geometry. To change the geometry's unit, please transform the coordinate reference system. See  ST_Transform .", 
            "title": "Distance join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#predicate-pushdown", 
            "text": "Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query*  Spark SQL Example:  SELECT   *  FROM   polygondf ,   pointdf   WHERE   ST_Contains ( polygondf . polygonshape , pointdf . pointshape )  AND   ST_Contains ( ST_PolygonFromEnvelope ( 1 . 0 , 101 . 0 , 501 . 0 , 601 . 0 ),   polygondf . polygonshape )   Spark SQL Physical plan:  ==   Physical   Plan   ==  RangeJoin   polygonshape# 20 :   geometry ,   pointshape# 43 :   geometry ,   false  :-   Project   [ st_polygonfromenvelope ( cast ( _ c0# 0   as   decimal ( 24 , 20 )),   cast ( _ c1# 1   as   decimal ( 24 , 20 )),   cast ( _ c2# 2   as   decimal ( 24 , 20 )),   cast ( _ c3# 3   as   decimal ( 24 , 20 )),   mypolygonid )   AS   polygonshape# 20 ]  :    +-   Filter    **org . apache . spark . sql . geosparksql . expressions . ST_Contains $ **  :       +-   * FileScan   csv  +-   Project   [ st_point ( cast ( _ c0# 31   as   decimal ( 24 , 20 )),   cast ( _ c1# 32   as   decimal ( 24 , 20 )),   myPointId )   AS   pointshape# 43 ] \n    +-   * FileScan   csv", 
            "title": "Predicate pushdown"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/", 
            "text": "Usage\n\n\nGeoSparkSQL supports many parameters. To change their values,\n\n\n\n\nSet it through SparkConf:\n\nsparkSession\n \n=\n \nSparkSession\n.\nbuilder\n().\n\n      \nconfig\n(\nspark.serializer\n,\nclassOf\n[\nKryoSerializer\n].\ngetName\n).\n\n      \nconfig\n(\nspark.kryo.registrator\n,\n \nclassOf\n[\nGeoSparkKryoRegistrator\n].\ngetName\n).\n\n      \nconfig\n(\ngeospark.global.index\n,\ntrue\n)\n\n      \nmaster\n(\nlocal[*]\n).\nappName\n(\nmyGeoSparkSQLdemo\n).\ngetOrCreate\n()\n\n\n\n\nCheck your current GeoSparkSQL configuration:\n\nval\n \ngeosparkConf\n \n=\n \nnew\n \nGeoSparkConf\n(\nsparkSession\n.\nsparkContext\n.\ngetConf\n)\n\n\nprintln\n(\ngeosparkConf\n)\n\n\n\n\n\n\nExplanation\n\n\n\n\ngeospark.global.index\n\n\nUse spatial index (currently, only supports in SQL range join and SQL distance join)\n\n\nDefault: true\n\n\nPossible values: true, false\n\n\n\n\n\n\ngeospark.global.indextype\n\n\nSpatial index type, only valid when \"geospark.global.index\" is true\n\n\nDefault: rtree\n\n\nPossible values: rtree, quadtree\n\n\n\n\n\n\ngeospark.join.gridtype\n\n\nSpatial partitioning grid type for join query\n\n\nDefault: quadtree\n\n\nPossible values: quadtree, kdbtree, rtree, voronoi\n\n\n\n\n\n\ngeospark.join.numpartition \n(Advanced users only!)\n\n\nNumber of partitions for both sides in a join query\n\n\nDefault: -1, which means use the existing partitions\n\n\nPossible values: any integers\n\n\n\n\n\n\ngeospark.join.indexbuildside \n(Advanced users only!)\n\n\nThe side which GeoSpark builds spatial indices on\n\n\nDefault: left\n\n\nPossible values: left, right\n\n\n\n\n\n\ngeospark.join.spatitionside \n(Advanced users only!)\n\n\nThe dominant side in spatial partitioning stage\n\n\nDefault: left\n\n\nPossible values: left, right", 
            "title": "Parameter"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#usage", 
            "text": "GeoSparkSQL supports many parameters. To change their values,   Set it through SparkConf: sparkSession   =   SparkSession . builder (). \n       config ( spark.serializer , classOf [ KryoSerializer ]. getName ). \n       config ( spark.kryo.registrator ,   classOf [ GeoSparkKryoRegistrator ]. getName ). \n       config ( geospark.global.index , true ) \n       master ( local[*] ). appName ( myGeoSparkSQLdemo ). getOrCreate ()   Check your current GeoSparkSQL configuration: val   geosparkConf   =   new   GeoSparkConf ( sparkSession . sparkContext . getConf )  println ( geosparkConf )", 
            "title": "Usage"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#explanation", 
            "text": "geospark.global.index  Use spatial index (currently, only supports in SQL range join and SQL distance join)  Default: true  Possible values: true, false    geospark.global.indextype  Spatial index type, only valid when \"geospark.global.index\" is true  Default: rtree  Possible values: rtree, quadtree    geospark.join.gridtype  Spatial partitioning grid type for join query  Default: quadtree  Possible values: quadtree, kdbtree, rtree, voronoi    geospark.join.numpartition  (Advanced users only!)  Number of partitions for both sides in a join query  Default: -1, which means use the existing partitions  Possible values: any integers    geospark.join.indexbuildside  (Advanced users only!)  The side which GeoSpark builds spatial indices on  Default: left  Possible values: left, right    geospark.join.spatitionside  (Advanced users only!)  The dominant side in spatial partitioning stage  Default: left  Possible values: left, right", 
            "title": "Explanation"
        }, 
        {
            "location": "/api/Babylon-Scala-and-Java-API/", 
            "text": "Scala and Java API\n\n\nGeoSpark-Viz (former name, Babylon) Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geosparkviz/javadoc/\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "Scala/Java doc"
        }, 
        {
            "location": "/api/Babylon-Scala-and-Java-API/#scala-and-java-api", 
            "text": "GeoSpark-Viz (former name, Babylon) Scala and Java API:  http://www.public.asu.edu/~jiayu2/geosparkviz/javadoc/  Note: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/contribute/rule/", 
            "text": "Contributing to GeoSpark\n\n\nThe project welcomes contributions. You can contribute to GeoSpark code or documentation by making Pull Requests on \nGeoSpark GitHub Repo\n.\n\n\nThe following sections brief the workflow of how to complete a contribution.\n\n\nPick / Annouce a task\n\n\nIt is important to confirm that your contribution is acceptable. Hence, you have two options to start with:\n\n\n\n\n\n\nPick an issue from the Issues tagged by \nHelp Wanted\n on \nGeoSpark Issues\n.\n\n\n\n\n\n\nAnnounce what you are going to work on in advance if no GitHub issues match your scenario. To do this, contact \nGeoSpark project committer\n.\n\n\n\n\n\n\nDevelop a code contribution\n\n\nCode contributions should include the following:\n\n\n\n\nDetailed documentations on classes and methods.\n\n\nUnit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some GeoSpark functions need to be tested in both Scala and Java.\n\n\nUpdates on corresponding GeoSpark documentation if necessary.\n\n\n\n\nCode contributions must include a license header at the top of each file.  A sample header for Scala/Java files is as follows:\n\n/*\n\n\n * FILE: SpatialRDD\n\n\n * Copyright (c) 2015 - 2018 GeoSpark Development Team\n\n\n *\n\n\n * MIT License\n\n\n *\n\n\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n\n\n * of this software and associated documentation files (the \nSoftware\n), to deal\n\n\n * in the Software without restriction, including without limitation the rights\n\n\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\n\n * copies of the Software, and to permit persons to whom the Software is\n\n\n * furnished to do so, subject to the following conditions:\n\n\n *\n\n\n * The above copyright notice and this permission notice shall be included in all\n\n\n * copies or substantial portions of the Software.\n\n\n *\n\n\n * THE SOFTWARE IS PROVIDED \nAS IS\n, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\n\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\n\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\n\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\n\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\n\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n\n\n * SOFTWARE.\n\n\n *\n\n\n */\n\n\n\n\nDevelop a document contribution\n\n\nDocumentation contributions should satisfy the following requirements:\n\n\n\n\nDetailed explanation with examples.\n\n\nPlace a newly added document in a proper folder\n\n\nChange the \nmkdocs.yml\n if necessary\n\n\n\n\n\n\nNote\n\n\nPlease read \nCompile the source code\n to learn how to compile GeoSpark website.\n\n\n\n\nMake a Pull Request\n\n\nAfter developing a contribution, the easiest and most visible way to push a contribution is to submit a Pull Request (PR) to the \nGitHub repo\n.  \n\n\nWhen preparing a PR, please answser the following questions in the PR:\n\n\n\n\n\n\nWhat changes were proposed in this pull request?\n\n\n\n\n\n\nHow was this patch tested?\n\n\n\n\n\n\nWhen a PR is submitted Travis CI will check the build correctness. Please check the PR status, and fix any reported problems.", 
            "title": "Contributing rule"
        }, 
        {
            "location": "/contribute/rule/#contributing-to-geospark", 
            "text": "The project welcomes contributions. You can contribute to GeoSpark code or documentation by making Pull Requests on  GeoSpark GitHub Repo .  The following sections brief the workflow of how to complete a contribution.", 
            "title": "Contributing to GeoSpark"
        }, 
        {
            "location": "/contribute/rule/#pick-annouce-a-task", 
            "text": "It is important to confirm that your contribution is acceptable. Hence, you have two options to start with:    Pick an issue from the Issues tagged by  Help Wanted  on  GeoSpark Issues .    Announce what you are going to work on in advance if no GitHub issues match your scenario. To do this, contact  GeoSpark project committer .", 
            "title": "Pick / Annouce a task"
        }, 
        {
            "location": "/contribute/rule/#develop-a-code-contribution", 
            "text": "Code contributions should include the following:   Detailed documentations on classes and methods.  Unit Tests to demonstrate code correctness and allow this to be maintained going forward.  In the case of bug fixes the unit test should demonstrate the bug in the absence of the fix (if any).  Unit Tests can be JUnit test or Scala test. Some GeoSpark functions need to be tested in both Scala and Java.  Updates on corresponding GeoSpark documentation if necessary.   Code contributions must include a license header at the top of each file.  A sample header for Scala/Java files is as follows: /*   * FILE: SpatialRDD   * Copyright (c) 2015 - 2018 GeoSpark Development Team   *   * MIT License   *   * Permission is hereby granted, free of charge, to any person obtaining a copy   * of this software and associated documentation files (the  Software ), to deal   * in the Software without restriction, including without limitation the rights   * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell   * copies of the Software, and to permit persons to whom the Software is   * furnished to do so, subject to the following conditions:   *   * The above copyright notice and this permission notice shall be included in all   * copies or substantial portions of the Software.   *   * THE SOFTWARE IS PROVIDED  AS IS , WITHOUT WARRANTY OF ANY KIND, EXPRESS OR   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE   * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,   * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE   * SOFTWARE.   *   */", 
            "title": "Develop a code contribution"
        }, 
        {
            "location": "/contribute/rule/#develop-a-document-contribution", 
            "text": "Documentation contributions should satisfy the following requirements:   Detailed explanation with examples.  Place a newly added document in a proper folder  Change the  mkdocs.yml  if necessary    Note  Please read  Compile the source code  to learn how to compile GeoSpark website.", 
            "title": "Develop a document contribution"
        }, 
        {
            "location": "/contribute/rule/#make-a-pull-request", 
            "text": "After developing a contribution, the easiest and most visible way to push a contribution is to submit a Pull Request (PR) to the  GitHub repo .    When preparing a PR, please answser the following questions in the PR:    What changes were proposed in this pull request?    How was this patch tested?    When a PR is submitted Travis CI will check the build correctness. Please check the PR status, and fix any reported problems.", 
            "title": "Make a Pull Request"
        }, 
        {
            "location": "/contribute/contributor/", 
            "text": "Contributor\n\n\nGeoSpark has received numerous help from the community. This page lists the people who contributed at least a function to GeoSpark source code repository. The contributors are ordered by their last name.\n\n\n\n\n\n\n\n\nName\n\n\nAffiliation\n\n\n\n\n\n\n\n\n\n\nLucas C\n\n\nBrazil\n\n\n\n\n\n\nMasha Basmanova\n\n\nFacebook\n\n\n\n\n\n\nOmkar Kaptan\n\n\nQuantcast\n\n\n\n\n\n\nMohamed Sarwat\n\n\nArizona State University\n\n\n\n\n\n\nJinxuan Wu\n\n\nArizona State University\n\n\n\n\n\n\nJia Yu\n\n\nArizona State University\n\n\n\n\n\n\nZongsi Zhang\n\n\nArizona State University", 
            "title": "Contributor"
        }, 
        {
            "location": "/contribute/contributor/#contributor", 
            "text": "GeoSpark has received numerous help from the community. This page lists the people who contributed at least a function to GeoSpark source code repository. The contributors are ordered by their last name.     Name  Affiliation      Lucas C  Brazil    Masha Basmanova  Facebook    Omkar Kaptan  Quantcast    Mohamed Sarwat  Arizona State University    Jinxuan Wu  Arizona State University    Jia Yu  Arizona State University    Zongsi Zhang  Arizona State University", 
            "title": "Contributor"
        }, 
        {
            "location": "/contact/contact/", 
            "text": "Contact\n\n\nQuestions\n\n\n\n\nGeoSpark@Twitter\n\n\nGeoSpark Discussion Board\n\n\nChat with us! \n\n\nEmail us!\n\n\n\n\nCommitter\n\n\n\n\n\n\nJia Yu\n (Email: jiayu2 at asu.edu)\n\n\n\n\n\n\nMohamed Sarwat\n (Email: msarwat at asu.edu)\n\n\n\n\n\n\nData Systems Lab @ ASU\n\n\nGeoSpark is one of the projects initiated by \nData Systems Lab\n at Arizona State University. The mission of Data Systems Lab is designing and developing experimental data management systems (e.g., database systems).", 
            "title": "Questions"
        }, 
        {
            "location": "/contact/contact/#contact", 
            "text": "", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/contact/#questions", 
            "text": "GeoSpark@Twitter  GeoSpark Discussion Board  Chat with us!   Email us!", 
            "title": "Questions"
        }, 
        {
            "location": "/contact/contact/#committer", 
            "text": "Jia Yu  (Email: jiayu2 at asu.edu)    Mohamed Sarwat  (Email: msarwat at asu.edu)", 
            "title": "Committer"
        }, 
        {
            "location": "/contact/contact/#data-systems-lab-asu", 
            "text": "GeoSpark is one of the projects initiated by  Data Systems Lab  at Arizona State University. The mission of Data Systems Lab is designing and developing experimental data management systems (e.g., database systems).", 
            "title": "Data Systems Lab @ ASU"
        }, 
        {
            "location": "/contact/publication/", 
            "text": "Publication\n\n\n\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"\n (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2018. To appear.\n\n\n\"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"\n (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018\n\n\n\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\"\n (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016\n\n\n\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\"\n (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015\n\n\n\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"\n is the full research paper that talks about the entire GeoSpark ecosystem.", 
            "title": "Publications"
        }, 
        {
            "location": "/contact/publication/#publication", 
            "text": "\"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"  (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. Geoinformatica Journal 2018. To appear.  \"GeoSparkViz: A Scalable Geospatial Data Visualization Framework in the Apache Spark Ecosystem\"  (research paper). Jia Yu, Zongsi Zhang, Mohamed Sarwat. In Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM 2018, Bolzano-Bozen, Italy July 2018  \"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\"  (demo paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016  \"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\"  (short paper). Jia Yu, Jinxuan Wu, Mohamed Sarwat. In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015  \"Spatial Data Management in Apache Spark: The\nGeoSpark Perspective and Beyond\"  is the full research paper that talks about the entire GeoSpark ecosystem.", 
            "title": "Publication"
        }, 
        {
            "location": "/license/License/", 
            "text": "License\n\n\nMIT License (MIT)\n\n\nCopyright \n 2015 - 2018 GeoSpark Development Team\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", 
            "title": "License"
        }, 
        {
            "location": "/license/License/#license", 
            "text": "MIT License (MIT)  Copyright   2015 - 2018 GeoSpark Development Team  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.", 
            "title": "License"
        }
    ]
}