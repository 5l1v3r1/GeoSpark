{
    "docs": [
        {
            "location": "/", 
            "text": "Page hit count (since Jan. 2018): \n\n\n   \n   \n\n\n\n\n\n\n\n\nStatus\n\n\nStable\n\n\nLatest\n\n\nSource code\n\n\nSpark compatibility\n\n\n\n\n\n\n\n\n\n\nGeoSpark\n\n\n\n\n\n\n\n\nSpark 2.X, 1.X\n\n\n\n\n\n\nGeoSparkSQL\n\n\n\n\n\n\n\n\nSpark SQL 2.1, 2.2, 2.3\n\n\n\n\n\n\nGeoSparkViz\n\n\n\n\n\n\n\n\nSpark 2.X, 1.X\n\n\n\n\n\n\n\n\nGeoSpark@Twitter\n||\nGeoSpark Discussion Board\n||\n\n\nIntroduction\n\n\nGeoSpark is listed as \nInfrastructure Project\n on \nApache Spark Official Third Party Project Page\n\n\nGeoSpark is a cluster computing system for processing large-scale spatial data. GeoSpark extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.\n\n\nFeatures\n\n\n\n\nSpatial RDD\n\n\nSpatial SQL\n\n\n\n\nSELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom)\nAND city.name = 'Gotham';\n\n\n\n\n\n\nComplex geometries / trajectories: point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection\n\n\nCSV, TSV, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case\n\n\nSpatial query: range query, range join query, distance join query, K Nearest Neighbor query\n\n\nSpatial index: R-Tree, Quad-Tree\n\n\nSpatial partitioning: KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids\n\n\nCoordinate Reference System / Spatial Reference System Transformation: for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)\n\n\nHigh resolution map: Scatter plot, heat map, choropleth map\n\n\n\n\nCompanies are using GeoSpark (incomplete list)\n\n\n \n\n\nPlease make a Pull Request to add yourself!\n\n\nStructure\n\n\nGeoSpark contains three modules:\n\n\n\n\n\n\n\n\nName\n\n\nAPI\n\n\nSpark compatibility\n\n\nDependency\n\n\n\n\n\n\n\n\n\n\nGeoSpark-core\n\n\nRDD\n\n\nSupports Spark 2.X/1.X\n\n\nSpark-core\n\n\n\n\n\n\nGeoSpark-SQL\n\n\nSQL/DataFrame\n\n\nSupports SparkSQL 2.1 and later\n\n\nSpark-core, Spark-SQL, GeoSpark-core\n\n\n\n\n\n\nGeoSpark-Viz\n\n\nRDD\n\n\nSupports Spark 2.X/1.X\n\n\nSpark-core, GeoSpark-core\n\n\n\n\n\n\n\n\n\n\nCore: GeoSpark SpatialRDDs and Query Operators. \n\n\nSQL: SQL interfaces for GeoSpark core.\n\n\nViz: Visualization extension of GeoSpark core.\n\n\n\n\nGitHub\n\n\nYou can find GeoSpark source code on GeoSpark GitHub repository: \nhttps://github.com/DataSystemsLab/GeoSpark\n\n\nGeoSpark Visualization Extension (GeoSparkViz)\n\n\nGeoSparkViz is a large-scale in-memory geospatial visualization system.\n\n\nGeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.\n\n\nMore details are available here: \nGeoSpark Visualization Extension\n \n\n\nGeoSparkViz Gallery\n\n\n\n\nWatch the high resolution version on a real map", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "GeoSpark is listed as  Infrastructure Project  on  Apache Spark Official Third Party Project Page  GeoSpark is a cluster computing system for processing large-scale spatial data. GeoSpark extends Apache Spark / SparkSQL with a set of out-of-the-box Spatial Resilient Distributed Datasets (SRDDs)/ SpatialSQL that efficiently load, process, and analyze large-scale spatial data across machines.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#features", 
            "text": "Spatial RDD  Spatial SQL   SELECT superhero.name\nFROM city, superhero\nWHERE ST_Contains(city.geom, superhero.geom)\nAND city.name = 'Gotham';   Complex geometries / trajectories: point, polygon, linestring, multi-point, multi-polygon, multi-linestring, GeometryCollection  CSV, TSV, GeoJSON, NASA NetCDF/HDF, Shapefile (.shp, .shx, .dbf): extension must be in lower case  Spatial query: range query, range join query, distance join query, K Nearest Neighbor query  Spatial index: R-Tree, Quad-Tree  Spatial partitioning: KDB-Tree, Quad-Tree, R-Tree, Voronoi diagram, Hilbert curve, Uniform grids  Coordinate Reference System / Spatial Reference System Transformation: for exmaple, from WGS84 (EPSG:4326, degree-based), to EPSG:3857 (meter-based)  High resolution map: Scatter plot, heat map, choropleth map", 
            "title": "Features"
        }, 
        {
            "location": "/#companies-are-using-geospark-incomplete-list", 
            "text": "Please make a Pull Request to add yourself!", 
            "title": "Companies are using GeoSpark (incomplete list)"
        }, 
        {
            "location": "/#structure", 
            "text": "GeoSpark contains three modules:     Name  API  Spark compatibility  Dependency      GeoSpark-core  RDD  Supports Spark 2.X/1.X  Spark-core    GeoSpark-SQL  SQL/DataFrame  Supports SparkSQL 2.1 and later  Spark-core, Spark-SQL, GeoSpark-core    GeoSpark-Viz  RDD  Supports Spark 2.X/1.X  Spark-core, GeoSpark-core      Core: GeoSpark SpatialRDDs and Query Operators.   SQL: SQL interfaces for GeoSpark core.  Viz: Visualization extension of GeoSpark core.", 
            "title": "Structure"
        }, 
        {
            "location": "/#github", 
            "text": "You can find GeoSpark source code on GeoSpark GitHub repository:  https://github.com/DataSystemsLab/GeoSpark", 
            "title": "GitHub"
        }, 
        {
            "location": "/#geospark-visualization-extension-geosparkviz", 
            "text": "GeoSparkViz is a large-scale in-memory geospatial visualization system.  GeoSparkViz provides native support for general cartographic design by extending GeoSpark to process large-scale spatial data. It can visulize Spatial RDD and Spatial Queries and render super high resolution image in parallel.  More details are available here:  GeoSpark Visualization Extension    GeoSparkViz Gallery   Watch the high resolution version on a real map", 
            "title": "GeoSpark Visualization Extension (GeoSparkViz)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/", 
            "text": "v1.1.0\n\n\nThis version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See \nMaven Central coordinate\n to locate the particular version.\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n185\n: CRStransform throws Exception for Bursa wolf parameters. See PR #\n189\n.\n\n\nFixed Issue #\n190\n: Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR #\n192\n.\n\n\nAdd R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR #\n177\n.\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n194\n: doesn't support Spark 2.3.\n\n\nFixed Issue #\n188\n:ST_ConvexHull should accept any type of geometry as an input. See PR #\n189\n.\n\n\nAdd ST_Intersection function. See Issue #\n110\n and PR #\n189\n.\n\n\n\n\nGeoSpark Viz\n\n\n\n\nFixed Issue #\n154\n: GeoSpark kryp serializer and GeoSparkViz conflict. See PR #\n178\n\n\n\n\nv1.0.1\n\n\nGeoSpark Core\n\n\n\n\nFixed Issue #\n170\n\n\n\n\nGeoSpark SQL\n\n\n\n\nFixed Issue #\n171\n\n\nAdded the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately (\nMaven Coordinates\n).\n\n\n\n\nGeoSpark Viz\n\nNone\n\n\n\n\nv1.0.0\n\n\nGeoSpark Core\n\n\n\n\nAdd GeoSparkConf class to read GeoSparkConf from SparkConf\n\n\n\n\nGeoSpark SQL\n\n* Initial release: fully supports SQL/MM-Part3 Spatial SQL standard\n\n\nGeoSpark Viz\n\n* Republish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"\n\n\n\n\n\n\n\n\nv0.9.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fixed \"Missing values when reading Shapefile\": \nIssue #141\n\n\nPerformance improvement\n: Solved Issue \n#91\n, \n#103\n, \n#104\n, \n#125\n, \n#150\n.\n\n\nAdd GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See \nPR 139\n.\n\n\nDelete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See \nPR 131\n.\n\n\n\n\n\n\nNew Functionalities added\n:\n\n\nSpatialJoinQueryFlat/DistanceJoinQueryFlat\n returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results. \n\n\nThe left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe index side in Range query, Distance query, Range join query, Distance join query can be switched.\n\n\nThe generic SpatialRdd supports heterogenous geometries\n\n\nAdd KDB-Tree spatial partitioning method which is more balanced than Quad-Tree\n\n\nRange query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.\n\n\n\n\n\n\n\n\nv0.8.2 (GeoSpark-core)\n\n\n\n\nBug fixes\n: Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue https://github.com/DataSystemsLab/GeoSpark/issues/115\n\n\nNew function added\n: Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.\n\n\n\n\nv0.8.1 (GeoSpark-core)\n\n\n\n\nBug fixes\n: (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors\n\n\n\n\nv0.8.0 (GeoSpark-core)\n\n\n\n\nNew input format added\n: GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 1\n: GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. (\nScala Example\n, \nJava Example\n)\n\n\nJoin Query Performance enhancement 2\n: Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.\n\n\nSpatialRDD Initialization Performance enhancement\n: GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.\n\n\nAPI changed\n: Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see \nGeoSpark user supplied format mapper examples\n\n\nAlternative SpatialRDD constructor added\n: GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.\n\n\nIf he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.\n\n\nIf he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.\n\n\nIf he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset\n, the user still has to pass StorageLevel parameter.\n\n\nBug fix\n: Fix bug \nIssue #97\n and \nIssue #100\n.\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.7.0\n\n\nCoordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:\n GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor (\nExample\n); \nUnnecessary dependencies removed\n: NetCDF/HDF support depends on \nSerNetCDF\n. SetNetCDF becomes optional dependency to reduce fat jar size; \nDefault JDK/JRE change to JDK/JRE 1.8\n: To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default; \nBug fix\n: fix a small format bug when output spatial RDD to disk.\n\n\n\n\n\n\n0.6.2\n\n\nNew input format added:\n Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data (\nScala Example\n,\nJava Example\n); \nBug fix:\n Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file\n\n\n\n\n\n\n0.6.1\n\n\nBug fixes:\n Fix typos LineString DistanceJoin API\n\n\n\n\n\n\n0.6.0\n\n\nMajor updates:\n (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results. \nAPI changes:\n All spatial range and join  queries now take a parameter called \nConsiderBoundaryIntersection\n. This will tell GeoSpark whether returns the objects intersect with windows.\n\n\n\n\n\n\n0.5.3\n\n\nBug fix:\n Fix \nIssue #69\n: Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.\n\n\n\n\n\n\n0.5.2\n\n\nBug fix:\n Fix \nIssue #58\n and \nIssue #60\n; \nPerformance enhancement:\n (1) Deprecate all old Spatial RDD constructors. See the JavaDoc \nhere\n. (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step\n\n\n\n\n\n\n0.5.1\n\n\nBug fix:\n (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call \nIssue #55\n; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support\n\n\n\n\n\n\n0.5.0\n\n\nMajor updates:\n We are pleased to announce the initial version of \nBabylon\n a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here: \nBabylon GeoSpatial Visualization\n\n\n\n\n\n\n0.4.0\n\n\nMajor updates:\n (\nExample\n) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD; \nFunctionality enhancement:\n 1. Release the persist function back to users. 2. Add more exception explanations.\n\n\n\n\n\n\n0.3.2\n\n\nFunctionality enhancement: 1. \nJTSplus Spatial Objects\n now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.\n\n\n\n\n\n\n0.3.1\n\n\nBug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases\n\n\n\n\n\n\n0.3\n\n\nMajor updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining\n\n\n\n\n\n\n0.2\n\n\nImprove code structure and refactor API\n\n\n\n\n\n\n0.1\n\n\nSupport spatial range, join and Knn\n\n\n\n\n\n\n\n\nGeoSpark-Viz (old)\n\n\n\n\n\n\n\n\nVersion\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n0.2.2\n\n\nAdd the support of new output storage\n: Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.\n\n\n\n\n\n\n0.2.1\n\n\nPerformance enhancement\n: significantly accelerate single image generation pipeline. \nBug fix\n:fix a bug in scatter plot parallel rendering.\n\n\n\n\n\n\n0.2.0\n\n\nAPI updates for \nIssue #80\n:\n 1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs; \nNew feature:\n Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. (\nScala Example\n,\nJava Example\n)\n\n\n\n\n\n\n0.1.1\n\n\nMajor updates:\n Babylon supports vector image and outputs SVG image format\n\n\n\n\n\n\n0.1.0\n\n\nMajor updates:\n Babylon initial version supports raster images", 
            "title": "Release notes"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v110", 
            "text": "This version adds very efficient R-Tree and Quad-Tree index serializers and supports Apache Spark and  SparkSQL 2.3. See  Maven Central coordinate  to locate the particular version.  GeoSpark Core   Fixed Issue # 185 : CRStransform throws Exception for Bursa wolf parameters. See PR # 189 .  Fixed Issue # 190 : Shapefile reader doesn't support Chinese characters (\u4e2d\u6587\u5b57\u7b26). See PR # 192 .  Add R-Tree and Quad-Tree index serializer. GeoSpark custom index serializer has around 2 times smaller index size and faster serialization than Apache Spark kryo serializer. See PR # 177 .   GeoSpark SQL   Fixed Issue # 194 : doesn't support Spark 2.3.  Fixed Issue # 188 :ST_ConvexHull should accept any type of geometry as an input. See PR # 189 .  Add ST_Intersection function. See Issue # 110  and PR # 189 .   GeoSpark Viz   Fixed Issue # 154 : GeoSpark kryp serializer and GeoSparkViz conflict. See PR # 178", 
            "title": "v1.1.0"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v101", 
            "text": "GeoSpark Core   Fixed Issue # 170   GeoSpark SQL   Fixed Issue # 171  Added the support of SparkSQL 2.2. GeoSpark-SQL for Spark 2.1 is published separately ( Maven Coordinates ).   GeoSpark Viz \nNone", 
            "title": "v1.0.1"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v100", 
            "text": "GeoSpark Core   Add GeoSparkConf class to read GeoSparkConf from SparkConf   GeoSpark SQL \n* Initial release: fully supports SQL/MM-Part3 Spatial SQL standard  GeoSpark Viz \n* Republish GeoSpark Viz under \"GeoSparkViz\" folder. All \"Babylon\" strings have been replaced to \"GeoSparkViz\"", 
            "title": "v1.0.0"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v091-geospark-core", 
            "text": "Bug fixes : Fixed \"Missing values when reading Shapefile\":  Issue #141  Performance improvement : Solved Issue  #91 ,  #103 ,  #104 ,  #125 ,  #150 .  Add GeoSpark customized Kryo Serializer to significantly reduce memory footprint. This serializer which follows Shapefile compression rule takes less memory than the default Kryo. See  PR 139 .  Delete the duplicate removal by using Reference Point concept. This eliminates one data shuffle but still guarantees the accuracy. See  PR 131 .    New Functionalities added :  SpatialJoinQueryFlat/DistanceJoinQueryFlat  returns the join query in a flat way following database iteration model: Each row has fixed two members [Polygon, Point]. This API is more efficient for unbalanced length of join results.   The left and right shapes in Range query, Distance query, Range join query, Distance join query can be switched.  The index side in Range query, Distance query, Range join query, Distance join query can be switched.  The generic SpatialRdd supports heterogenous geometries  Add KDB-Tree spatial partitioning method which is more balanced than Quad-Tree  Range query, Distance query, Range join query, Distance join query, KNN query supports heterogenous inputs.", 
            "title": "v0.9.1 (GeoSpark-core)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v082-geospark-core", 
            "text": "Bug fixes : Fix the shapefile RDD null pointer bug when running in cluster mode. See Issue https://github.com/DataSystemsLab/GeoSpark/issues/115  New function added : Provide granular control to SpatialRDD sampling utils. SpatialRDD has a setter and getter for a parameter called \"sampleNumber\". The user can manually specify the sample size for spatial partitioning.", 
            "title": "v0.8.2 (GeoSpark-core)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v081-geospark-core", 
            "text": "Bug fixes : (1) Fix the blank DBF attribute error when load DBF along with SHX file. (2) Allow user to call CRS transformation function at any time. Previously, it was only allowed in GeoSpark constructors", 
            "title": "v0.8.1 (GeoSpark-core)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#v080-geospark-core", 
            "text": "New input format added : GeoSpark is able to load and query ESRI ShapeFile (.shp, .shx, .dbf) from local disk and HDFS! Users first need to build a Shapefile RDD by giving Spark Context and an input path then call ShapefileRDD.getSpatialRDD to retrieve Spatial RDD. ( Scala Example ,  Java Example )  Join Query Performance enhancement 1 : GeoSpark provides a new Quad-Tree Spatial Partitioning method to speed up Join Query. Users need to pass GridType.QUADTREE parameter to RDD1.spatialPartitioning() function. Then users need to use RDD1.partitionTree in RDD2.spatialPartitioning() function. This Quad-Tree partitioning method (1) avoids overflowed spatial objects when partitioning spatial objects. (2) checking a spatial object against the Quad-Tree grids is completed in a log complexity tree search. ( Scala Example ,  Java Example )  Join Query Performance enhancement 2 : Internally, GeoSpark uses zipPartitions instead of CoGroup to join two Spatial RDD so that the incurred shuffle overhead decreases.  SpatialRDD Initialization Performance enhancement : GeoSpark uses mapPartition instead of flatMapToPair to generate Spatial Objects. This will speed up the calculation.  API changed : Since it chooses mapPartition API in mappers, GeoSpark no longer supports the old user supplified format mapper. However, if you are using your own format mapper for old GeoSpark version, you just need to add one more loop to fit in GeoSpark 0.8.0. Please see  GeoSpark user supplied format mapper examples  Alternative SpatialRDD constructor added : GeoSpark no longer forces users to provide StorageLevel parameter in their SpatialRDD constructors. This will siginicantly accelerate all Spatial RDD initialization.  If he only needs Spatial Range Query and KNN query, the user can totally remove this parameter from their constructors.  If he needs Spatial Join Query or Distance Join Query but he knows his dataset boundary and approximate total count, the user can also remove StorageLevel parameter and append a Envelope type dataset boundary and an approxmiate total count as additional parameters.  If he needs Spatial Join Query or Distance Join Query but knows nothing about his dataset , the user still has to pass StorageLevel parameter.  Bug fix : Fix bug  Issue #97  and  Issue #100 .      Version  Summary      0.7.0  Coordinate Reference System (CRS) Transformation (aka. Coordinate projection) added:  GeoSpark allows users to transform the original CRS (e.g., degree based coordinates such as EPSG:4326 and WGS84) to any other CRS (e.g., meter based coordinates such as EPSG:3857) so that it can accurately process both geographic data and geometrical data. Please specify your desired CRS in GeoSpark Spatial RDD constructor ( Example );  Unnecessary dependencies removed : NetCDF/HDF support depends on  SerNetCDF . SetNetCDF becomes optional dependency to reduce fat jar size;  Default JDK/JRE change to JDK/JRE 1.8 : To satisfy CRS transformation requirement, GeoSpark is compiled by JDK 1.8 by default;  Bug fix : fix a small format bug when output spatial RDD to disk.    0.6.2  New input format added:  Add a new input format mapper called EarthdataHDFPointMapper so that GeoSpark can load, query and save NASA Petabytes NetCDF/HDF Earth Data ( Scala Example , Java Example );  Bug fix:  Print UserData attribute when output Spatial RDDs as GeoJSON or regular text file    0.6.1  Bug fixes:  Fix typos LineString DistanceJoin API    0.6.0  Major updates:  (1) DistanceJoin is merged into JoinQuery. GeoSpark now supports complete DistanceJoin between Points, Polygons, and LineStrings. (2) Add Refine Phase to Spatial Range and Join Query. Use real polygon coordinates instead of its MBR to filter the final results.  API changes:  All spatial range and join  queries now take a parameter called  ConsiderBoundaryIntersection . This will tell GeoSpark whether returns the objects intersect with windows.    0.5.3  Bug fix:  Fix  Issue #69 : Now, if two objects have the same coordinates but different non-spatial attributes (UserData), GeoSpark treats them as different objects.    0.5.2  Bug fix:  Fix  Issue #58  and  Issue #60 ;  Performance enhancement:  (1) Deprecate all old Spatial RDD constructors. See the JavaDoc  here . (2) Recommend the new SRDD constructors which take an additional RDD storage level and automatically cache rawSpatialRDD to accelerate internal SRDD analyze step    0.5.1  Bug fix:  (1) GeoSpark: Fix inaccurate KNN result when K is large (2) GeoSpark: Replace incompatible Spark API call  Issue #55 ; (3) Babylon: Remove JPG output format temporarily due to the lack of OpenJDK support    0.5.0  Major updates:  We are pleased to announce the initial version of  Babylon  a large-scale in-memory geospatial visualization system extending GeoSpark. Babylon and GeoSpark are integrated together. You can just import GeoSpark and enjoy! More details are available here:  Babylon GeoSpatial Visualization    0.4.0  Major updates:  ( Example ) 1. Refactor constrcutor API usage. 2. Simplify Spatial Join Query API. 3. Add native support for LineStringRDD;  Functionality enhancement:  1. Release the persist function back to users. 2. Add more exception explanations.    0.3.2  Functionality enhancement: 1.  JTSplus Spatial Objects  now carry the original input data. Each object stores \"UserData\" and provides getter and setter. 2. Add a new SpatialRDD constructor to transform a regular data RDD to a spatial partitioned SpatialRDD.    0.3.1  Bug fix: Support Apache Spark 2.X version, fix a bug which results in inaccurate results when doing join query, add more unit test cases    0.3  Major updates: Significantly shorten query time on spatial join for skewed data; Support load balanced spatial partitioning methods (also serve as the global index); Optimize code for iterative spatial data mining    0.2  Improve code structure and refactor API    0.1  Support spatial range, join and Knn", 
            "title": "v0.8.0 (GeoSpark-core)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Release-notes/#geospark-viz-old", 
            "text": "Version  Summary      0.2.2  Add the support of new output storage : Now the user is able to output gigapixel or megapixel resolution images (image tiles or stitched single image) to HDFS and Amazon S3. Please use the new ImageGenerator not the BabylonImageGenerator class.    0.2.1  Performance enhancement : significantly accelerate single image generation pipeline.  Bug fix :fix a bug in scatter plot parallel rendering.    0.2.0  API updates for  Issue #80 :  1. Babylon now has two different OverlayOperators for raster image and vector image: RasterOverlayOperator and VectorOverlayOperator; 2. Babylon merged old SparkImageGenerator and NativeJavaGenerator into a new BabylonImageGenerator which has neat APIs;  New feature:  Babylon can use Scatter Plot to visualize NASA Petabytes NetCDF/HDF format Earth Data. ( Scala Example , Java Example )    0.1.1  Major updates:  Babylon supports vector image and outputs SVG image format    0.1.0  Major updates:  Babylon initial version supports raster images", 
            "title": "GeoSpark-Viz (old)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/", 
            "text": "Apache Spark 2.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId: org.datasyslab\nartifactId: geospark\nversion: 1.1.0\n\n\n\n\nGeoSpark-SQL\n\n\nFor SparkSQL-2.3\n\n\ngroupId: org.datasyslab\nartifactId: geospark-sql_2.3\nversion: 1.1.0\n\n\n\n\nFor SparkSQL-2.2\n\n\ngroupId: org.datasyslab\nartifactId: geospark-sql_2.2\nversion: 1.1.0\n\n\n\n\nFor SparkSQL-2.1\n\n\ngroupId: org.datasyslab\nartifactId: geospark-sql_2.1\nversion: 1.1.0\n\n\n\n\nGeoSpark-Viz\n\n\ngroupId: org.datasyslab\nartifactId: geospark-viz\nversion: 1.1.0\n\n\n\n\n\n\nApache Spark 1.X versions\n\n\nPlease add the following dependencies into your POM.xml or build.sbt\n\n\nGeoSpark-Core\n\n\ngroupId: org.datasyslab\nartifactId: geospark\nversion: 0.8.2-spark-1.x\n\n\n\n\nGeoSpark-Viz\n\n\ngroupId: org.datasyslab\nartifactId: babylon\nversion: 0.2.1-spark-1.x\n\n\n\n\n\n\nSNAPSHOT versions (only supports Spark 2.2)\n\n\nSometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase.\n\n\ngroupId: org.datasyslab\nartifactId: geospark\nversion: 1.0.0-SNAPSHOT\n\n\n\n\ngroupId: org.datasyslab\nartifactId: geospark-sql\nversion: 1.0.0-SNAPSHOT\n\n\n\n\ngroupId: org.datasyslab\nartifactId: geospark-viz\nversion: 1.0.0-SNAPSHOT\n\n\n\n\nIn order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt\n\n\nbuild.sbt\n\n\nresolvers +=\n  \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\"\n\n\nPOM.XML\n\n\nprofiles\n\n    \nprofile\n\n        \nid\nallow-snapshots\n/id\n\n        \nactivation\nactiveByDefault\ntrue\n/activeByDefault\n/activation\n\n        \nrepositories\n\n            \nrepository\n\n                \nid\nsnapshots-repo\n/id\n\n                \nurl\nhttps://oss.sonatype.org/content/repositories/snapshots\n/url\n\n                \nreleases\nenabled\nfalse\n/enabled\n/releases\n\n                \nsnapshots\nenabled\ntrue\n/enabled\n/snapshots\n\n            \n/repository\n\n        \n/repositories\n\n    \n/profile\n\n\n/profiles", 
            "title": "Maven Central Coordinate"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-2x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 2.X versions"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core", 
            "text": "groupId: org.datasyslab\nartifactId: geospark\nversion: 1.1.0", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-sql", 
            "text": "", 
            "title": "GeoSpark-SQL"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-23", 
            "text": "groupId: org.datasyslab\nartifactId: geospark-sql_2.3\nversion: 1.1.0", 
            "title": "For SparkSQL-2.3"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-22", 
            "text": "groupId: org.datasyslab\nartifactId: geospark-sql_2.2\nversion: 1.1.0", 
            "title": "For SparkSQL-2.2"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#for-sparksql-21", 
            "text": "groupId: org.datasyslab\nartifactId: geospark-sql_2.1\nversion: 1.1.0", 
            "title": "For SparkSQL-2.1"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz", 
            "text": "groupId: org.datasyslab\nartifactId: geospark-viz\nversion: 1.1.0", 
            "title": "GeoSpark-Viz"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#apache-spark-1x-versions", 
            "text": "Please add the following dependencies into your POM.xml or build.sbt", 
            "title": "Apache Spark 1.X versions"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-core_1", 
            "text": "groupId: org.datasyslab\nartifactId: geospark\nversion: 0.8.2-spark-1.x", 
            "title": "GeoSpark-Core"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#geospark-viz_1", 
            "text": "groupId: org.datasyslab\nartifactId: babylon\nversion: 0.2.1-spark-1.x", 
            "title": "GeoSpark-Viz"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#snapshot-versions-only-supports-spark-22", 
            "text": "Sometimes GeoSpark has a SNAPSHOT version for the upcoming release. \"SNAPSHOT\" is uppercase.  groupId: org.datasyslab\nartifactId: geospark\nversion: 1.0.0-SNAPSHOT  groupId: org.datasyslab\nartifactId: geospark-sql\nversion: 1.0.0-SNAPSHOT  groupId: org.datasyslab\nartifactId: geospark-viz\nversion: 1.0.0-SNAPSHOT  In order to download SNAPSHOTs, you need to add the following repositories in your POM.XML or build.sbt", 
            "title": "SNAPSHOT versions (only supports Spark 2.2)"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#buildsbt", 
            "text": "resolvers +=\n  \"Sonatype OSS Snapshots\" at \"https://oss.sonatype.org/content/repositories/snapshots\"", 
            "title": "build.sbt"
        }, 
        {
            "location": "/news/GeoSpark-All-Modules-Maven-Central-Coordinates/#pomxml", 
            "text": "profiles \n     profile \n         id allow-snapshots /id \n         activation activeByDefault true /activeByDefault /activation \n         repositories \n             repository \n                 id snapshots-repo /id \n                 url https://oss.sonatype.org/content/repositories/snapshots /url \n                 releases enabled false /enabled /releases \n                 snapshots enabled true /enabled /snapshots \n             /repository \n         /repositories \n     /profile  /profiles", 
            "title": "POM.XML"
        }, 
        {
            "location": "/news/publication/", 
            "text": "Publication\n\n\nJia Yu, Jinxuan Wu, Mohamed Sarwat. \n\"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\"\n. (demo paper) In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016\n\n\nJia Yu, Jinxuan Wu, Mohamed Sarwat. \n\"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\"\n. (short paper) In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015\n\n\nCurrently, we have published two papers about GeoSpark. Only these two papers are associated with GeoSpark Development Team.", 
            "title": "Publications"
        }, 
        {
            "location": "/news/publication/#publication", 
            "text": "Jia Yu, Jinxuan Wu, Mohamed Sarwat.  \"A Demonstration of GeoSpark: A Cluster Computing Framework for Processing Big Spatial Data\" . (demo paper) In Proceeding of IEEE International Conference on Data Engineering ICDE 2016, Helsinki, FI, May 2016  Jia Yu, Jinxuan Wu, Mohamed Sarwat.  \"GeoSpark: A Cluster Computing Framework for Processing Large-Scale Spatial Data\" . (short paper) In Proceeding of the ACM International Conference on Advances in Geographic Information Systems ACM SIGSPATIAL GIS 2015, Seattle, WA, USA November 2015  Currently, we have published two papers about GeoSpark. Only these two papers are associated with GeoSpark Development Team.", 
            "title": "Publication"
        }, 
        {
            "location": "/download/overview/", 
            "text": "Install GeoSpark\n\n\nDirect download\n\n\nGeoSpark source code is hosted on \nGeoSpark GitHub repository\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nGeoSpark GitHub Releases\n.\n\n\nGeoSpark pre-compiled JARs are hosted on \nMaven Central\n.\n\n\nHow to install GeoSpark\n\n\nBefore starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.\n\n\nThere are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.\n\n\n\n\nSpark interactive Scala shell: easy to start, good for new learners to try simple functions\n\n\nSelf-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "Overview"
        }, 
        {
            "location": "/download/overview/#install-geospark", 
            "text": "", 
            "title": "Install GeoSpark"
        }, 
        {
            "location": "/download/overview/#direct-download", 
            "text": "GeoSpark source code is hosted on  GeoSpark GitHub repository .  GeoSpark pre-compiled JARs are hosted on  GeoSpark GitHub Releases .  GeoSpark pre-compiled JARs are hosted on  Maven Central .", 
            "title": "Direct download"
        }, 
        {
            "location": "/download/overview/#how-to-install-geospark", 
            "text": "Before starting the GeoSpark journey, you need to make sure your Apache Spark cluster is ready.  There are two ways to use a Scala or Java library with Apache Spark. You can user either one to run GeoSpark.   Spark interactive Scala shell: easy to start, good for new learners to try simple functions  Self-contained Scala / Java project: a steep learning curve of package management, but good for large projects", 
            "title": "How to install GeoSpark"
        }, 
        {
            "location": "/download/cluster/", 
            "text": "Set up your Apache Spark cluster\n\n\nAfter downloading a Spark distribution from \nSpark download page\n, you will find this shell in \n./bin/\n folder.\n\n\nPreliminary\n\n\n\n\nSet up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.\n\n\nMake sure you have installed JRE 1.8 or later.\n\n\nAdd the list of your workers' IP address in ./conf/slaves\n\n\nBesides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:\n\n\n\n\nIn \n./conf/spark-defaults.conf\n\n\nspark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g\n\n\n\n\n\n\nspark.driver.memory\n tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.\n\n\nspark.network.timeout\n is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.\n\n\nspark.driver.maxResultSize\n is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.\n\n\n\n\nFor more details of Spark parameters, please visit \nSpark Website\n.\n\n\nStart your cluster\n\n\nGo the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal\n\n\n./sbin/start-all.sh", 
            "title": "Set up Spark cluser"
        }, 
        {
            "location": "/download/cluster/#set-up-your-apache-spark-cluster", 
            "text": "After downloading a Spark distribution from  Spark download page , you will find this shell in  ./bin/  folder.", 
            "title": "Set up your Apache Spark cluster"
        }, 
        {
            "location": "/download/cluster/#preliminary", 
            "text": "Set up password-less SSH on your cluster. Each master-worker pair should have bi-directional password-less SSH.  Make sure you have installed JRE 1.8 or later.  Add the list of your workers' IP address in ./conf/slaves  Besides the necessary Spark settings, you may need to add the following lines in Spark configuration files to avoid GeoSpark memory errors:   In  ./conf/spark-defaults.conf  spark.driver.memory 10g\nspark.network.timeout 1000s\nspark.driver.maxResultSize 5g   spark.driver.memory  tells Spark to allocate enough memory for the driver program because GeoSpark needs to build global grid files (global index) on the driver program. If you have a large amount of data (normally, over 100 GB), set this parameter to 2~5 GB will be good. Otherwise, you may observe \"out of memory\" error.  spark.network.timeout  is the default timeout for all network interactions. Sometimes, spatial join query takes longer time to shuffle data. This will ensure Spark has enough patience to wait for the result.  spark.driver.maxResultSize  is the limit of total size of serialized results of all partitions for each Spark action. Sometimes, the result size of spatial queries is large. The \"Collect\" operation may throw errors.   For more details of Spark parameters, please visit  Spark Website .", 
            "title": "Preliminary"
        }, 
        {
            "location": "/download/cluster/#start-your-cluster", 
            "text": "Go the root folder of the uncompressed Apache Spark folder. Start your spark cluster via a terminal  ./sbin/start-all.sh", 
            "title": "Start your cluster"
        }, 
        {
            "location": "/download/scalashell/", 
            "text": "Spark Scala shell\n\n\nSpark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.\n\n\nThis mode currently works with GeoSpark-core and GeoSparkViz. \nGeoSparkSQL cannot run under this mode\n. \n\n\nDownload GeoSpark jar automatically\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nRun Spark shell with \n--packages\n option. This command will automatically download GeoSpark jars from Maven Central.\n  \n./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n  \n./bin/spark-shell --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n  \n./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0\n\n\n\n\n\n\nDownload GeoSpark jar manually\n\n\n\n\n\n\nHave your Spark cluster ready.\n\n\n\n\n\n\nDownload GeoSpark jars:\n\n\n\n\nDownload the pre-compiled jars from \nGeoSpark Releases on GitHub\n\n\nDownload / Git clone GeoSpark source code and compile the code by yourself: \nmvn clean install -DskipTests\n\n\n\n\n\n\n\n\nRun Spark shell with \n--jars\n option.\n  \n./bin/spark-shell --jars /Path/To/GeoSparkJars.jar\n\n\n\n\n\n\nLocal mode: test GeoSpark without setting up a cluster\n  \n./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar\n\n\n\n\n\n\nCluster mode: you need to specify Spark Master IP\n\n\n./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#spark-scala-shell", 
            "text": "Spark distribution provides an interactive Scala shell that allows a user to execute Scala code in a terminal.  This mode currently works with GeoSpark-core and GeoSparkViz.  GeoSparkSQL cannot run under this mode .", 
            "title": "Spark Scala shell"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-automatically", 
            "text": "Have your Spark cluster ready.    Run Spark shell with  --packages  option. This command will automatically download GeoSpark jars from Maven Central.\n   ./bin/spark-shell --packages org.datasyslab:geospark:GEOSPARK_VERSION    Local mode: test GeoSpark without setting up a cluster\n   ./bin/spark-shell --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0    Cluster mode: you need to specify Spark Master IP\n   ./bin/spark-shell --master spark://localhost:7077 --packages org.datasyslab:geospark:1.1.0,org.datasyslab:geospark-viz:1.1.0", 
            "title": "Download GeoSpark jar automatically"
        }, 
        {
            "location": "/download/scalashell/#download-geospark-jar-manually", 
            "text": "Have your Spark cluster ready.    Download GeoSpark jars:   Download the pre-compiled jars from  GeoSpark Releases on GitHub  Download / Git clone GeoSpark source code and compile the code by yourself:  mvn clean install -DskipTests     Run Spark shell with  --jars  option.\n   ./bin/spark-shell --jars /Path/To/GeoSparkJars.jar    Local mode: test GeoSpark without setting up a cluster\n   ./bin/spark-shell --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar    Cluster mode: you need to specify Spark Master IP  ./bin/spark-shell --master spark://localhost:7077 --jars geospark-1.0.1.jar,geospark-viz-1.0.1.jar", 
            "title": "Download GeoSpark jar manually"
        }, 
        {
            "location": "/download/project/", 
            "text": "Self-contained Spark projects\n\n\nA self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.\n\n\nQuick start\n\n\n\n\nTo add GeoSpark as dependencies, please read \nGeoSpark Maven Central coordinates\n\n\nUse GeoSpark Template project to start: \nGeoSpark Template Project\n\n\nCompile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.\n\n\nSubmit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:\n\n\n\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar\n\n\n\n\nThe detailed explanation of spark-submit is available on \nSpark website\n.\n\n\nHow to use GeoSpark in an IDE\n\n\nSelect an IDE\n\n\nTo develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.\n\n\nEclipse is also fine if you just want to use Java and Maven.\n\n\nOpen GeoSpark template project\n\n\nSelect a proper GeoSpark project you want from \nGeoSpark Template Project\n. In this tutorial, we use GeoSparkSQL Scala project as an example.\n\n\nOpen the folder that contains \nbuild.sbt\n file in your IDE. The IDE may take a while to index dependencies and source code.\n\n\nTry GeoSpark SQL functions\n\n\nIn your IDE, run \nScalaExample.scala\n file.\n\n\nYou don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.\n\n\nPackage the project\n\n\nTo run this project in cluster mode, you have to package this project to a JAR and then run it using \nspark-submit\n command.\n\n\nBefore packaging this project, you always need to check two places:\n\n\n\n\nRemove the hardcoded Master IP \nmaster(\"local[*]\")\n. This hardcoded IP is only needed when you run this project in an IDE.\n\n\n\n\n    var sparkSession:SparkSession = SparkSession.builder()\n    .config(\nspark.serializer\n,classOf[KryoSerializer].getName)\n    .config(\nspark.kryo.registrator\n,classOf[GeoSparkKryoRegistrator].getName)\n    .master(\nlocal[*]\n)\n    .appName(\nGeoSparkSQL-demo\n).getOrCreate()\n\n\n\n\n\n\nIn build.sbt (or POM.xml), set Spark dependency scope to \nprovided\n instead of \ncompile\n. \ncompile\n is only needed when you run this project in an IDE.\n\n\n\n\norg.apache.spark\n %% \nspark-core\n % SparkVersion % \ncompile,\norg.apache.spark\n %% \nspark-sql\n % SparkVersion % \ncompile\n\n\n\n\nForgetting to change this scope will lead to a very big fat JAR and dependency conflicts when call \nspark-submit\n. For more details, please visit \nMaven Dependency Scope\n.\n\n\n\n\nMake sure your downloaded Spark binary distribution is the same version with the Spark used in your \nbuild.sbt\n or \nPOM.xml\n.\n\n\n\n\nSubmit the compiled jar\n\n\n\n\nGo to \n./target/scala-2.11\n folder and find a jar called \nGeoSparkSQLScalaTemplate-0.1.0.jar\n. Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under \n./target\n folder)\n\n\n\n\nSubmit this JAR using \nspark-submit\n.\n\n\n\n\n\n\nLocal mode:\n\n\n\n\n\n\n./bin/spark-submit /Path/To/YourJar.jar\n\n\n\n\n\n\nCluster mode:\n\n\n\n\n./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Self-contain project"
        }, 
        {
            "location": "/download/project/#self-contained-spark-projects", 
            "text": "A self-contained project allows you to create multiple Scala / Java files and write complex logics in one place. To use GeoSpark in your self-contained Spark project, you just need to add GeoSpark as a dependency in your POM.xml or build.sbt.", 
            "title": "Self-contained Spark projects"
        }, 
        {
            "location": "/download/project/#quick-start", 
            "text": "To add GeoSpark as dependencies, please read  GeoSpark Maven Central coordinates  Use GeoSpark Template project to start:  GeoSpark Template Project  Compile your project using SBT or Maven. Make sure you obtain the fat jar which packages all dependencies.  Submit your compiled fat jar to Spark cluster. Make sure you are in the root folder of Spark distribution. Then run the following command:   ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar  The detailed explanation of spark-submit is available on  Spark website .", 
            "title": "Quick start"
        }, 
        {
            "location": "/download/project/#how-to-use-geospark-in-an-ide", 
            "text": "", 
            "title": "How to use GeoSpark in an IDE"
        }, 
        {
            "location": "/download/project/#select-an-ide", 
            "text": "To develop a complex GeoSpark project, we suggest you use IntelliJ IDEA. It supports JVM languages, Scala and Java, and many dependency management systems, Maven and SBT.  Eclipse is also fine if you just want to use Java and Maven.", 
            "title": "Select an IDE"
        }, 
        {
            "location": "/download/project/#open-geospark-template-project", 
            "text": "Select a proper GeoSpark project you want from  GeoSpark Template Project . In this tutorial, we use GeoSparkSQL Scala project as an example.  Open the folder that contains  build.sbt  file in your IDE. The IDE may take a while to index dependencies and source code.", 
            "title": "Open GeoSpark template project"
        }, 
        {
            "location": "/download/project/#try-geospark-sql-functions", 
            "text": "In your IDE, run  ScalaExample.scala  file.  You don't need to change anything in this file. The IDE will run all SQL queries in this example in local mode.", 
            "title": "Try GeoSpark SQL functions"
        }, 
        {
            "location": "/download/project/#package-the-project", 
            "text": "To run this project in cluster mode, you have to package this project to a JAR and then run it using  spark-submit  command.  Before packaging this project, you always need to check two places:   Remove the hardcoded Master IP  master(\"local[*]\") . This hardcoded IP is only needed when you run this project in an IDE.       var sparkSession:SparkSession = SparkSession.builder()\n    .config( spark.serializer ,classOf[KryoSerializer].getName)\n    .config( spark.kryo.registrator ,classOf[GeoSparkKryoRegistrator].getName)\n    .master( local[*] )\n    .appName( GeoSparkSQL-demo ).getOrCreate()   In build.sbt (or POM.xml), set Spark dependency scope to  provided  instead of  compile .  compile  is only needed when you run this project in an IDE.   org.apache.spark  %%  spark-core  % SparkVersion %  compile,\norg.apache.spark  %%  spark-sql  % SparkVersion %  compile  Forgetting to change this scope will lead to a very big fat JAR and dependency conflicts when call  spark-submit . For more details, please visit  Maven Dependency Scope .   Make sure your downloaded Spark binary distribution is the same version with the Spark used in your  build.sbt  or  POM.xml .", 
            "title": "Package the project"
        }, 
        {
            "location": "/download/project/#submit-the-compiled-jar", 
            "text": "Go to  ./target/scala-2.11  folder and find a jar called  GeoSparkSQLScalaTemplate-0.1.0.jar . Note that, this JAR normally is larger than 1MB. (If you use POM.xml, the jar is under  ./target  folder)   Submit this JAR using  spark-submit .    Local mode:    ./bin/spark-submit /Path/To/YourJar.jar   Cluster mode:   ./bin/spark-submit --master spark://YOUR-IP:7077 /Path/To/YourJar.jar", 
            "title": "Submit the compiled jar"
        }, 
        {
            "location": "/download/compile/", 
            "text": "Compile GeoSpark source code\n\n\nSome GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:\n\n\n\n\nDownload / Git clone GeoSpark source code from \nGeoSpark Github repo\n.\n\n\n\n\nGeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3. \n\n\n\n\nMake sure your machine has Java 1.8 and Apache Maven 3.\n\n\n\n\nTo compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:\n\n\nmvn clean install -DskipTests\n\n\n\n\nThis command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.\n\n\nTo compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.\n\n\nTo run unit tests, just simply remove \n-DskipTests\n option. The command is like this:\n\n\nmvn clean install\n\n\n\n\nNote, the unit tests of all three modules may take up to 20 minutes.", 
            "title": "Compile the source code"
        }, 
        {
            "location": "/download/compile/#compile-geospark-source-code", 
            "text": "Some GeoSpark hackers may want to change some source code to fit in their own scenarios. To compile GeoSpark source code, you first need to download GeoSpark source code:   Download / Git clone GeoSpark source code from  GeoSpark Github repo .   GeoSpark is a a project with three modules, core, sql, and viz. Each module is a Scala/Java mixed project which is managed by Apache Maven 3.    Make sure your machine has Java 1.8 and Apache Maven 3.   To compile all modules, please make sure you are in the root folder of three modules. Then enter the following command in the terminal:  mvn clean install -DskipTests  This command will first delete the old binary files and compile all three modules. This compilation will skip the unit tests of GeoSpark.  To compile a module of GeoSpark, please make sure you are in the folder of that module. Then enter the same command.  To run unit tests, just simply remove  -DskipTests  option. The command is like this:  mvn clean install  Note, the unit tests of all three modules may take up to 20 minutes.", 
            "title": "Compile GeoSpark source code"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/", 
            "text": "Scala and Java API\n\n\nGeoSpark Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/geospark/javadoc/\n\n\nThe \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala/Java doc"
        }, 
        {
            "location": "/api/GeoSpark-Scala-and-Java-API/#scala-and-java-api", 
            "text": "GeoSpark Scala and Java API:  http://www.public.asu.edu/~jiayu2/geospark/javadoc/  The \"SNAPSHOT\" folder has the API for the latest GeoSpark SNAPSHOT version.  Note: Scala can call Java APIs seamlessly. That means GeoSpark Scala users use the same APIs with GeoSpark Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/", 
            "text": "Introduction\n\n\nGeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through sparkSession.sql(\"YOUR_SQL\").\n\n\n\n\nConstructor: Construct a Geometry given an input string or coordinates\n\n\nExample: ST_GeomFromWKT (string). Create a Geometry from a WKT String.\n\n\nDocumentation: \nHere\n\n\nFunction:\n\n\nExample: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.\n\n\nDocumentation: \nHere\n\n\nAggregate function\n\n\nExample: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.\n\n\nDocumentation: \nHere\n\n\nPredicate\n\n\nExample: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".\n\n\nDocumentation: \nHere\n\n\n\n\nGeoSparkSQL supports SparkSQL query optimizer, documentation is \nHere\n\n\nUsage\n\n\n\n\nAdd GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt\n\n\nDeclare your Spark Session\n\n\n\n\nsparkSession = SparkSession.builder().\n      config(\nspark.serializer\n,classOf[KryoSerializer].getName).\n      config(\nspark.kryo.registrator\n, classOf[GeoSparkKryoRegistrator].getName).\n      master(\nlocal[*]\n).appName(\nmyGeoSparkSQLdemo\n).getOrCreate()\n\n\n\n\n\n\nAdd the following line after your SparkSession declaration:\n\n\n\n\nGeoSparkSQLRegistrator.registerAll(sparkSession.sqlContext)", 
            "title": "Overview"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#introduction", 
            "text": "GeoSparkSQL supports SQL/MM Part3 Spatial SQL Standard. It includes four kinds of SQL operators as follows. All these operators can be directly called through sparkSession.sql(\"YOUR_SQL\").   Constructor: Construct a Geometry given an input string or coordinates  Example: ST_GeomFromWKT (string). Create a Geometry from a WKT String.  Documentation:  Here  Function:  Example: ST_Distance (A, B). Given two Geometry A and B, return the Euclidean distance of A and B.  Documentation:  Here  Aggregate function  Example: ST_Envelope_Aggr (Geometry column). Given a Geometry column, calculate the entire envelope boundary of this column.  Documentation:  Here  Predicate  Example: ST_Contains (A, B). Check if A fully contains B. Return \"True\" if yes, else return \"False\".  Documentation:  Here   GeoSparkSQL supports SparkSQL query optimizer, documentation is  Here", 
            "title": "Introduction"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Overview/#usage", 
            "text": "Add GeoSpark-core and GeoSparkSQL into your project POM.xml or build.sbt  Declare your Spark Session   sparkSession = SparkSession.builder().\n      config( spark.serializer ,classOf[KryoSerializer].getName).\n      config( spark.kryo.registrator , classOf[GeoSparkKryoRegistrator].getName).\n      master( local[*] ).appName( myGeoSparkSQLdemo ).getOrCreate()   Add the following line after your SparkSession declaration:   GeoSparkSQLRegistrator.registerAll(sparkSession.sqlContext)", 
            "title": "Usage"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/", 
            "text": "ST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Geometry from Wkt. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_GeomFromWKT(polygontable._c0) as polygonshape from polygontable\n\n\n\n\nST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Geometry from GeoJson. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nvar polygonJsonDf = sparkSession.read.format(\ncsv\n).option(\ndelimiter\n,\n\\t\n).option(\nheader\n,\nfalse\n).load(geoJsonGeomInputLocation)\npolygonJsonDf.createOrReplaceTempView(\npolygontable\n)\npolygonJsonDf.show()\nvar polygonDf = sparkSession.sql(\nselect ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable\n)\npolygonDf.show()\n\n\n\n\nRegarding how to read JSON files in Spark, read \nthe document from DataBricks\n\n\nST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Point from X and Y. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as pointshape from pointtable\n\n\n\n\nST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nSpark SQL example:\n\n\nselect ST_PointFromText(pointtable._c0,',') as pointshape from pointtable\n\n\n\n\nST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Polygon from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_PolygonFromText(polygontable._c0,',') as polygonshape from polygontable\n\n\n\n\nST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_LineStringFromText(linestringtable._c0,',') as linestringshape from linestringtable\n\n\n\n\nST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)\n\n\nIntroduction:\n\n\nConstruct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect * from pointdf where ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.pointshape)\n\n\n\n\nST_Circle (A:Geometry, Radius:decimal)\n\n\nIntroduction:\n\n\nConstruct a Circle from A with a Radius.\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Circle(pointdf.pointshape, 1.0) from pointdf)\n\n\n\n\nConstruct DataFrame from ESRI Shapefile\n\n\nIntroduction:\n\n\nConstruct a DataFrame from a Shapefile\n\n\nSince: v1.0.0\n\n\nSparkSQL example:\n\n\nvar spatialRDD = new SpatialRDD[Geometry]\nspatialRDD.rawSpatialRDD = ShapefileReader.readToGeometryRDD(sparkSession.sparkContext, shapefileInputLocation)\nvar rawSpatialDf = Adapter.toDf(spatialRDD,sparkSession)\nrawSpatialDf.createOrReplaceTempView(\nrawSpatialDf\n)\nvar spatialDf = sparkSession.sql(\nSELECT ST_GeomFromWKT(rddshape), _c1, _c2 FROM rawSpatialDf\n)\nspatialDf.show()\nspatialDf.printSchema()", 
            "title": "Constructor"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromwkt-wktstring-uuid1-uuid2", 
            "text": "Introduction:  Construct a Geometry from Wkt. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  select ST_GeomFromWKT(polygontable._c0) as polygonshape from polygontable", 
            "title": "ST_GeomFromWKT (Wkt:string, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_geomfromgeojson-geojsonstring-uuid1-uuid2", 
            "text": "Introduction:  Construct a Geometry from GeoJson. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  var polygonJsonDf = sparkSession.read.format( csv ).option( delimiter , \\t ).option( header , false ).load(geoJsonGeomInputLocation)\npolygonJsonDf.createOrReplaceTempView( polygontable )\npolygonJsonDf.show()\nvar polygonDf = sparkSession.sql( select ST_GeomFromGeoJSON(polygontable._c0) as countyshape from polygontable )\npolygonDf.show()  Regarding how to read JSON files in Spark, read  the document from DataBricks", 
            "title": "ST_GeomFromGeoJSON (GeoJson:string, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_point-xdecimal-ydecimal-uuid1-uuid2", 
            "text": "Introduction:  Construct a Point from X and Y. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  select ST_Point(cast(pointtable._c0 as Decimal(24,20)), cast(pointtable._c1 as Decimal(24,20))) as pointshape from pointtable", 
            "title": "ST_Point (X:decimal, Y:decimal, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_pointfromtext-textstring-delimiterchar-uuid1-uuid2", 
            "text": "Introduction:  Construct a Point from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Spark SQL example:  select ST_PointFromText(pointtable._c0,',') as pointshape from pointtable", 
            "title": "ST_PointFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromtext-textstring-delimiterchar-uuid1-uuid2", 
            "text": "Introduction:  Construct a Polygon from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  select ST_PolygonFromText(polygontable._c0,',') as polygonshape from polygontable", 
            "title": "ST_PolygonFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_linestringfromtext-textstring-delimiterchar-uuid1-uuid2", 
            "text": "Introduction:  Construct a LineString from Text, delimited by Delimiter. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  select ST_LineStringFromText(linestringtable._c0,',') as linestringshape from linestringtable", 
            "title": "ST_LineStringFromText (Text:string, Delimiter:char, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_polygonfromenvelope-minxdecimal-minydecimal-maxxdecimal-maxydecimal-uuid1-uuid2", 
            "text": "Introduction:  Construct a Polygon from MinX, MinY, MaxX, MaxY. Unlimited UUID strings can be appended.  Since: v1.0.0  Spark SQL example:  select * from pointdf where ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.pointshape)", 
            "title": "ST_PolygonFromEnvelope (MinX:decimal, MinY:decimal, MaxX:decimal, MaxY:decimal, UUID1, UUID2, ...)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#st_circle-ageometry-radiusdecimal", 
            "text": "Introduction:  Construct a Circle from A with a Radius.  Since: v1.0.0  Spark SQL example:  select ST_Circle(pointdf.pointshape, 1.0) from pointdf)", 
            "title": "ST_Circle (A:Geometry, Radius:decimal)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Constructor/#construct-dataframe-from-esri-shapefile", 
            "text": "Introduction:  Construct a DataFrame from a Shapefile  Since: v1.0.0  SparkSQL example:  var spatialRDD = new SpatialRDD[Geometry]\nspatialRDD.rawSpatialRDD = ShapefileReader.readToGeometryRDD(sparkSession.sparkContext, shapefileInputLocation)\nvar rawSpatialDf = Adapter.toDf(spatialRDD,sparkSession)\nrawSpatialDf.createOrReplaceTempView( rawSpatialDf )\nvar spatialDf = sparkSession.sql( SELECT ST_GeomFromWKT(rddshape), _c1, _c2 FROM rawSpatialDf )\nspatialDf.show()\nspatialDf.printSchema()", 
            "title": "Construct DataFrame from ESRI Shapefile"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/", 
            "text": "ST_Distance (A:geometry, B:geometry)\n\n\nIntroduction:\n\n\nReturn the Euclidean distance between A and B\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Distance(polygondf.countyshape, polygondf.countyshape) from polygondf\n\n\n\n\nST_ConvexHull (A:geometry)\n\n\nIntroduction:\n\n\nReturn the Convex Hull of polgyon A\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_ConvexHull(polygondf.countyshape) from polygondf\n\n\n\n\nST_Envelope (A:geometry)\n\n\nIntroduction:\n\n\nReturn the envelop boundary of A\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Envelope(polygondf.countyshape) from polygondf\n\n\n\n\nST_Length (A:geometry)\n\n\nIntroduction:\n\n\nReturn the perimeter of A\n\n\nSpark SQL example:\n\n\nselect ST_Length(polygondf.countyshape) from polygondf\n\n\n\n\nST_Area (A:geometry)\n\n\nIntroduction:\n\n\nReturn the area of A\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Area(polygondf.countyshape) from polygondf\n\n\n\n\nST_Centroid (A:geometry)\n\n\nIntroduction:\n\n\nReturn the centroid point of A\n\n\nSpark SQL example:\n\n\nselect ST_Centroid(polygondf.countyshape) from polygondf\n\n\n\n\nST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)\n\n\nIntroduction:\n\n\nTransform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS\n\n\nBy default, ST_Transform assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".\n\n\nIf your ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.\n\n\nSince: v1.0.0\n\n\nSpark SQL example (simple):\n\n\nselect ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857',true, false) from polygondf\n\n\n\n\nSpark SQL example (with optional parameters):\n\n\nselect ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857',true, false) from polygondf\n\n\n\n\nST_Intersection (A:geometry, B:geometry)\n\n\nIntroduction:\n\n\nReturn the intersection geometry of A and B\n\n\nSince: v1.1.0\n\n\nSpark SQL example:\n\n\nselect ST_Intersection(polygondf.countyshape, polygondf.countyshape) from polygondf", 
            "title": "Function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_distance-ageometry-bgeometry", 
            "text": "Introduction:  Return the Euclidean distance between A and B  Since: v1.0.0  Spark SQL example:  select ST_Distance(polygondf.countyshape, polygondf.countyshape) from polygondf", 
            "title": "ST_Distance (A:geometry, B:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_convexhull-ageometry", 
            "text": "Introduction:  Return the Convex Hull of polgyon A  Since: v1.0.0  Spark SQL example:  select ST_ConvexHull(polygondf.countyshape) from polygondf", 
            "title": "ST_ConvexHull (A:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_envelope-ageometry", 
            "text": "Introduction:  Return the envelop boundary of A  Since: v1.0.0  Spark SQL example:  select ST_Envelope(polygondf.countyshape) from polygondf", 
            "title": "ST_Envelope (A:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_length-ageometry", 
            "text": "Introduction:  Return the perimeter of A  Spark SQL example:  select ST_Length(polygondf.countyshape) from polygondf", 
            "title": "ST_Length (A:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_area-ageometry", 
            "text": "Introduction:  Return the area of A  Since: v1.0.0  Spark SQL example:  select ST_Area(polygondf.countyshape) from polygondf", 
            "title": "ST_Area (A:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_centroid-ageometry", 
            "text": "Introduction:  Return the centroid point of A  Spark SQL example:  select ST_Centroid(polygondf.countyshape) from polygondf", 
            "title": "ST_Centroid (A:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_transform-ageometry-sourcecrsstring-targetcrsstring-optional-uselongitudelatitudeorderboolean-optional-disableerror", 
            "text": "Introduction:  Transform the Spatial Reference System / Coordinate Reference System of A, from SourceCRS to TargetCRS  By default, ST_Transform assumes Longitude/Latitude is your coordinate X/Y. If this is not the case, set UseLongitudeLatitudeOrder as \"false\".  If your ST_Transform throws an Exception called \"Bursa wolf parameters required\", you need to disable the error notification in ST_Transform. You can append a boolean value at the end.  Since: v1.0.0  Spark SQL example (simple):  select ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857',true, false) from polygondf  Spark SQL example (with optional parameters):  select ST_Transform(polygondf.countyshape, 'epsg:4326','epsg:3857',true, false) from polygondf", 
            "title": "ST_Transform (A:geometry, SourceCRS:string, TargetCRS:string, [Optional] UseLongitudeLatitudeOrder:Boolean, [Optional] DisableError)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Function/#st_intersection-ageometry-bgeometry", 
            "text": "Introduction:  Return the intersection geometry of A and B  Since: v1.1.0  Spark SQL example:  select ST_Intersection(polygondf.countyshape, polygondf.countyshape) from polygondf", 
            "title": "ST_Intersection (A:geometry, B:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/", 
            "text": "ST_Contains (A:geometry, B:geometry)\n\n\nIntroduction:\n\n\nReturn true if A fully contains B\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect * from pointdf where ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n\n\n\n\nST_Intersects (A:geometry, B:geometry)\n\n\nIntroduction:\n\n\nReturn true if A intersects B\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect * from pointdf where ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)\n\n\n\n\nST_Within (A:geometry, B:geometry)\n\n\nIntroduction:\n\n\nReturn true if A is fully contained by B\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect * from pointdf where ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))", 
            "title": "Predicate"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_contains-ageometry-bgeometry", 
            "text": "Introduction:  Return true if A fully contains B  Since: v1.0.0  Spark SQL example:  select * from pointdf where ST_Contains(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)", 
            "title": "ST_Contains (A:geometry, B:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_intersects-ageometry-bgeometry", 
            "text": "Introduction:  Return true if A intersects B  Since: v1.0.0  Spark SQL example:  select * from pointdf where ST_Intersects(ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0), pointdf.arealandmark)", 
            "title": "ST_Intersects (A:geometry, B:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Predicate/#st_within-ageometry-bgeometry", 
            "text": "Introduction:  Return true if A is fully contained by B  Since: v1.0.0  Spark SQL example:  select * from pointdf where ST_Within(pointdf.arealandmark, ST_PolygonFromEnvelope(1.0,100.0,1000.0,1100.0))", 
            "title": "ST_Within (A:geometry, B:geometry)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/", 
            "text": "ST_Envelope_Aggr (A:geometryColumn)\n\n\nIntroduction:\n\n\nReturn the entire envelope boundary of all geometries in A\n\n\nSince: v1.0.0\n\n\nSpark SQL example:\n\n\nselect ST_Envelope_Aggr(pointdf.arealandmark) from pointdf\n\n\n\n\nST_Union_Aggr (A:geometryColumn)\n\n\nIntroduction:\n\n\nSince: v1.0.0\n\n\nReturn the polygon union of all polygons in A\n\n\nSpark SQL example:\n\n\nselect ST_Union_Aggr(polygondf.polygonshape) from polygondf", 
            "title": "Aggregate Function"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_envelope_aggr-ageometrycolumn", 
            "text": "Introduction:  Return the entire envelope boundary of all geometries in A  Since: v1.0.0  Spark SQL example:  select ST_Envelope_Aggr(pointdf.arealandmark) from pointdf", 
            "title": "ST_Envelope_Aggr (A:geometryColumn)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-AggregateFunction/#st_union_aggr-ageometrycolumn", 
            "text": "Introduction:  Since: v1.0.0  Return the polygon union of all polygons in A  Spark SQL example:  select ST_Union_Aggr(polygondf.polygonshape) from polygondf", 
            "title": "ST_Union_Aggr (A:geometryColumn)"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/", 
            "text": "GeoSparkSQL query optimizer\n\n\nGeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:\n\n Automatically optimizes range join query and distance join query.\n\n Automatically performs predicate pushdown.\n\n\nRange join\n\n\nIntroduction:\n\n\nFind geometries from A and geometries from B such that each geometry pair satisfies a certain predicate\n\n\nSpark SQL Example:\n\n\nselect * from polygondf, pointdf where ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nselect * from polygondf, pointdf where ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\nselect * from pointdf, polygondf where ST_Within(pointdf.pointshape, polygondf.polygonshape)\n\n\n\n\n\nSpark SQL Physical plan:\n\n\n== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv\n\n\n\n\n\nDistance join\n\n\nIntroduction:\n\n\nFind geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance\n\n\nSpark SQL Example:\n\n\nOnly consider \"fully within a certain distance\"\n\n\nselect * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) \n 2\n\n\n\n\nConsider \"intersects within a certain distance\"\n\n\nselect * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2) \n= 2\n\n\n\n\nSpark SQL Physical plan:\n\n\n== Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv\n\n\n\n\n\nPredicate pushdown\n\n\nIntroduction:\n\nGiven a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query\n\n\nSpark SQL Example:\n\n\nselect * from polygondf, pointdf \n\nwhere ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n\nand ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)\n\n\n\n\nSpark SQL Physical plan:\n\n\n== Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.geosparksql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv", 
            "title": "Optimizer"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#geosparksql-query-optimizer", 
            "text": "GeoSpark Spatial operators fully supports Apache SparkSQL query optimizer. It has the following query optimization features:  Automatically optimizes range join query and distance join query.  Automatically performs predicate pushdown.", 
            "title": "GeoSparkSQL query optimizer"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#range-join", 
            "text": "Introduction:  Find geometries from A and geometries from B such that each geometry pair satisfies a certain predicate  Spark SQL Example:  select * from polygondf, pointdf where ST_Contains(polygondf.polygonshape,pointdf.pointshape)\nselect * from polygondf, pointdf where ST_Intersects(polygondf.polygonshape,pointdf.pointshape)\nselect * from pointdf, polygondf where ST_Within(pointdf.pointshape, polygondf.polygonshape)  Spark SQL Physical plan:  == Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv", 
            "title": "Range join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#distance-join", 
            "text": "Introduction:  Find geometries from A and geometries from B such that the internal Euclidean distance of each geometry pair is less or equal than a certain distance  Spark SQL Example:  Only consider \"fully within a certain distance\"  select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2)   2  Consider \"intersects within a certain distance\"  select * from pointdf1, pointdf2 where ST_Distance(pointdf1.pointshape1,pointdf2.pointshape2)  = 2  Spark SQL Physical plan:  == Physical Plan ==\nDistanceJoin pointshape1#12: geometry, pointshape2#33: geometry, 2.0, true\n:- Project [st_point(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), myPointId) AS pointshape1#12]\n:  +- *FileScan csv\n+- Project [st_point(cast(_c0#21 as decimal(24,20)), cast(_c1#22 as decimal(24,20)), myPointId) AS pointshape2#33]\n   +- *FileScan csv", 
            "title": "Distance join"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Optimizer/#predicate-pushdown", 
            "text": "Introduction: Given a join query and a predicate in the same WHERE clause, first executes the Predicate as a filter, then executes the join query  Spark SQL Example:  select * from polygondf, pointdf \n\nwhere ST_Contains(polygondf.polygonshape,pointdf.pointshape)\n\nand ST_Contains(ST_PolygonFromEnvelope(1.0,101.0,501.0,601.0), polygondf.polygonshape)  Spark SQL Physical plan:  == Physical Plan ==\nRangeJoin polygonshape#20: geometry, pointshape#43: geometry, false\n:- Project [st_polygonfromenvelope(cast(_c0#0 as decimal(24,20)), cast(_c1#1 as decimal(24,20)), cast(_c2#2 as decimal(24,20)), cast(_c3#3 as decimal(24,20)), mypolygonid) AS polygonshape#20]\n:  +- Filter  **org.apache.spark.sql.geosparksql.expressions.ST_Contains$**\n:     +- *FileScan csv\n+- Project [st_point(cast(_c0#31 as decimal(24,20)), cast(_c1#32 as decimal(24,20)), myPointId) AS pointshape#43]\n   +- *FileScan csv", 
            "title": "Predicate pushdown"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/", 
            "text": "Usage\n\n\nGeoSparkSQL supports many parameters. To change their values,\n1. Set it through SparkConf:\n\n\nsparkSession = SparkSession.builder().\n      config(\nspark.serializer\n,classOf[KryoSerializer].getName).\n      config(\nspark.kryo.registrator\n, classOf[GeoSparkKryoRegistrator].getName).\n      config(\ngeospark.global.index\n,\ntrue\n)\n      master(\nlocal[*]\n).appName(\nmyGeoSparkSQLdemo\n).getOrCreate()\n\n\n\n\n\n\nCheck your current GeoSparkSQL configuration:\n\n\n\n\n      val geosparkConf = new GeoSparkConf(sparkSession.sparkContext.getConf)\n      println(geosparkConf)\n\n\n\n\nExplanation\n\n\n\n\ngeospark.global.index\n\n\nUse spatial index (currently, only supports in SQL range join and SQL distance join)\n\n\nDefault: true\n\n\nPossible values: true, false\n\n\ngeospark.global.indextype\n\n\nSpatial index type, only valid when \"geospark.global.index\" is true\n\n\nDefault: rtree\n\n\nPossible values: rtree, quadtree\n\n\ngeospark.join.gridtype\n\n\nSpatial partitioning grid type for join query\n\n\nDefault: quadtree\n\n\nPossible values: quadtree, kdbtree, rtree, voronoi\n\n\ngeospark.join.numpartition \n(Advanced users only!)\n\n\nNumber of partitions for both sides in a join query\n\n\nDefault: -1, which means use the existing partitions\n\n\nPossible values: any integers\n\n\ngeospark.join.indexbuildside \n(Advanced users only!)\n\n\nThe side which GeoSpark builds spatial indices on\n\n\nDefault: left\n\n\nPossible values: left, right\n\n\ngeospark.join.spatitionside \n(Advanced users only!)\n\n\nThe dominant side in spatial partitioning stage\n\n\nDefault: left\n\n\nPossible values: left, right", 
            "title": "Parameter"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#usage", 
            "text": "GeoSparkSQL supports many parameters. To change their values,\n1. Set it through SparkConf:  sparkSession = SparkSession.builder().\n      config( spark.serializer ,classOf[KryoSerializer].getName).\n      config( spark.kryo.registrator , classOf[GeoSparkKryoRegistrator].getName).\n      config( geospark.global.index , true )\n      master( local[*] ).appName( myGeoSparkSQLdemo ).getOrCreate()   Check your current GeoSparkSQL configuration:         val geosparkConf = new GeoSparkConf(sparkSession.sparkContext.getConf)\n      println(geosparkConf)", 
            "title": "Usage"
        }, 
        {
            "location": "/api/sql/GeoSparkSQL-Parameter/#explanation", 
            "text": "geospark.global.index  Use spatial index (currently, only supports in SQL range join and SQL distance join)  Default: true  Possible values: true, false  geospark.global.indextype  Spatial index type, only valid when \"geospark.global.index\" is true  Default: rtree  Possible values: rtree, quadtree  geospark.join.gridtype  Spatial partitioning grid type for join query  Default: quadtree  Possible values: quadtree, kdbtree, rtree, voronoi  geospark.join.numpartition  (Advanced users only!)  Number of partitions for both sides in a join query  Default: -1, which means use the existing partitions  Possible values: any integers  geospark.join.indexbuildside  (Advanced users only!)  The side which GeoSpark builds spatial indices on  Default: left  Possible values: left, right  geospark.join.spatitionside  (Advanced users only!)  The dominant side in spatial partitioning stage  Default: left  Possible values: left, right", 
            "title": "Explanation"
        }, 
        {
            "location": "/api/Babylon-Scala-and-Java-API/", 
            "text": "Scala and Java API\n\n\nGeoSpark-Viz (former name, Babylon) Scala and Java API: \nhttp://www.public.asu.edu/~jiayu2/babylon/javadoc/\n\n\nNote: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "Scala/Java doc"
        }, 
        {
            "location": "/api/Babylon-Scala-and-Java-API/#scala-and-java-api", 
            "text": "GeoSpark-Viz (former name, Babylon) Scala and Java API:  http://www.public.asu.edu/~jiayu2/babylon/javadoc/  Note: Scala can call Java APIs seamlessly. That means GeoSparkViz Scala users use the same APIs with GeoSparkViz Java users.", 
            "title": "Scala and Java API"
        }, 
        {
            "location": "/tutorial/benchmark/", 
            "text": "Benchmark\n\n\nWe welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,\n\n\n\n\nPlease always use the latest version or state the version used in your benchmark so that we can trace back to the issues.\n\n\nPlease consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.\n\n\nPlease open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/tutorial/benchmark/#benchmark", 
            "text": "We welcome people to use GeoSpark for benchmark purpose. To achieve the best performance or enjoy all features of GeoSpark,   Please always use the latest version or state the version used in your benchmark so that we can trace back to the issues.  Please consider using GeoSpark core instead of GeoSparkSQL. Due to the limitation of SparkSQL (for instance, not support clustered index), we are not able to expose all features to SparkSQL.  Please open GeoSpark kryo serializer to reduce the memory footprint.", 
            "title": "Benchmark"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/", 
            "text": "Advanced tutorial: Tune your GeoSpark application\n\n\nBefore getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.\n\n\nPick a proper GeoSpark version\n\n\nThe versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.\n\n\nThe first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.\n\n\nThe second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read \nGeoSpark version release notes\n and make sure you are ready to accept the API changes.\n\n\nThe third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.\n\n\nChoose a proper Spatial RDD constructor\n\n\nGeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.\n\n\n\n\n\n\nInitialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:\n\n\npublic PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, \nFileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel)\n\n2. Initialize a SpatialRDD from an existing RDD. A typical example is as follows:\n\n\npublic PointRDD(JavaRDD\nPoint\n rawSpatialRDD, StorageLevel newLevel)\n\n\n\n\n\n\nYou may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.\n\n\nHowever, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:\n\n\n    public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset,\n    FileDataSplitter splitter, boolean carryInputData, Integer partitions,\n    Envelope datasetBoundary, Integer approximateTotalCount) {\n\n\n\n\nManually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.\n\n\nCache the Spatial RDD that is repeatedly used\n\n\nEach SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:\n\n\n\n\nrawSpatialRDD: The RDD generated by SpatialRDD constructors.\n\n\nspatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.\n\n\nindexedRawRDD: The RDD generated by indexing a rawSpatialRDD.\n\n\nindexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.\n\n\n\n\nThese four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:\n\n\n\n\nSpatial Range Query / KNN Query, no index: rawSpatialRDD is used.\n\n\nSpatial Range Query / KNN Query, use index: indexedRawRDD is used.\n\n\nSpatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.\n\n\nSpatial Join Query / Distance Join Query, use index: indexed RDD is used.\n\n\n\n\nTherefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:\n\n\n\n\nIn Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.\n\n\nIn Spark RDD sharing applications such as \nLivy\n and \nSpark Job Server\n, many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.\n\n\n\n\nBe aware of Spatial RDD partitions\n\n\nSometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.\n\n\nAfter that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Tune GeoSpark core"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#advanced-tutorial-tune-your-geospark-application", 
            "text": "Before getting into this advanced tutorial, please make sure that you have tried several GeoSpark functions on your local machine.", 
            "title": "Advanced tutorial: Tune your GeoSpark application"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#pick-a-proper-geospark-version", 
            "text": "The versions of GeoSpark have three levels: X.X.X (i.e., 0.8.1). In addition, GeoSpark also supports Spark 1.X in Spark1.X version.  The first level means that this verion contains big structure redesign which may bring big changes in APIs and performance. Hopefully, we can see these big changes in GeoSpark 1.X version.  The second level (i.e., 0.8) indicates that this version contains significant performance enhancement, big new features and API changes. An old GeoSpark user who wants to pick this version needs to be careful about the API changes. Before you move to this version, please read  GeoSpark version release notes  and make sure you are ready to accept the API changes.  The third level (i.e., 0.8.1) tells that this version only contains bug fixes, some small new features and slight performance enhancement. This version will not contain any API changes. Moving to this version is safe. We highly suggest all GeoSpark users that stay at the same level move to the latest version in this level.", 
            "title": "Pick a proper GeoSpark version"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#choose-a-proper-spatial-rdd-constructor", 
            "text": "GeoSpark provides a number of constructors for each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD). In general, you have two options to start with.    Initialize a SpatialRDD from your data source such as HDFS and S3. A typical example is as follows:  public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset, \nFileDataSplitter splitter, boolean carryInputData, Integer partitions, StorageLevel newLevel) \n2. Initialize a SpatialRDD from an existing RDD. A typical example is as follows:  public PointRDD(JavaRDD Point  rawSpatialRDD, StorageLevel newLevel)    You may notice that these constructors all take as input a \"StorageLevel\" parameter. This is to tell Apache Spark cache the \"rawSpatialRDD\", one attribute of SpatialRDD. The reason why GeoSpark does this is that GeoSpark wants to calculate the dataset boundary and approximate total count using several Apache Spark \"Action\"s. These information are useful when doing Spatial Join Query and Distance Join Query.  However, in some cases, you may know well about your datasets. If so, you can manually provide these information by calling this kind of Spatial RDD constructors:      public PointRDD(JavaSparkContext sparkContext, String InputLocation, Integer Offset,\n    FileDataSplitter splitter, boolean carryInputData, Integer partitions,\n    Envelope datasetBoundary, Integer approximateTotalCount) {  Manually providing the dataset boundary and approxmiate total count helps GeoSpark avoiding several slow \"Action\"s during initialization.", 
            "title": "Choose a proper Spatial RDD constructor"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#cache-the-spatial-rdd-that-is-repeatedly-used", 
            "text": "Each SpatialRDD (PointRDD, PolygonRDD and LineStringRDD) possesses four RDD attributes. They are:   rawSpatialRDD: The RDD generated by SpatialRDD constructors.  spatialPartitionedRDD: The RDD generated by spatial partition a rawSpatialRDD. Note that: this RDD has replicated spatial objects.  indexedRawRDD: The RDD generated by indexing a rawSpatialRDD.  indexedRDD: The RDD generated by indexing a spatialPartitionedRDD. Note that: this RDD has replicated spatial objects.   These four RDDs don't co-exist so you don't need to worry about the memory issue.\nThese four RDDs are invoked in different queries:   Spatial Range Query / KNN Query, no index: rawSpatialRDD is used.  Spatial Range Query / KNN Query, use index: indexedRawRDD is used.  Spatial Join Query / Distance Join Query, no index: spatialPartitionedRDD is used.  Spatial Join Query / Distance Join Query, use index: indexed RDD is used.   Therefore, if you use one of the queries above many times, you'd better cache the associated RDD into memory. There are several possible use cases:   In Spatial Data Mining such as Spatial Autocorrelation and Spatial Co-location Pattern Mining, you may need to use Spatial Join / Spatial Self-join iteratively in order to calculate the adjacency matrix. If so, please cache the spatialPartitionedRDD/indexedRDD which is queries many times.  In Spark RDD sharing applications such as  Livy  and  Spark Job Server , many users may do Spatial Range Query / KNN Query on the same Spatial RDD with different query predicates. You'd better cache the rawSpatialRDD/indexedRawRDD.", 
            "title": "Cache the Spatial RDD that is repeatedly used"
        }, 
        {
            "location": "/tutorial/Advanced-Tutorial-Tune-your-GeoSpark-Application/#be-aware-of-spatial-rdd-partitions", 
            "text": "Sometimes users complain that the execution time is slow in some cases. As the first step, you should always consider increasing the number of your SpatialRDD partitions (2 - 8 times more than the original number). You can do this when you initialize a SpatialRDD. This may significantly improve your performance.  After that, you may consider tuning some other parameters in Apache Spark. For example, you may use Kyro serializer or change the RDD fraction that is cached into memory.", 
            "title": "Be aware of Spatial RDD partitions"
        }, 
        {
            "location": "/tutorial/GeoSpark-Runnable-DEMO/", 
            "text": "GeoSpark Scala Runnable DEMO\n\n\nScala/Java Runnable Example", 
            "title": "Runnable demo"
        }, 
        {
            "location": "/tutorial/GeoSpark-Runnable-DEMO/#geospark-scala-runnable-demo", 
            "text": "Scala/Java Runnable Example", 
            "title": "GeoSpark Scala Runnable DEMO"
        }, 
        {
            "location": "/contact/", 
            "text": "Contact\n\n\nQuestions\n\n\n\n\nGeoSpark@Twitter\n\n\nGeoSpark Discussion Board\n\n\nChat with us! \n\n\nEmail us!\n\n\n\n\nContact\n\n\n\n\n\n\nJia Yu\n (Email: jiayu2@asu.edu)\n\n\n\n\n\n\nMohamed Sarwat\n (Email: msarwat@asu.edu)\n\n\n\n\n\n\nProject website\n\n\nPlease visit \nGeoSpark project wesbite\n for latest news and releases.\n\n\nData Systems Lab\n\n\nGeoSpark is one of the projects initiated by \nData Systems Lab\n at Arizona State University. The mission of Data Systems Lab is designing and developing experimental data management systems (e.g., database systems).", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#contact", 
            "text": "", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#questions", 
            "text": "GeoSpark@Twitter  GeoSpark Discussion Board  Chat with us!   Email us!", 
            "title": "Questions"
        }, 
        {
            "location": "/contact/#contact_1", 
            "text": "Jia Yu  (Email: jiayu2@asu.edu)    Mohamed Sarwat  (Email: msarwat@asu.edu)", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#project-website", 
            "text": "Please visit  GeoSpark project wesbite  for latest news and releases.", 
            "title": "Project website"
        }, 
        {
            "location": "/contact/#data-systems-lab", 
            "text": "GeoSpark is one of the projects initiated by  Data Systems Lab  at Arizona State University. The mission of Data Systems Lab is designing and developing experimental data management systems (e.g., database systems).", 
            "title": "Data Systems Lab"
        }
    ]
}